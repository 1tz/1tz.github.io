<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>俄罗斯韬娃</title>
    <description>An undergraduate in Beijing Normal University</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 07 Jun 2018 21:26:28 +0800</pubDate>
    <lastBuildDate>Thu, 07 Jun 2018 21:26:28 +0800</lastBuildDate>
    <generator>Jekyll v3.6.2</generator>
    
      <item>
        <title>学习如何学习</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;活到老，学到老&lt;/p&gt;

  &lt;p&gt;学习如何学习是Coursera的一门课，详见&lt;a href=&quot;https://www.coursera.org/learn/learning-how-to-learn&quot;&gt;此处&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;去掉了一些鸡汤，留下来了一点鸡胸，慢慢嚼吧&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;1-集中与发散思维&quot;&gt;1. 集中与发散思维&lt;/h2&gt;

&lt;p&gt;人有两种思维模式：专注模式和发散模式。这两种思维模式只能同时激活其中之一。大脑需要练习这两种模式的切换，以针对不同的问题。这样，才能发挥这两种思维模式的各自优势。通常，发散模式在大脑较为放松的时候被激活，而专注模式在完成一系列相关过程时被激活。&lt;/p&gt;

&lt;h2 id=&quot;2-拖延症记忆与睡眠&quot;&gt;2. 拖延症、记忆与睡眠&lt;/h2&gt;

&lt;h3 id=&quot;21-拖延症&quot;&gt;2.1 拖延症&lt;/h3&gt;

&lt;p&gt;当人们遇到一件极其不愿意做的事情时，大脑中与疼痛感相关的区域被激活。自然而然，大脑就会寻找停止这种负面刺激的方式，即将注意力转移到其他事情上。但是当人们能够面对这些不愿意做的事情时，这种不适感很快就消失了。使用番茄工作法可以解决拖延症，一个番茄时钟完成后一定要给自己奖励、放松放松，这样才能不断激励自己。&lt;/p&gt;

&lt;h3 id=&quot;22-记忆&quot;&gt;2.2 记忆&lt;/h3&gt;

&lt;p&gt;记忆主要分为工作记忆和长期记忆两种。工作记忆是大脑即时、有意识处理信息时的记忆，它只能储存大约四个组块的信息。长期记忆则是一个记忆仓库，能够存储很多记忆。将工作记忆转入长期记忆仓库，需要间隔重复。&lt;/p&gt;

&lt;p&gt;数学和理论科学中的思想和概念普遍抽象。越是抽象的东西，越要反复练习、加强，记忆就会不断加深。学习不是一蹴而就的，学习一段时间后，休息一下，把注意力转移到其他东西上。这期间，大脑的发散模式在后台不断固化对概念的理解。这样建立起来的知识体系才是牢固的。&lt;/p&gt;

&lt;h3 id=&quot;23-睡眠&quot;&gt;2.3 睡眠&lt;/h3&gt;

&lt;p&gt;睡眠能够祛除大脑产生的有毒物质。在睡梦中，大脑会把学习到的东西反复演练，清楚一些记忆中不太重要的部分，增强想要记住的记忆。&lt;/p&gt;

&lt;h2 id=&quot;3-组块要领&quot;&gt;3. 组块——要领&lt;/h2&gt;

&lt;h3 id=&quot;31-何为组块&quot;&gt;3.1 何为组块&lt;/h3&gt;

&lt;p&gt;组块就是把信息碎片拼接起来的的过程。组块化让新的信息更容易记忆，也更容易将其整合到大框架内。&lt;/p&gt;

&lt;h3 id=&quot;32-如何构成组块&quot;&gt;3.2 如何构成组块&lt;/h3&gt;

&lt;p&gt;首先，需要对待组块化的信息保持注意力。然后，要求对建立组块的对象有基本了解，只有在自己实际操作和完全掌握的情况下才能建立起组块。其次，要获取背景知识。背景认识意味着学会在特定的时候使用正确的方法。最后，熟能生巧。通过练习，不论是由上而下的认识，还是由下而上的组块化，都能够让你更熟练地运用这个组块。&lt;/p&gt;

&lt;h3 id=&quot;33-能力错觉&quot;&gt;3.3 能力错觉&lt;/h3&gt;

&lt;p&gt;阅读会让人产生能力错觉。仅仅扫一眼答案而不代入题目中并没有什么作用。同样，做笔记时，高亮和下划线标记需要谨慎，否则也会让自己误认为已经记住这些概念。如果要做标记，试着在勾画前找到中心思想，并试着尽量减少划线和高亮的内容，每段不超过一句。除此之外，空白处总结关键概念是一种很好的办法 。&lt;/p&gt;

&lt;p&gt;回顾，即在心中检索关键概念，可以使学习更加专注和高效，也能够避免部分能力错觉。回顾需要间隔时间，需要潜意识在大脑中不断巩固。在不同场景回顾学习的东西，与会帮助加深对学习内容的理解。&lt;/p&gt;

&lt;h2 id=&quot;4-看到全局&quot;&gt;4. 看到全局&lt;/h2&gt;

&lt;h3 id=&quot;41-组块库的价值&quot;&gt;4.1 组块库的价值&lt;/h3&gt;

&lt;p&gt;组块越多、运用越熟练，能更容易地解决问题。即使是理解不同领域的新概念，组块还能起到迁移的作用。组块能够在不熟悉新知识的情况下，给发散模式带来直觉，再由专注模式小心求证。&lt;/p&gt;

&lt;h3 id=&quot;42-过度学习抑制思维定式与交叉&quot;&gt;4.2 过度学习、抑制、思维定式与交叉&lt;/h3&gt;

&lt;p&gt;当学会一个东西时，在那段时间内不断重复并不能加强关于它的长期记忆，反而容易造成能力错觉——让人误以为掌握了所有内容，但是实际只了解个皮毛。这时，应该把精力集中于困难的部分，使用刻意训练。&lt;/p&gt;

&lt;p&gt;原先的思维模式，加上专注模式，会形成思维定势，这会阻止你走向可能发现解法的新区域。所以在学习新事物的时候，你必须摒弃错误的旧思想和方法。&lt;/p&gt;

&lt;p&gt;当你掌握了某一技巧的基本概念时，你应该开始将其交叉于不同类别的问题、方法、概念、过程和学科中。交叉学习让大脑更具有灵活性和创造性，让大脑学会独立思考。&lt;/p&gt;

&lt;h2 id=&quot;5-拖延症&quot;&gt;5. 拖延症&lt;/h2&gt;

&lt;h3 id=&quot;51-我们都需要治疗拖延症&quot;&gt;5.1 我们都需要治疗拖延症&lt;/h3&gt;

&lt;p&gt;拖延症如同饮鸩止渴，短时间看似解决了问题，实则更糟。&lt;/p&gt;

&lt;h3 id=&quot;52-习惯&quot;&gt;5.2 习惯&lt;/h3&gt;

&lt;p&gt;习惯可以分成四个阶段：&lt;/p&gt;

&lt;p&gt;信号（cue）。本身无所谓好坏，重要的是我们对信号做出的反应&lt;/p&gt;

&lt;p&gt;惯式（routine）。大脑接收到信号后做出的习惯反应。&lt;/p&gt;

&lt;p&gt;奖励（reward）。任何一种习惯的养成都是因为这种习惯可以给我们带来好处。&lt;/p&gt;

&lt;p&gt;信念（belief）。坚信习惯，习惯就回变得强大，想要改变习惯，就必须改变潜藏其中的信念。&lt;/p&gt;

&lt;h3 id=&quot;53-过程与结果&quot;&gt;5.3 过程与结果&lt;/h3&gt;

&lt;p&gt;拖延的一个原因就是，人们经常把注意力放在结果上。为防止拖延，应把注意力集中在过程上。过程与一个个小习惯相关，这些小习惯可以帮助完成不愉快但是必须完成的任务。越是注重结果，越会适得其反。&lt;/p&gt;

&lt;p&gt;使用番茄时钟，为自己制定这一时间段的工作过程，而不是必须完成的任务，可以让人专注于过程。当然，使用降噪耳机为自己提供安静的学习环境也可以帮助集中注意力。&lt;/p&gt;

&lt;h3 id=&quot;54-利用僵尸意识来帮助自己&quot;&gt;5.4 利用僵尸意识来帮助自己&lt;/h3&gt;

&lt;p&gt;习惯的第一个要素是信号。克服一个习惯的诀窍就是改变你对某个信号的反应，唯一需要毅力的地方就是改变对这个信号的反应。拖延的问题在于自己意识不到自己正在拖延。你可以关闭对你时常有诱惑力的东西，来隔离那些最具杀伤力的信号。&lt;/p&gt;

&lt;p&gt;第二个要素是惯式。拖延时，经常把注意力转移到一些不那么痛苦的事情上去，大脑便会自动进入这个惯式。你必须主动重设旧习惯的反应信号，其关键是制订一个计划、养成一个新习惯。&lt;/p&gt;

&lt;p&gt;第三个是奖励。需要在摆脱拖延后，给自己一些奖励：毫无罪意地刷剧、看一场电影或是喝一瓶肥仔快乐水。当完成事情的效果越好，这件事情带来的愉悦感也越强，好好奖励奖励自己吧。&lt;/p&gt;

&lt;p&gt;最后是信念。可能事情越棘手时，越想回到拖延的常态中，但是新习惯的养成，能够让你坚持下去。&lt;/p&gt;

&lt;h3 id=&quot;55-应对生活与学习&quot;&gt;5.5 应对生活与学习&lt;/h3&gt;

&lt;p&gt;前一天晚上写下第二天的计划，这样就不会占用你的工作记忆。&lt;/p&gt;

&lt;p&gt;设定番茄时钟，劳逸结合，设置结束时间同样重要。&lt;/p&gt;

&lt;p&gt;最好睡前回忆复习一遍学习的新鲜事物。&lt;/p&gt;

&lt;h2 id=&quot;6-复兴式学习释放潜力&quot;&gt;6. 复兴式学习、释放潜力&lt;/h2&gt;

&lt;h3 id=&quot;61-怎样成为更好的学习者&quot;&gt;6.1 怎样成为更好的学习者&lt;/h3&gt;

&lt;p&gt;我们不能局限于学习狭义的课堂上的知识，学习如何学习是一种任何人都能掌握的技巧。&lt;/p&gt;

&lt;h3 id=&quot;62-创建生动的视觉比喻或类比&quot;&gt;6.2 创建生动的视觉比喻或类比&lt;/h3&gt;

&lt;p&gt;学习中引入比喻或类比，有助于记忆和理解概念，还能摆脱思维定式，将其运用在其他领域。&lt;/p&gt;

&lt;h3 id=&quot;63-团队合作的价值&quot;&gt;6.3 团队合作的价值&lt;/h3&gt;

&lt;p&gt;人的右脑能够帮助反观全局，发现其不合常理的地方，不断质疑。人的左脑则让人容易变得自满。最容易欺骗的人是自己。找到自己盲点和误区的最好方式之一，就是与同领域的人精诚合作。但是也要注意学习小组的效率，尽量减少闲聊、避免迟到。&lt;/p&gt;

&lt;h3 id=&quot;64-清单&quot;&gt;6.4 清单&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;你是否认真努力地去理解过课文？ 仅仅是找出课文里有解答过程的例题不算&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;你是否跟同学讨论过作业中的问题或是至少和其他人对过答案？&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;你是否尝试过在和同学讨论之前，先列出每道作业的解题大纲？&lt;/li&gt;
  &lt;li&gt;你是否积极参与作业小组中的讨论、贡献自己的观点并提出问题？&lt;/li&gt;
  &lt;li&gt;当你遇到问题的时候，是否会去咨询讲师或助教？&lt;/li&gt;
  &lt;li&gt;交作业的时候，你是否已经弄清了所有问题的答案？&lt;/li&gt;
  &lt;li&gt;对于作业中不明白的问题，你是否在课上提出疑问寻求解答？&lt;/li&gt;
  &lt;li&gt;如果你有辅导书，在考试前你是否已经认真通读它，并且相信自己弄明白了书上所有的问题？&lt;/li&gt;
  &lt;li&gt;你是否尝试略过具体计算，直接快速写出一些问题的解题思路？&lt;/li&gt;
  &lt;li&gt;你是否和同学一起复习过辅导书上的内容和其他问题并相互提问？&lt;/li&gt;
  &lt;li&gt;如果考前有复习课，你是否参加过，并对自己不确定的部分提出疑问？&lt;/li&gt;
  &lt;li&gt;考试前睡眠时间安排是否合理？&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;65-先难后易&quot;&gt;6.5 先难后易&lt;/h3&gt;

&lt;p&gt;难题能够唤起发散模式中的创造力，一两分钟内无法解决就迅速切换，做几道简单的后再来挑战。这个方法也可以有效帮你避开思维定势，或是避免陷入错误思维的泥沼，因为你将有机会从不同的角度看待这些问题。&lt;/p&gt;

&lt;h3 id=&quot;66-有用的有关测验的最后提示&quot;&gt;6.6 有用的、有关测验的最后提示&lt;/h3&gt;

&lt;p&gt;在考试或测试前一天，最后快速浏览一遍资料、温习所学知识。不要为自己似乎没在大考前一天拼命复习而自责，不要为自己似乎没在大考前一天拼命复习而自责。如果你好好准备了，考前稍稍放松就是一种自然反应，就像下意识地节省脑力一样。考试时，你也应该谨记：你的大脑还会欺骗你。你应该时刻擦亮眼睛 ，转移注意力并再次检查，从全局角度核对你的答案。多问问自己：这样做真的合乎逻辑吗？在你检查时，由后至前的顺序往往更能给大脑一个新鲜的视角，让你更容易揪出错误。当你充分准备、勤奋练习、储备知识，考试时再注意技巧，你就会发现好运气会逐渐地出现在你面前。&lt;/p&gt;
</description>
        <pubDate>Thu, 03 May 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/05/03/lhtl/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/03/lhtl/</guid>
        
        <category>学习</category>
        
        
      </item>
    
      <item>
        <title>Machine Learning Week 6</title>
        <description>&lt;h1 id=&quot;8-advice-for-applying-machine-learning&quot;&gt;8. Advice for Applying Machine Learning&lt;/h1&gt;

&lt;h2 id=&quot;81-evaluating-a-learning-algorithm&quot;&gt;8.1 Evaluating a Learning Algorithm&lt;/h2&gt;

&lt;h3 id=&quot;811-evaluating-a-hypothesis&quot;&gt;8.1.1 Evaluating a Hypothesis&lt;/h3&gt;

&lt;p&gt;Once we have done some trouble shooting for errors in our predictions by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Getting more training examples&lt;/li&gt;
  &lt;li&gt;Trying smaller sets of features&lt;/li&gt;
  &lt;li&gt;Trying additional features&lt;/li&gt;
  &lt;li&gt;Trying polynomial features&lt;/li&gt;
  &lt;li&gt;Increasing or decreasing $\lambda$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can move on to evaluate our new hypothesis.&lt;/p&gt;

&lt;p&gt;A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a &lt;strong&gt;training set&lt;/strong&gt; and a &lt;strong&gt;test set&lt;/strong&gt;. Typically, the training set consists of 70 % of your data and the test set is the remaining 30 %.&lt;/p&gt;

&lt;p&gt;The new procedure using these two sets is then:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Learn $\Theta$ and minimize $J_{train}(\Theta)$ using the training set&lt;/li&gt;
  &lt;li&gt;Compute the test set error $J_{test}(\Theta)$&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;the-test-set-error&quot;&gt;The test set error&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;For linear regression: $J_{test}(\Theta) = \dfrac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_\Theta(x^{(i)}&lt;em&gt;{test}) - y^{(i)}&lt;/em&gt;{test})^2$&lt;/li&gt;
  &lt;li&gt;For classification ~ Misclassification error (aka 0/1 misclassification error):&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
err(h_\Theta(x),y) = \begin{matrix} 1 &amp; \mbox{if } h_\Theta(x) \geq 0.5\ and\ y = 0\ or\ h_\Theta(x) &lt; 0.5\ and\ y = 1\newline 0 &amp; \mbox otherwise \end{matrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;This gives us a binary 0 or 1 error result based on a misclassification. The average test error for the test set is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Test Error} = \dfrac{1}{m_{test}} \sum^{m_{test}}_{i=1} err(h_\Theta(x^{(i)}_{test}), y^{(i)}_{test})&lt;/script&gt;

&lt;p&gt;This gives us the proportion of the test data that was misclassified.&lt;/p&gt;

&lt;h3 id=&quot;812-model-selection-and-trainvalidationtest-sets&quot;&gt;8.1.2 Model Selection and Train/Validation/Test Sets&lt;/h3&gt;

&lt;p&gt;Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. It could over fit and as a result your predictions on the test set would be poor. The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than the error on any other data set.&lt;/p&gt;

&lt;p&gt;Given many models with different polynomial degrees, we can use a systematic approach to identify the ‘best’ function. In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result.&lt;/p&gt;

&lt;p&gt;One way to break down our dataset into the three sets is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training set: 60%&lt;/li&gt;
  &lt;li&gt;Cross validation set: 20%&lt;/li&gt;
  &lt;li&gt;Test set: 20%&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can now calculate three separate error values for the three different sets using the following method:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Optimize the parameters in Θ using the training set for each polynomial degree.&lt;/li&gt;
  &lt;li&gt;Find the polynomial degree d with the least error using the cross validation set.&lt;/li&gt;
  &lt;li&gt;Estimate the generalization error using the test set with $J_{test}(\Theta^{(d)})$, (d = theta from polynomial with lower error);&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This way, the degree of the polynomial d has not been trained using the test set.&lt;/p&gt;

&lt;h2 id=&quot;82-bias-vs-variance&quot;&gt;8.2 Bias vs. Variance&lt;/h2&gt;

&lt;h3 id=&quot;821-diagnosing-bias-vs-variance&quot;&gt;8.2.1 Diagnosing Bias vs. Variance&lt;/h3&gt;

&lt;p&gt;In this section we examine the relationship between the degree of the polynomial d and the underfitting or overfitting of our hypothesis.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We need to distinguish whether &lt;strong&gt;bias&lt;/strong&gt; or &lt;strong&gt;variance&lt;/strong&gt; is the problem contributing to bad predictions.&lt;/li&gt;
  &lt;li&gt;High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The training error will tend to &lt;strong&gt;decrease&lt;/strong&gt; as we increase the degree d of the polynomial.&lt;/p&gt;

&lt;p&gt;At the same time, the cross validation error will tend to &lt;strong&gt;decrease&lt;/strong&gt; as we increase d up to a point, and then it will &lt;strong&gt;increase&lt;/strong&gt; as d is increased, forming a convex curve.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;High bias (underfitting)&lt;/strong&gt;: both $J_{train}(\Theta)$ and $ J_{CV}(\Theta)$ will be high. Also, $J_{CV}(\Theta) \approx J_{train}(\Theta)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;High variance (overfitting)&lt;/strong&gt;: $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be much greater than $J_{train}(\Theta)$.&lt;/p&gt;

&lt;p&gt;The is summarized in the figure below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79gy1frl0tbtmzjj308c073q3l.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;822-regularization-and-biasvariance&quot;&gt;8.2.2 Regularization and Bias/Variance&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; [The regularization term below and through out the video should be $\frac \lambda {2m} \sum _{j=1}^n \theta_j ^2$ and &lt;strong&gt;NOT&lt;/strong&gt; $\frac \lambda {2m} \sum _{j=1}^m \theta_j ^2$]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1frl1fgdjz8j30jj0akq4m.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the figure above, we see that as $\lambda$ increases, our fit becomes more rigid. On the other hand, as $\lambda$ approaches 0, we tend to over overfit the data. So how do we choose our parameter $\lambda$ to get it ‘just right’ ? In order to choose the model and the regularization term $\lambda$, we need to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Create a list of lambdas (i.e. $\lambda∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24}$);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a set of models with different degrees or any other variants.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Iterate through the $\lambda$s and for each $\lambda$ go through all the models to learn some $\Theta$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute the cross validation error using the learned $\Theta$ (computed with $\lambda$) on the $J_{CV}(\Theta)$ &lt;strong&gt;without&lt;/strong&gt; regularization or $\lambda$ = 0.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Select the best combo that produces the lowest error on the cross validation set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using the best combo $\Theta$ and $\lambda$, apply it on $ J_{test}(\Theta)$ to see if it has a good generalization of the problem.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;823-learning-curves&quot;&gt;8.2.3 Learning Curves&lt;/h3&gt;

&lt;p&gt;Training an algorithm on a very few number of data points (such as 1, 2 or 3) will easily have 0 errors because we can always find a quadratic curve that touches exactly those number of points. Hence:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As the training set gets larger, the error for a quadratic function increases.&lt;/li&gt;
  &lt;li&gt;The error value will plateau out after a certain m, or training set size.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Experiencing high bias:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Low training set size&lt;/strong&gt;: causes $J_{train}(\Theta)$ to be low and $J_{CV}(\Theta)$ to be high.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Large training set size&lt;/strong&gt;: causes both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ to be high with $J_{train}(\Theta)\approx J_{CV}(\Theta)$.&lt;/p&gt;

&lt;p&gt;If a learning algorithm is suffering from &lt;strong&gt;high bias&lt;/strong&gt;, getting more training data will not &lt;strong&gt;(by itself)&lt;/strong&gt; help much.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1frl2eobr1kj308c056q3e.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experiencing high variance:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Low training set size&lt;/strong&gt;: $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be high.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Large training set size&lt;/strong&gt;: $J_{train}(\Theta)$ increases with training set size and $J_{CV}(\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\Theta) &amp;lt; J_{CV}(\Theta)$ but the difference between them remains significant.&lt;/p&gt;

&lt;p&gt;If a learning algorithm is suffering from &lt;strong&gt;high variance&lt;/strong&gt;, getting more training data is likely to help.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1frl2epp3moj308c04rdgb.jpg&quot; alt=&quot;img&quot; /&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1frl2etwmn4j308c04rdgb.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;824-deciding-what-to-do-next-revisited&quot;&gt;8.2.4 Deciding What to Do Next Revisited&lt;/h3&gt;

&lt;p&gt;Our decision process can be broken down as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Getting more training examples:&lt;/strong&gt; Fixes high variance&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Trying smaller sets of features:&lt;/strong&gt; Fixes high variance&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Adding features:&lt;/strong&gt; Fixes high bias&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Adding polynomial features:&lt;/strong&gt; Fixes high bias&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Decreasing $\lambda$:&lt;/strong&gt; Fixes high bias&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Increasing $\lambda$:&lt;/strong&gt; Fixes high variance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;825-diagnosing-neural-networks&quot;&gt;8.2.5 Diagnosing Neural Networks&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A neural network with fewer parameters is &lt;strong&gt;prone to underfitting&lt;/strong&gt;. It is also &lt;strong&gt;computationally cheaper&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;A large neural network with more parameters is &lt;strong&gt;prone to overfitting&lt;/strong&gt;. It is also &lt;strong&gt;computationally expensive&lt;/strong&gt;. In this case you can use regularization (increase $\lambda$) to address the overfitting.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using your cross validation set. You can then select the one that performs best.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model Complexity Effects:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.&lt;/li&gt;
  &lt;li&gt;Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.&lt;/li&gt;
  &lt;li&gt;In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;83-building-a-spam-classifier&quot;&gt;8.3 Building a Spam Classifier&lt;/h2&gt;

&lt;h3 id=&quot;831-prioritizing-what-to-work-on&quot;&gt;8.3.1 Prioritizing What to Work On&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;System Design Example:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Given a data set of emails, we could construct a vector for each email. Each entry in this vector represents a word. The vector normally contains 10,000 to 50,000 entries gathered by finding the most frequently used words in our data set. If a word is to be found in the email, we would assign its respective entry a 1, else if it is not found, that entry would be a 0. Once we have all our x vectors ready, we train our algorithm and finally, we could use it to classify if an email is a spam or not.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1frm85bqkz7j30ki09mdk2.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So how could you spend your time to improve the accuracy of this classifier?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Collect lots of data (for example “honeypot” project but doesn’t always work)&lt;/li&gt;
  &lt;li&gt;Develop sophisticated features (for example: using email header data in spam emails)&lt;/li&gt;
  &lt;li&gt;Develop algorithms to process your input in different ways (recognizing misspellings in spam).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is difficult to tell which of the options will be most helpful.&lt;/p&gt;

&lt;h3 id=&quot;832-error-analysis&quot;&gt;8.3.2 Error Analysis&lt;/h3&gt;

&lt;p&gt;The recommended approach to solving machine learning problems is to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start with a simple algorithm, implement it quickly, and test it early on your cross validation data.&lt;/li&gt;
  &lt;li&gt;Plot learning curves to decide if more data, more features, etc. are likely to help.&lt;/li&gt;
  &lt;li&gt;Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, assume that we have 500 emails and our algorithm misclassifies a 100 of them. We could manually analyze the 100 emails and categorize them based on what type of emails they are. We could then try to come up with new cues and features that would help us classify these 100 emails correctly. Hence, if most of our misclassified emails are those which try to steal passwords, then we could find some features that are particular to those emails and add them to our model. We could also see how classifying each word according to its root changes our error rate:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79gy1frm8mnk8l5j30jg0a143a.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is very important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm’s performance. For example if we use stemming, which is the process of treating the same word with different forms (fail/failing/failed) as one word (fail), and get a 3% error rate instead of 5%, then we should definitely add it to our model. However, if we try to distinguish between upper case and lower case letters and end up getting a 3.2% error rate instead of 3%, then we should avoid using this new feature. Hence, we should try new things, get a numerical value for our error rate, and based on our result decide whether we want to keep the new feature or not.&lt;/p&gt;

&lt;h2 id=&quot;84-handling-skewed-data&quot;&gt;8.4 Handling Skewed Data&lt;/h2&gt;

&lt;h3 id=&quot;841-error-metrics-for-skewed-classes&quot;&gt;8.4.1 Error Metrics for Skewed Classes&lt;/h3&gt;

&lt;p&gt;Precision and recall are defined according to:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1frm90iwi53j30ae0akweg.jpg&quot; alt=&quot;If predicted class and actual class are both 1, then a test example is a True Positive. If predicted class and actual class are both 0, then a test example is a True Negative. If predicted class is 0 actual class is 1, then a test example is a False Negative. If predicted class is 1 and actual class is 0, then a test example is a False Positive.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Precision&lt;/strong&gt; (Of all patients where we predicted $y=1$, what fraction actually has cancer?)
&lt;script type=&quot;math/tex&quot;&gt;\text{Precision} = \frac{\text{True positives}}{\text{# predicted as positive}} = \frac{\text{True positives}}{\text{True positives + False positives}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recall&lt;/strong&gt; (Of all patients that actually have cancer, what fraction did we correctly detect as having cancer?)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Recall} = \frac{\text{True positives}}{\text{# actual positives}} = \frac{\text{True positives}}{\text{True positives + False negatives}}&lt;/script&gt;

&lt;h3 id=&quot;842-trading-off-precision-an-recall&quot;&gt;8.4.2 Trading Off Precision an Recall&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79gy1frm9eyehi3j30q70eqdim.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79gy1frm9m7r3jpj30q80ergnj.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Mon, 30 Apr 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/04/30/ml-week6/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/04/30/ml-week6/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>Machine Learning Week 5</title>
        <description>&lt;h1 id=&quot;7-neural-networks-learning&quot;&gt;7. Neural Networks: Learning&lt;/h1&gt;

&lt;h2 id=&quot;71-backpropagation&quot;&gt;7.1 Backpropagation&lt;/h2&gt;

&lt;h3 id=&quot;711-cost-function&quot;&gt;7.1.1 Cost Function&lt;/h3&gt;

&lt;p&gt;Let’s first define a few variables that we will need to use:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;L = total number of layers in the network&lt;/li&gt;
  &lt;li&gt;$s_l$ = number of units (not counting bias unit) in layer l&lt;/li&gt;
  &lt;li&gt;K = number of output units/classes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recall that in neural networks, we may have many output nodes. We denote $h_\Theta(x)_k$ as being a hypothesis that results in the $k^{th}$ output. Our cost function for neural networks is going to be a generalization of the one we used for logistic regression. Recall that the cost function for regularized logistic regression was:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(θ)=−\frac{1}{m}∑_{i=1}^m[y^{(i)} log(h_\theta(x^{(i)}))+(1−y^{(i)}) log(1−h_\theta(x^{(i)}))]+\frac\lambda{2m}∑_{j=1}^nθ_j^2&lt;/script&gt;

&lt;p&gt;For neural networks, it is going to be slightly more complicated:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gather*} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather*}&lt;/script&gt;

&lt;p&gt;We have added a few nested summations to account for our multiple output nodes. In the first part of the equation, before the square brackets, we have an additional nested summation that loops through the number of output nodes.&lt;/p&gt;

&lt;p&gt;In the regularization part, after the square brackets, we must account for multiple theta matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term.&lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the double sum simply adds up the logistic regression costs calculated for each cell in the output layer&lt;/li&gt;
  &lt;li&gt;the triple sum simply adds up the squares of all the individual Θs in the entire network.&lt;/li&gt;
  &lt;li&gt;the i in the triple sum does &lt;strong&gt;not&lt;/strong&gt; refer to training example i&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;712-backpropagation-algorithm&quot;&gt;7.1.2 Backpropagation Algorithm&lt;/h3&gt;

&lt;p&gt;“Backpropagation” is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression. Our goal is to compute:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_\Theta J(\Theta)&lt;/script&gt;

&lt;p&gt;That is, we want to minimize our cost function J using an optimal set of parameters in theta. In this section we’ll look at the equations we use to compute the partial derivative of J(Θ):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta)&lt;/script&gt;

&lt;p&gt;To do so, we use the following algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tKfTcgy1fredq4s50ij30om0d9gs6.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Back propagation Algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Given training set $\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Set $\Delta^{(l)}_{i,j} := 0$ for all (l,i,j), (hence you end up having a matrix full of zeros)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For training example t =1 to m:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Set $a^{(1)} := x^{(t)}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Perform forward propagation to compute $a^{(l)}$ for l=2,3,…,L&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcgy1frkciksidkj30d8074q53.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Using $y^{(t)}$, compute $\delta^{(L)} = a^{(L)} - y^{(t)}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Where L is our total number of layers and $a^{(L)}$ is the vector of outputs of the activation units for the last layer. So our “error values” for the last layer are simply the differences of our actual results in the last layer and the correct outputs in y. To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Compute $\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}$ using $\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .&lt;em&gt;\ a^{(l)}\ .&lt;/em&gt;\ (1 - a^{(l)})$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The delta values of layer l are calculated by multiplying the delta values in the next layer with the theta matrix of layer l. We then element-wise multiply that with a function called g’, or g-prime, which is the derivative of the activation function g evaluated with the input values given by $z^{(l)}$.&lt;/p&gt;

&lt;p&gt;The g-prime derivative terms can also be written out as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g'(z^{(l)}) = a^{(l)}\ .*\ (1 - a^{(l)})&lt;/script&gt;

&lt;ol&gt;
  &lt;li&gt;$\Delta^{(l)}&lt;em&gt;{i,j} := \Delta^{(l)}&lt;/em&gt;{i,j} + a_j^{(l)} \delta_i^{(l+1)}$ or with vectorization, $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hence we update our new $\Delta$ matrix.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$D^{(l)}&lt;em&gt;{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}&lt;/em&gt;{i,j} + \lambda\Theta^{(l)}_{i,j}\right)$, if j≠0.&lt;/li&gt;
  &lt;li&gt;$D^{(l)}&lt;em&gt;{i,j} := \dfrac{1}{m}\Delta^{(l)}&lt;/em&gt;{i,j}$ If j=0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The capital-delta matrix D is used as an “accumulator” to add up our values as we go along and eventually compute our partial derivative. Thus we get $\frac \partial {\partial \Theta_{ij}^{(l)}} J(\Theta)= D_{ij}^{(l)}$&lt;/p&gt;

&lt;h3 id=&quot;713-backpropagation-intuition&quot;&gt;7.1.3 Backpropagation Intuition&lt;/h3&gt;

&lt;p&gt;Recall that the cost function for a neural network is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gather*}J(\Theta) = - \frac{1}{m} \sum_{t=1}^m\sum_{k=1}^K \left[ y^{(t)}_k \ \log (h_\Theta (x^{(t)}))_k + (1 - y^{(t)}_k)\ \log (1 - h_\Theta(x^{(t)})_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_l+1} ( \Theta_{j,i}^{(l)})^2\end{gather*}&lt;/script&gt;

&lt;p&gt;If we consider simple non-multiclass classification (k = 1) and disregard regularization, the cost is computed with:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;cost(t)=y^{(t)}log(h_\Theta(x^{(t)}))+(1-y^{(t)})log(1-h_\Theta(x^{(t)}))&lt;/script&gt;

&lt;p&gt;Intuitively, $\delta_j^{(l)}$ is the “error” for $a^{(l)}_j$ (unit j in layer l). More formally, the delta values are actually the derivative of the cost function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta_j^{(l)}=\frac \partial {\partial z_j^{(l)}}cost(t)&lt;/script&gt;

&lt;p&gt;Recall that our derivative is the slope of a line tangent to the cost function, so the steeper the slope the more incorrect we are. Let us consider the following neural network below and see how we could calculate some $\delta_j^{(l)}$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcly1frekhy8t1bj30k00b9wi3.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the image above, to calculate $\delta_2^{(2)}$, we multiply the weights $\Theta_{12}^{(2)}$ and $\Theta_{22}^{(2)}$ by their respective $\delta$ values found to the right of each edge. So we get $\delta_2^{(2)}= \Theta_{12}^{(2)}&lt;em&gt;\delta_1^{(3)}+\Theta_{22}^{(2)}&lt;/em&gt;\delta_2^{(3)}$. To calculate every single possible $\delta_j^{(l)}$, we could start from the right of our diagram. We can think of our edges as our $\Theta_{ij}$. Going from right to left, to calculate the value of $\delta_j^{(l)}$, you can just take the over all sum of each weight times the $\delta$ it is coming from. Hence, another example would be $\delta_2^{(3)}=\Theta_{12}^{(3)}*\delta_1^{(4)}$.&lt;/p&gt;

&lt;h2 id=&quot;72-backpropagation-in-practice&quot;&gt;7.2 Backpropagation in Practice&lt;/h2&gt;

&lt;h3 id=&quot;721-implementation-note-unrolling-parameters&quot;&gt;7.2.1 Implementation Note: Unrolling Parameters&lt;/h3&gt;

&lt;p&gt;With neural networks, we are working with sets of matrices:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*} \Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}, \dots \newline D^{(1)}, D^{(2)}, D^{(3)}, \dots \end{align*}&lt;/script&gt;

&lt;p&gt;In order to use optimizing functions such as “fminunc()”, we will want to “unroll” all the elements and put them into one long vector:&lt;/p&gt;

&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;thetaVector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Theta1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Theta2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Theta3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:);&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;deltaVector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11, then we can get back our original matrices from the “unrolled” versions as follows:&lt;/p&gt;

&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Theta1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thetaVector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;110&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Theta2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thetaVector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;111&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;220&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Theta3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thetaVector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;221&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;231&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To summarize:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tKfTcly1frekyv4avsj30ey06aq57.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;722-gradient-checking&quot;&gt;7.2.2 Gradient Checking&lt;/h3&gt;

&lt;p&gt;Gradient checking will assure that our backpropagation works as intended. We can approximate the derivative of our cost function with:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}&lt;/script&gt;

&lt;p&gt;With multiple theta matrices, we can approximate the derivative &lt;strong&gt;with respect to&lt;/strong&gt; Θ_jΘj as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}&lt;/script&gt;

&lt;p&gt;A small value for ${\epsilon}$ (epsilon) such as ${\epsilon = 10^{-4}}$, guarantees that the math works out properly. If the value for $\epsilon$ is too small, we can end up with numerical problems.&lt;/p&gt;

&lt;p&gt;Hence, we are only adding or subtracting epsilon to the $\Theta_j$ matrix. In octave we can do it as follows:&lt;/p&gt;

&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;thetaPlus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;thetaPlus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;thetaMinus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;thetaMinus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gradApprox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;J&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thetaPlus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;J&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thetaMinus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))/(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We previously saw how to calculate the deltaVector. So once we compute our gradApprox vector, we can check that gradApprox $ \approx$ deltaVector.&lt;/p&gt;

&lt;p&gt;Once you have verified &lt;strong&gt;once&lt;/strong&gt; that your backpropagation algorithm is correct, you don’t need to compute gradApprox again. The code to compute gradApprox can be very slow.&lt;/p&gt;

&lt;h3 id=&quot;723-random-initialization&quot;&gt;7.2.3 Random Initialization&lt;/h3&gt;

&lt;p&gt;Initializing all theta weights to zero does not work with neural networks. When we backpropagate, all nodes will update to the same value repeatedly. Instead we can randomly initialize our weights for our \ThetaΘmatrices using the following method:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcly1frgukh5tezj30fb07qjsk.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hence, we initialize each $\Theta^{(l)}_{ij}$ to a random value between $[-\epsilon,\epsilon]$. Using the above formula guarantees that we get the desired bound. The same procedure applies to all the $\Theta$’s. Below is some working code you could use to experiment.&lt;/p&gt;

&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;If&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dimensions&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Theta1&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Theta2&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x11&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Theta3&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x11&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Theta1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INIT_EPSILON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INIT_EPSILON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Theta2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INIT_EPSILON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INIT_EPSILON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Theta3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INIT_EPSILON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INIT_EPSILON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;rand(x,y) is just a function in octave that will initialize a matrix of random real numbers between 0 and 1.&lt;/p&gt;

&lt;p&gt;(Note: the epsilon used above is unrelated to the epsilon from Gradient Checking)&lt;/p&gt;

&lt;h3 id=&quot;724-putting-it-together&quot;&gt;7.2.4 Putting it Together&lt;/h3&gt;

&lt;p&gt;First, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers in total you want to have.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Number of input units = dimension of features $x^{(i)}$&lt;/li&gt;
  &lt;li&gt;Number of output units = number of classes&lt;/li&gt;
  &lt;li&gt;Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)&lt;/li&gt;
  &lt;li&gt;Defaults: 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Training a Neural Network&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Randomly initialize the weights&lt;/li&gt;
  &lt;li&gt;Implement forward propagation to get $h_\Theta(x^{(i)})$ for any $x^{(i)}$&lt;/li&gt;
  &lt;li&gt;Implement the cost function&lt;/li&gt;
  &lt;li&gt;Implement backpropagation to compute partial derivatives&lt;/li&gt;
  &lt;li&gt;Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.&lt;/li&gt;
  &lt;li&gt;Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When we perform forward and back propagation, we loop on every training example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for i = 1:m,
   Perform forward propagation and backpropagation using example (x(i),y(i))
   (Get activations a(l) and delta terms d(l) for l = 2,...,L
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following image gives us an intuition of what is happening as we are implementing our neural network:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tKfTcly1frgukfnqmcj30eu08aq5g.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ideally, you want $h_\Theta(x^{(i)}) \approx y^{(i)}$. This will minimize our cost function. However, keep in mind that $J(\Theta)$ is not convex and thus we can end up in a local minimum instead.&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Apr 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/04/30/ml-week5/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/04/30/ml-week5/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>Machine Learning Week 4</title>
        <description>&lt;h1 id=&quot;6-neural-networks-representation&quot;&gt;6. Neural Networks: Representation&lt;/h1&gt;

&lt;h2 id=&quot;61-neural-networks&quot;&gt;6.1 Neural Networks&lt;/h2&gt;

&lt;h3 id=&quot;611-model-representation-i&quot;&gt;6.1.1 Model Representation I&lt;/h3&gt;

&lt;p&gt;Let’s examine how we will represent a hypothesis function using neural networks. At a very simple level, neurons are basically computational units that take inputs (&lt;strong&gt;dendrites&lt;/strong&gt;) as electrical inputs (called “spikes”) that are channeled to outputs (&lt;strong&gt;axons&lt;/strong&gt;). In our model, our dendrites are like the input features $x_1\cdots x_n$, and the output is the result of our hypothesis function. In this model our $x_0$ input node is sometimes called the “bias unit.” It is always equal to 1. In neural networks, we use the same logistic function as in classification, $\frac{1}{1 + e^{-\theta^Tx}}$, yet we sometimes call it a sigmoid (logistic) &lt;strong&gt;activation&lt;/strong&gt; function. In this situation, our “theta” parameters are sometimes called “weights”.&lt;/p&gt;

&lt;p&gt;Visually, a simplistic representation looks like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline \end{bmatrix}\rightarrow\begin{bmatrix}\ \ \ \newline \end{bmatrix}\rightarrow h_\theta(x)&lt;/script&gt;

&lt;p&gt;Our input nodes (layer 1), also known as the “input layer”, go into another node (layer 2), which finally outputs the hypothesis function, known as the “output layer”.&lt;/p&gt;

&lt;p&gt;We can have intermediate layers of nodes between the input and output layers called the “hidden layers.”&lt;/p&gt;

&lt;p&gt;In this example, we label these intermediate or “hidden” layer nodes $a^2_0 \cdots a^2_n$and call them “activation units.”&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; a_i^{(j)} = \text{&quot;activation&quot; of unit $i$ in layer $j$} \newline&amp; \Theta^{(j)} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$}\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;If we had one hidden layer, it would look like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline x_3\end{bmatrix}\rightarrow\begin{bmatrix}a_1^{(2)} \newline a_2^{(2)} \newline a_3^{(2)} \newline \end{bmatrix}\rightarrow h_\theta(x)&lt;/script&gt;

&lt;p&gt;The values for each of the “activation” nodes is obtained as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*}&lt;/script&gt;

&lt;p&gt;This is saying that we compute our activation nodes by using a 3×4 matrix of parameters. We apply each row of the parameters to our inputs to obtain the value for one activation node. Our hypothesis output is the logistic function applied to the sum of the values of our activation nodes, which have been multiplied by yet another parameter matrix $\Theta^{(2)}$ containing the weights for our second layer of nodes.&lt;/p&gt;

&lt;p&gt;Each layer gets its own matrix of weights, $\Theta^{(j)}$.&lt;/p&gt;

&lt;p&gt;The dimensions of these matrices of weights is determined as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{If network has }{s_j}\text{ units in layer j and }{s_{j+1}}\text{ units in layer j+1, then }{\Theta^{(j)}}\text{ will be of dimension }{s_{j+1} \times (s_j + 1)}\text{ .}&lt;/script&gt;

&lt;p&gt;The +1 comes from the addition in $\Theta^{(j)}$ of the “bias nodes,” $x_0$ and $\Theta_0^{(j)}$. In other words the output nodes will not include the bias nodes while the inputs will. The following image summarizes our model representation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcly1fr2jxyyawgj30jk0ay772.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Example: If layer 1 has 2 input nodes and layer 2 has 4 activation nodes. Dimension of $\Theta^{(1)}$ is going to be 4×3 where $ s_j = 2$ and $s_{j+1} = 4$, so $s_{j+1} \times (s_j + 1) = 4 \times 3$.&lt;/p&gt;

&lt;h3 id=&quot;612-model-representation-ii&quot;&gt;6.1.2 Model Representation II&lt;/h3&gt;

&lt;p&gt;To re-iterate, the following is an example of a neural network:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*}&lt;/script&gt;

&lt;p&gt;In this section we’ll do a vectorized implementation of the above functions. We’re going to define a new variable $z_k^{(j)}$zk(j) that encompasses the parameters inside our g function. In our previous example if we replaced by the variable z for all the parameters we would get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}a_1^{(2)} = g(z_1^{(2)}) \newline a_2^{(2)} = g(z_2^{(2)}) \newline a_3^{(2)} = g(z_3^{(2)}) \newline \end{align*}&lt;/script&gt;

&lt;p&gt;In other words, for layer j=2 and node k, the variable z will be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_k^{(2)}=\Theta_{k,0}^{(1)}x_0+\Theta_{k,1}^{(1)}x_1+\cdots+\Theta_{k,n}^{(1)}x_n&lt;/script&gt;

&lt;p&gt;The vector representation of x and $z^{j}$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}x = \begin{bmatrix}x_0 \newline x_1 \newline\cdots \newline x_n\end{bmatrix} &amp;z^{(j)} = \begin{bmatrix}z_1^{(j)} \newline z_2^{(j)} \newline\cdots \newline z_n^{(j)}\end{bmatrix}\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Setting $x = a^{(1)}$, we can rewrite the equation as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z^{(j)}=\Theta^{(j-1)}a^{(j-1)}&lt;/script&gt;

&lt;p&gt;We are multiplying our matrix $\Theta^{(j-1)}$ with dimensions $s_j\times (n+1)$ (where s_jsj is the number of our activation nodes) by our vector $a^{(j-1)}$ with height (n+1). This gives us our vector $z^{(j)}$ with height $s_j$. Now we can get a vector of our activation nodes for layer j as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a^{(j+1)}=\Theta^{(j)}a^{(j)}&lt;/script&gt;

&lt;p&gt;We get this final z vector by multiplying the next theta matrix after $\Theta^{(j-1)}$ with the values of all the activation nodes we just got. This last theta matrix $\Theta^{(j)}$ will have only &lt;strong&gt;one row&lt;/strong&gt; which is multiplied by one column $a^{(j)}$ so that our result is a single number. We then get our final result with:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\Theta(x)=a^{(j+1)}=g(z^{(j+1)})&lt;/script&gt;

&lt;p&gt;Notice that in this &lt;strong&gt;last step&lt;/strong&gt;, between layer j and layer j+1, we are doing &lt;strong&gt;exactly the same thing&lt;/strong&gt; as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypotheses.&lt;/p&gt;

&lt;h2 id=&quot;62-applications&quot;&gt;6.2 Applications&lt;/h2&gt;

&lt;h3 id=&quot;621-examples-and-intuitions-i&quot;&gt;6.2.1 Examples and Intuitions I&lt;/h3&gt;

&lt;p&gt;A simple example of applying neural networks is by predicting $x_1$ AND $x_2$, which is the logical ‘and’ operator and is only true if both $x_1$ and $x_2$ are 1.&lt;/p&gt;

&lt;p&gt;The graph of our functions will look like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}\begin{bmatrix}x_0 \newline x_1 \newline x_2\end{bmatrix} \rightarrow\begin{bmatrix}g(z^{(2)})\end{bmatrix} \rightarrow h_\Theta(x)\end{align*}&lt;/script&gt;

&lt;p&gt;Remember that $x_0$ is our bias variable and is always 1.&lt;/p&gt;

&lt;p&gt;Let’s set our first theta matrix as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Theta^{(1)}=[-30\ 20\ 20]&lt;/script&gt;

&lt;p&gt;This will cause the output of our hypothesis to only be positive if both $x_1$ and $x_2$ are 1. In other words:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; h_\Theta(x) = g(-30 + 20x_1 + 20x_2) \newline \newline &amp; x_1 = 0 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-30) \approx 0 \newline &amp; x_1 = 0 \ \ and \ \ x_2 = 1 \ \ then \ \ g(-10) \approx 0 \newline &amp; x_1 = 1 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-10) \approx 0 \newline &amp; x_1 = 1 \ \ and \ \ x_2 = 1 \ \ then \ \ g(10) \approx 1\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;So we have constructed one of the fundamental operations in computers by using a small neural network rather than using an actual AND gate. Neural networks can also be used to simulate all the other logical gates. The following is an example of the logical operator ‘OR’, meaning either $x_1$ is true or $x_2$ is true, or both:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tKfTcly1fr2qx34y7mj30gb07raaw.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where g(z) is the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tKfTcly1fr2qx34y7mj30gb07raaw.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;622-examples-and-intuitions-ii&quot;&gt;6.2.2 Examples and Intuitions II&lt;/h3&gt;

&lt;p&gt;The $Θ^{(1)}$ matrices for AND, NOR, and OR are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}AND:\newline\Theta^{(1)} &amp;=\begin{bmatrix}-30 &amp; 20 &amp; 20\end{bmatrix} \newline NOR:\newline\Theta^{(1)} &amp;= \begin{bmatrix}10 &amp; -20 &amp; -20\end{bmatrix} \newline OR:\newline\Theta^{(1)} &amp;= \begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix} \newline\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can combine these to get the XNOR logical operator (which gives 1 if $x_1$ and $x_2$ are both 0 or both 1).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}\begin{bmatrix}x_0 \newline x_1 \newline x_2\end{bmatrix} \rightarrow\begin{bmatrix}a_1^{(2)} \newline a_2^{(2)} \end{bmatrix} \rightarrow\begin{bmatrix}a^{(3)}\end{bmatrix} \rightarrow h_\Theta(x)\end{align*}&lt;/script&gt;

&lt;p&gt;For the transition between the first and second layer, we’ll use a $Θ^{(1)}$ matrix that combines the values for AND and NOR:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\Theta^{(1)} =\begin{bmatrix}-30 &amp; 20 &amp; 20 \newline 10 &amp; -20 &amp; -20\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;For the transition between the second and third layer, we’ll use a $Θ^{(2)}$ matrix that uses the value for OR:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Theta^{(2)}=[-10\ 20\ 20]&lt;/script&gt;

&lt;p&gt;Let’s write out the values for all our nodes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; a^{(2)} = g(\Theta^{(1)} \cdot x) \newline&amp; a^{(3)} = g(\Theta^{(2)} \cdot a^{(2)}) \newline&amp; h_\Theta(x) = a^{(3)}\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;And there we have the XNOR operator using a hidden layer with two nodes! The following summarizes the above algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcly1fr2rikyh1ij30hb09h40o.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;623-multiclass-classification&quot;&gt;6.2.3 Multiclass Classification&lt;/h3&gt;

&lt;p&gt;To classify data into multiple classes, we let our hypothesis function return a vector of values. Say we wanted to classify our data into one of four categories. We will use the following example to see how this classification is done. This algorithm takes as input an image and classifies it accordingly:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcly1fr2rxa2aybj30h309en0f.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can define our set of resulting classes as y:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y^{(i)}=\begin {bmatrix}1\\0\\0\\0\end {bmatrix} ,\begin {bmatrix}0\\1\\0\\0\end {bmatrix} ,\begin {bmatrix}0\\0\\1\\0\end {bmatrix} ,\begin {bmatrix}0\\0\\0\\1\end {bmatrix}&lt;/script&gt;

&lt;p&gt;Each $y^{(i)}$ represents a different image corresponding to either a car, pedestrian, truck, or motorcycle. The inner layers, each provide us with some new information which leads to our final hypothesis function. The setup looks like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin {bmatrix}x_0\\x_1\\x_2\\\cdots\\x_n\end {bmatrix} \rightarrow \begin {bmatrix}a_0^{(2)}\\a_1^{(2)}\\a_2^{(2)}\\\cdots\end {bmatrix} \rightarrow \begin {bmatrix}a_0^{(3)}\\a_1^{(3)}\\a_2^{(3)}\\\cdots\end {bmatrix} \rightarrow \cdots \rightarrow \begin {bmatrix}h_\Theta(x)_1\\h_\Theta(x)_2\\h_\Theta(x)_3\\h_\Theta(x)_4\end {bmatrix}&lt;/script&gt;

&lt;p&gt;Our resulting hypothesis for one set of inputs may look like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\Theta(x) =\begin{bmatrix}0 \newline 0 \newline 1 \newline 0 \newline\end{bmatrix}&lt;/script&gt;

&lt;p&gt;In which case our resulting class is the third one down, or $h_\Theta(x)_3$, which represents the motorcycle.&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Apr 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/04/30/ml-week4/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/04/30/ml-week4/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>Machine Learning Week 3</title>
        <description>&lt;h1 id=&quot;4-logistic-regression&quot;&gt;4. Logistic Regression&lt;/h1&gt;

&lt;h2 id=&quot;41-classification-and-representation&quot;&gt;4.1 Classification and Representation&lt;/h2&gt;

&lt;h3 id=&quot;411-classification&quot;&gt;4.1.1 Classification&lt;/h3&gt;

&lt;p&gt;To attempt classification, one method is to use linear regression and map all predictions greater than 0.5 as a 1 and all less than 0.5 as a 0. However, this method doesn’t work well because classification is not actually a linear function.&lt;/p&gt;

&lt;p&gt;The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values. For now, we will focus on the &lt;strong&gt;binary classification&lt;/strong&gt; &lt;strong&gt;problem&lt;/strong&gt; in which y can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.) For instance, if we are trying to build a spam classifier for email, then $x^{(i)}$ may be some features of a piece of email, and y may be 1 if it is a piece of spam mail, and 0 otherwise. Hence, y∈{0,1}. 0 is also called the negative class, and 1 the positive class, and they are sometimes also denoted by the symbols “-” and “+.” Given $x^{(i)}$, the corresponding $y^{(i)}$ is also called the label for the training example.&lt;/p&gt;

&lt;h3 id=&quot;412-hypothesis-representation&quot;&gt;4.1.2 Hypothesis Representation&lt;/h3&gt;

&lt;p&gt;We could approach the classification problem ignoring the fact that y is discrete-valued, and use our old linear regression algorithm to try to predict y given x. However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn’t make sense for hθ(x) to take values larger than 1 or smaller than 0 when we know that y ∈ {0, 1}. To fix this, let’s change the form for our hypotheses hθ(x)to satisfy 0≤hθ(x)≤1. This is accomplished by plugging θTx into the Logistic Function.&lt;/p&gt;

&lt;p&gt;Our new form uses the “Sigmoid Function,” also called the “Logistic Function”:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; h_\theta (x) = g ( \theta^T x ) \newline \newline&amp; z = \theta^T x \newline&amp; g(z) = \dfrac{1}{1 + e^{-z}}\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The following image shows us what the sigmoid function looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcly1fquoucaittj30my03k74g.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The function g(z), shown here, maps any real number to the (0, 1) interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification.&lt;/p&gt;

&lt;p&gt;hθ(x) will give us the &lt;strong&gt;probability&lt;/strong&gt; that our output is 1. For example, $h_θ(x)=0.7$ gives us a probability of 70% that our output is 1. Our probability that our prediction is 0 is just the complement of our probability that it is 1 (e.g. if probability that it is 1 is 70%, then the probability that it is 0 is 30%).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align*} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;413-decision-boundary&quot;&gt;4.1.3 Decision Boundary&lt;/h3&gt;

&lt;p&gt;In order to get our discrete 0 or 1 classification, we can translate the output of the hypothesis function as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; h_\theta(x) \geq 0.5 \rightarrow y = 1 \newline&amp; h_\theta(x) &lt; 0.5 \rightarrow y = 0 \newline\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The way our logistic function g behaves is that when its input is greater than or equal to zero, its output is greater than or equal to 0.5:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; g(z) \geq 0.5 \newline&amp; when \; z \geq 0\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Remember.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}z=0, e^{0}=1 \Rightarrow g(z)=1/2\newline z \to \infty, e^{-\infty} \to 0 \Rightarrow g(z)=1 \newline z \to -\infty, e^{\infty}\to \infty \Rightarrow g(z)=0 \end{align*}&lt;/script&gt;

&lt;p&gt;So if our input to g is θTX, then that means:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; h_\theta(x) = g(\theta^T x) \geq 0.5 \newline&amp; when \; \theta^T x \geq 0\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;From these statements we can now say:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; \theta^T x \geq 0 \Rightarrow y = 1 \newline&amp; \theta^T x &lt; 0 \Rightarrow y = 0 \newline\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The &lt;strong&gt;decision boundary&lt;/strong&gt; is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; \theta = \begin{bmatrix}5 \newline -1 \newline 0\end{bmatrix} \newline &amp; y = 1 \; if \; 5 + (-1) x_1 + 0 x_2 \geq 0 \newline &amp; 5 - x_1 \geq 0 \newline &amp; - x_1 \geq -5 \newline&amp; x_1 \leq 5 \newline \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In this case, our decision boundary is a straight vertical line placed on the graph where $x_1=5$, and everything to the left of that denotes y = 1, while everything to the right denotes y = 0.&lt;/p&gt;

&lt;p&gt;Again, the input to the sigmoid function g(z) (e.g. $θ^TX$) doesn’t need to be linear, and could be a function that describes a circle (e.g. $z=θ_0+θ_1x^2_1+θ_2x^2_2$) or any shape to fit our data.&lt;/p&gt;

&lt;h2 id=&quot;42-logistic-regression-model&quot;&gt;4.2 Logistic Regression Model&lt;/h2&gt;

&lt;h3 id=&quot;421-cost-function&quot;&gt;4.2.1 Cost Function&lt;/h3&gt;

&lt;p&gt;We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.&lt;/p&gt;

&lt;p&gt;Instead, our cost function for logistic regression looks like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; &amp; \text{if y = 1} \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; &amp; \text{if y = 0}\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;When y = 1, we get the following plot for $J(θ)$ vs $h_θ(x)$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fqx182bk1vj308c06s0t7.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similarly, when y = 0, we get the following plot for $J(θ)$ vs $h_θ(x)$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fqx1839j8fj308b07sdg8.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; \mathrm{Cost}(h_\theta(x),y) = 0 \text{ if } h_\theta(x) = y \newline &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 0 \; \mathrm{and} \; h_\theta(x) \rightarrow 1 \newline &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 1 \; \mathrm{and} \; h_\theta(x) \rightarrow 0 \newline \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;If our correct answer ‘y’ is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity.&lt;/p&gt;

&lt;p&gt;If our correct answer ‘y’ is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity.&lt;/p&gt;

&lt;p&gt;Note that writing the cost function in this way guarantees that J(θ) is convex for logistic regression.&lt;/p&gt;

&lt;h3 id=&quot;422-simplified-cost-function-and-gradient-descent&quot;&gt;4.2.2 Simplified Cost Function and Gradient Descent&lt;/h3&gt;

&lt;p&gt;We can compress our cost function’s two conditional cases into one case:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Cost(h_θ(x),y)=−ylog(h_θ(x))−(1−y)log(1−h_θ(x))&lt;/script&gt;

&lt;p&gt;Notice that when y is equal to 1, then the second term (1−y)log(1−hθ(x)) will be zero and will not affect the result. If y is equal to 0, then the first term −ylog(hθ(x)) will be zero and will not affect the result.&lt;/p&gt;

&lt;p&gt;We can fully write out our entire cost function as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]&lt;/script&gt;

&lt;p&gt;A vectorized implementation is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} &amp; h = g(X\theta)\newline &amp; J(\theta) = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right) \end{align*} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h4&gt;

&lt;p&gt;Remember that the general form of gradient descent is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \newline &amp; \rbrace\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can work out the derivative part using calculus to get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} &amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline &amp; \rbrace \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Notice that this algorithm is identical to the one we used in linear regression. We still have to simultaneously update all values in theta.&lt;/p&gt;

&lt;p&gt;A vectorized implementation is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;θ:=θ−\frac{α}{m}X^T(g(Xθ)−\hat{y})&lt;/script&gt;

&lt;h3 id=&quot;423-advanced-optimization&quot;&gt;4.2.3 Advanced Optimization&lt;/h3&gt;

&lt;p&gt;“Conjugate gradient”, “BFGS”, and “L-BFGS” are more sophisticated, faster ways to optimize $θ$ that can be used instead of gradient descent. We suggest that you should not write these more sophisticated algorithms yourself (unless you are an expert in numerical computing) but use the libraries instead, as they’re already tested and highly optimized. Octave provides them.&lt;/p&gt;

&lt;p&gt;We first need to provide a function that evaluates the following two functions for a given input value θ:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(θ)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{∂}{∂θ_j}J(θ)&lt;/script&gt;

&lt;p&gt;We can write a single function that returns both of these:&lt;/p&gt;

&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jVal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;costFunction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;jVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;code to compute J(theta)...];&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;code to compute derivative of J(theta)...];&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we can use octave’s “fminunc()” optimization algorithm along with the “optimset()” function that creates an object containing the options we want to send to “fminunc()”.&lt;/p&gt;

&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;optimset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'GradObj'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'on'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'MaxIter'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;initialTheta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optTheta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;functionVal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exitFlag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fminunc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;costFunction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialTheta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
     &lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We give to the function “fminunc()” our cost function, our initial vector of theta values, and the “options” object that we created beforehand.&lt;/p&gt;

&lt;h2 id=&quot;43-multiclass-classification&quot;&gt;4.3 Multiclass Classification&lt;/h2&gt;

&lt;h3 id=&quot;431multiclass-classification-one-vs-all&quot;&gt;4.3.1Multiclass Classification: One-vs-all&lt;/h3&gt;

&lt;p&gt;Now we will approach the classification of data when we have more than two categories. Instead of y = {0,1} we will expand our definition so that y = {0,1…n}.&lt;/p&gt;

&lt;p&gt;Since y = {0,1…n}, we divide our problem into n+1 (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that ‘y’ is a member of one of our classes.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; y \in \lbrace0, 1 ... n\rbrace \newline&amp; h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \newline&amp; h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \newline&amp; \cdots \newline&amp; h_\theta^{(n)}(x) = P(y = n | x ; \theta) \newline&amp; \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\newline\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We are basically choosing one class and then lumping all the others into a single second class. We do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.&lt;/p&gt;

&lt;p&gt;The following image shows how one could classify 3 classes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fqx182rhj0j30d507agmp.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To summarize:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Train a logistic regression classifier $h_θ(x)$ for each class￼ to predict the probability that ￼ ￼y = i￼ ￼.&lt;/p&gt;

&lt;p&gt;To make a prediction on a new x, pick the class ￼that maximizes $h_θ(x)$&lt;/p&gt;

&lt;h1 id=&quot;5-regularization&quot;&gt;5. Regularization&lt;/h1&gt;

&lt;h2 id=&quot;51-solving-the-problem-of-overfitting&quot;&gt;5.1 Solving the Problem of Overfitting&lt;/h2&gt;

&lt;h3 id=&quot;511-the-problem-of-overfitting&quot;&gt;5.1.1 The Problem of Overfitting&lt;/h3&gt;

&lt;p&gt;Consider the problem of predicting y from x ∈ R. The leftmost figure below shows the result of fitting a $y = θ_0+θ_1x$ to a dataset. We see that the data doesn’t really lie on straight line, and so the fit is not very good.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fqy3d4x38dj30f0046dg2.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Instead, if we had added an extra feature $x^2$ , and fit $y=θ_0+θ_1x+θ_2x^2$ , then we obtain a slightly better fit to the data (See middle figure). Naively, it might seem that the more features we add, the better. However, there is also a danger in adding too many features: The rightmost figure is the result of fitting a 5th order polynomial $y=∑^{5}_{j=0}θ_jx^j$. We see that even though the fitted curve passes through the data perfectly, we would not expect this to be a very good predictor of, say, housing prices (y) for different living areas (x). Without formally defining what these terms mean, we’ll say the figure on the left shows an instance of &lt;strong&gt;underfitting&lt;/strong&gt;—in which the data clearly shows structure not captured by the model—and the figure on the right is an example of &lt;strong&gt;overfitting&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Underfitting, or high bias, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. At the other extreme, overfitting, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.&lt;/p&gt;

&lt;p&gt;This terminology is applied to both linear and logistic regression. There are two main options to address the issue of overfitting:&lt;/p&gt;

&lt;p&gt;1) Reduce the number of features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Manually select which features to keep.&lt;/li&gt;
  &lt;li&gt;Use a model selection algorithm (studied later in the course).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2) Regularization&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Keep all the features, but reduce the magnitude of parameters $θ_j$.&lt;/li&gt;
  &lt;li&gt;Regularization works well when we have a lot of slightly useful features.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;512-cost-function&quot;&gt;5.1.2 Cost Function&lt;/h3&gt;

&lt;p&gt;If we have overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost.&lt;/p&gt;

&lt;p&gt;Say we wanted to make the following function more quadratic:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;θ_0+θ_1x+θ_2x^2+θ_3x^3+θ_4x^4&lt;/script&gt;

&lt;p&gt;We’ll want to eliminate the influence of $θ_3x^3$ and $θ_4x^4$ . Without actually getting rid of these features or changing the form of our hypothesis, we can instead modify our &lt;strong&gt;cost function&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;min_θ \frac{1}{2m}∑^m_{i=1}(h_θ(x^{(i)})−y^{(i)})^2+1000⋅θ^2_3+1000⋅θ^2_4&lt;/script&gt;

&lt;p&gt;We’ve added two extra terms at the end to inflate the cost of $θ_3$ and $θ_4$. Now, in order for the cost function to get close to zero, we will have to reduce the values of $θ_3$ and $θ_4$ to near zero. This will in turn greatly reduce the values of $θ_3x^3$ and $θ_4x^4$ in our hypothesis function. As a result, we see that the new hypothesis (depicted by the pink curve) looks like a quadratic function but fits the data better due to the extra small terms $θ_3x^3$ and $θ_4x^4$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcly1fr0rrwonzxj30gh091wfl.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We could also regularize all of our theta parameters in a single summation as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;min_\theta\ \dfrac{1}{2m}\  [ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2 ]&lt;/script&gt;

&lt;p&gt;The $\lambda$, or lambda, is the &lt;strong&gt;regularization parameter&lt;/strong&gt;. It determines how much the costs of our theta parameters are inflated.&lt;/p&gt;

&lt;p&gt;Using the above cost function with the extra summation, we can smooth the output of our hypothesis function to reduce overfitting. If lambda is chosen to be too large, it may smooth out the function too much and cause underfitting. Hence, what would happen if $λ=0$ or is too small ? If $\lambda$ is very small, $\theta$ will not be restricted. So it will cause overfitting.&lt;/p&gt;

&lt;h3 id=&quot;513-regularized-linear-regression&quot;&gt;5.1.3 Regularized Linear Regression&lt;/h3&gt;

&lt;p&gt;We can apply regularization to both linear regression and logistic regression. We will approach linear regression first.&lt;/p&gt;

&lt;h4 id=&quot;gradient-descent-1&quot;&gt;Gradient Descent&lt;/h4&gt;

&lt;p&gt;We will modify our gradient descent function to separate out $\theta_0$ from the rest of the parameters because we do not want to penalize $\theta_0$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} &amp; \text{Repeat}\ \lbrace \newline &amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline &amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline &amp; \rbrace \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The term $\frac{λ}{m}θ_j$ performs our regularization. With some manipulation our update rule can also be represented as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;θ_j:=θ_j(1−α\frac{λ}{m})−α\frac{1}{m}∑^m_{i=1}(h_θ(x^{(i)})−y^{(i)})x^{(i)}_j&lt;/script&gt;

&lt;p&gt;The first term in the above equation, $1−α\frac{λ}m$ will always be less than 1. Intuitively you can see it as reducing the value of $θ_j$ by some amount on every update. Notice that the second term is now exactly the same as it was before.&lt;/p&gt;

&lt;h4 id=&quot;normal-equation&quot;&gt;&lt;strong&gt;Normal Equation&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;Now let’s approach regularization using the alternate method of the non-iterative normal equation.&lt;/p&gt;

&lt;p&gt;To add in regularization, the equation is the same as our original, except that we add another term inside the parentheses:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline&amp; \text{where}\ \ L = \begin{bmatrix} 0 &amp; &amp; &amp; &amp; \newline &amp; 1 &amp; &amp; &amp; \newline &amp; &amp; 1 &amp; &amp; \newline &amp; &amp; &amp; \ddots &amp; \newline &amp; &amp; &amp; &amp; 1 \newline\end{bmatrix}\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;L is a matrix with 0 at the top left and 1’s down the diagonal, with 0’s everywhere else. It should have dimension (n+1)×(n+1). Intuitively, this is the identity matrix (though we are not including $x_0$), multiplied with a single real number λ.&lt;/p&gt;

&lt;p&gt;Recall that if m &amp;lt; n, then $X^TX$ is non-invertible. However, when we add the term λ⋅L, then $X^TX + λ⋅L$ becomes invertible.&lt;/p&gt;

&lt;h3 id=&quot;514-regularized-logistic-regression&quot;&gt;5.1.4 Regularized Logistic Regression&lt;/h3&gt;

&lt;p&gt;We can regularize logistic regression in a similar way that we regularize linear regression. As a result, we can avoid overfitting. The following image shows how the regularized function, displayed by the pink line, is less likely to overfit than the non-regularized function represented by the blue line:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcly1fr0rr7x64lj30do07edh2.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;cost-function&quot;&gt;Cost Function&lt;/h4&gt;

&lt;p&gt;Recall that our cost function for logistic regression was:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(θ)=−\frac1m∑^m_{i=1}[y^{(i)} log(h_θ(x^{(i)}))+(1−y^{(i)}) log(1−h_θ(x^{(i)}))]&lt;/script&gt;

&lt;p&gt;We can regularize this equation by adding a term to the end:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2&lt;/script&gt;

&lt;p&gt;The second sum, $∑^n_{j=1}θ^2_j$ &lt;strong&gt;means to explicitly exclude&lt;/strong&gt; the bias term, $θ_0$. I.e. the θ vector is indexed from 0 to n (holding n+1 values, $θ_0$ through $θ_n$), and this sum explicitly skips $θ_0$, by running from 1 to n, skipping 0. Thus, when computing the equation, we should continuously update the two following equations:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fqy6zag3yhj30de06h75k.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Apr 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/04/30/ml-week3/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/04/30/ml-week3/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>Machine Learning Week 2</title>
        <description>&lt;h1 id=&quot;3-linear-regression-with-multiple-variables&quot;&gt;3. Linear Regression with Multiple Variables&lt;/h1&gt;

&lt;h2 id=&quot;31-multivariate-linear-regression&quot;&gt;3.1 Multivariate Linear Regression&lt;/h2&gt;

&lt;h3 id=&quot;311-multiple-features&quot;&gt;3.1.1 Multiple Features&lt;/h3&gt;

&lt;p&gt;Linear regression with multiple variables is also known as “multivariate linear regression”.&lt;/p&gt;

&lt;p&gt;We now introduce notation for equations where we can have any number of input variables.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}x_j^{(i)} &amp;= \text{value of feature } j \text{ in the }i^{th}\text{ training example} \newline x^{(i)}&amp; = \text{the column vector of all the feature inputs of the }i^{th}\text{ training example} \newline m &amp;= \text{the number of training examples} \newline n &amp;= \left| x^{(i)} \right| ; \text{(the number of features)} \end{align*}​ %]]&gt;&lt;/script&gt;

&lt;p&gt;The multivariable form of the hypothesis function accommodating these multiple features is as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_θ(x)=θ_0+θ_1x_1+θ_2x_2+θ_3x_3+⋯+θ_nx_n&lt;/script&gt;

&lt;p&gt;In order to develop intuition about this function, we can think about $θ_0$ as the basic price of a house, $θ_1$ as the price per square meter, $θ_2$ as the price per floor, etc. $x_1$ will be the number of square meters in the house, $x_2$ the number of floors, etc.&lt;/p&gt;

&lt;p&gt;Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
h_θ(x)=\begin{bmatrix}θ_0&amp;θ_1&amp;…&amp;θ_n\end{bmatrix}\begin{bmatrix}x_0\\x_1\\…\\x_n\end{bmatrix}=θ^Tx %]]&gt;&lt;/script&gt;

&lt;p&gt;This is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more.&lt;/p&gt;

&lt;p&gt;Remark: Note that for convenience reasons in this course we assume $x^{(i)}_0=1$ for $(i∈1,…,m)$.&lt;/p&gt;

&lt;p&gt;[&lt;strong&gt;Note&lt;/strong&gt;: So that we can do matrix operations with theta and x, we will set $x^{(i)}&lt;em&gt;0 = 1$, for all values of i. This makes the two vectors ‘theta’ and $x&lt;/em&gt;{(i)}$ match each other element-wise  (that is, have the same number of elements: n+1).]&lt;/p&gt;

&lt;p&gt;The training examples are stored in X row-wise, like such:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}X = \begin{bmatrix}x^{(1)}_0 &amp; x^{(1)}_1  \newline x^{(2)}_0 &amp; x^{(2)}_1  \newline x^{(3)}_0 &amp; x^{(3)}_1 \end{bmatrix}&amp;,\theta = \begin{bmatrix}\theta_0 \newline \theta_1 \newline\end{bmatrix}\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;You can calculate the hypothesis as a column vector of size (m x 1) with:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_θ(X)=Xθ&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;For the rest of these notes, and other lecture notes, X will represent a matrix of training examples&lt;/strong&gt; $x_{(i)}$ &lt;strong&gt;stored row-wise.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;312-cost-function&quot;&gt;3.1.2 Cost function&lt;/h3&gt;

&lt;p&gt;For the parameter vector θ (of type $ℝ^{n+1}$ or in $ℝ^{(n+1)×1}$, the cost function is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(θ)=\frac {1}{2m}∑_{i=1}^m(hθ(x(i))−y(i))^2&lt;/script&gt;

&lt;p&gt;The vectorized version is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(θ)=\frac{1}{2m}(Xθ−\hat y )^T(Xθ−\hat y )&lt;/script&gt;

&lt;p&gt;Where $\hat{y}$  denotes the vector of all y values.&lt;/p&gt;

&lt;h3 id=&quot;313-gradient-descent-for-multiple-variables&quot;&gt;3.1.3 Gradient Descent for Multiple Variables&lt;/h3&gt;

&lt;p&gt;The gradient descent equation itself is generally the same form; we just have to repeat it for our ‘n’ features:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} &amp; \text{repeat until convergence:} \; \lbrace \newline \; &amp; \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_0^{(i)}\newline \; &amp; \theta_1 := \theta_1 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_1^{(i)} \newline \; &amp; \theta_2 := \theta_2 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_2^{(i)} \newline &amp; \cdots \newline \rbrace \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In other words:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; \text{repeat until convergence:} \; \lbrace \newline \; &amp; \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \; &amp; \text{for j := 0...n}\newline \rbrace\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The following image compares gradient descent with one variable to gradient descent with multiple variables:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcly1fqs784zggaj30g508ttam.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;314-gradient-descent-in-practice-i---feature-scaling&quot;&gt;3.1.4 Gradient Descent in Practice I - Feature Scaling&lt;/h3&gt;

&lt;p&gt;We can speed up gradient descent by having each of our input values in roughly the same range. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.&lt;/p&gt;

&lt;p&gt;The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;−1 ≤ x_{(i)} ≤ 1&lt;/script&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;−0.5 ≤ x_{(i)} ≤ 0.5&lt;/script&gt;

&lt;p&gt;These aren’t exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.&lt;/p&gt;

&lt;p&gt;Two techniques to help with this are &lt;strong&gt;feature scaling&lt;/strong&gt; and &lt;strong&gt;mean normalization&lt;/strong&gt;. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;xi:=\frac {x_i−μ_i}{s_i}&lt;/script&gt;

&lt;p&gt;Where $μ_i$ is the &lt;strong&gt;average&lt;/strong&gt; of all the values for feature (i) and $s_i$ is the range of values (max - min), or si is the standard deviation.&lt;/p&gt;

&lt;p&gt;Note that dividing by the range, or dividing by the standard deviation, give different results. The quizzes in this course use range - the programming exercises use standard deviation.&lt;/p&gt;

&lt;p&gt;For example, if xi represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, $xi:=\frac {price−1000}{1900}$.&lt;/p&gt;

&lt;h3 id=&quot;315-gradient-descent-in-practice-ii---learning-rate&quot;&gt;3.1.5 Gradient Descent in Practice II - Learning Rate&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Debugging gradient descent.&lt;/strong&gt; Make a plot with &lt;em&gt;number of iterations&lt;/em&gt; on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Automatic convergence test.&lt;/strong&gt; Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as $10^{−3}$. However in practice it’s difficult to choose this threshold value.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcly1fqscb3p3vsj30eg07wt9v.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcly1fqscb4de3zj30e807pjsd.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To summarize:&lt;/p&gt;

&lt;p&gt;If α is too small: slow convergence.&lt;/p&gt;

&lt;p&gt;If α is too large: ￼may not decrease on every iteration and thus may not converge.&lt;/p&gt;

&lt;h3 id=&quot;316-features-and-polynomial-regression&quot;&gt;3.1.6 Features and Polynomial Regression&lt;/h3&gt;

&lt;p&gt;We can improve our features and the form of our hypothesis function in a couple different ways.&lt;/p&gt;

&lt;p&gt;We can &lt;strong&gt;combine&lt;/strong&gt; multiple features into one. For example, we can combine x1 and x2 into a new feature x3 by taking $x_1⋅x_2$.&lt;/p&gt;

&lt;h4 id=&quot;polynomial-regression&quot;&gt;Polynomial Regression&lt;/h4&gt;

&lt;p&gt;Our hypothesis function need not be linear (a straight line) if that does not fit the data well.&lt;/p&gt;

&lt;p&gt;We can &lt;strong&gt;change the behavior or curve&lt;/strong&gt; of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).&lt;/p&gt;

&lt;p&gt;For example, if our hypothesis function is $h_θ(x)=θ_0+θ_1x_1$ then we can create additional features based on $x_1$, to get the quadratic function $h_θ(x)=θ_0+θ_1x_1+θ_2x^2_1$ or the cubic function $h_θ(x)=θ_0+θ_1x_1+θ_2x^2_1+θ_3x^3_1$&lt;/p&gt;

&lt;p&gt;In the cubic version, we have created new features $x_2$ and $x_3$ where $x_2=x^2_1$ and $x_3=x^3_1$.&lt;/p&gt;

&lt;p&gt;To make it a square root function, we could do: $h_θ(x)=θ_0+θ_1x_1+θ_2\sqrt{x_1}$&lt;/p&gt;

&lt;p&gt;One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important.&lt;/p&gt;

&lt;p&gt;eg. if $x_1$ has range 1 - 1000 then range of $x^2_1$ becomes 1 - 1000000 and that of $x^3_1$ becomes 1 - 1000000000&lt;/p&gt;

&lt;h2 id=&quot;32-computing-parameters-analytically&quot;&gt;3.2 Computing Parameters Analytically&lt;/h2&gt;

&lt;h3 id=&quot;321-normal-equation&quot;&gt;3.2.1 Normal Equation&lt;/h3&gt;

&lt;p&gt;Gradient descent gives one way of minimizing J. Let’s discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. In the “Normal Equation” method, we will minimize J by explicitly taking its derivatives with respect to the $θ_j$ ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;θ=(X^TX)^{−1}X^Ty&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcly1fqsdxv5p9zj30gq09dgn1.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is &lt;strong&gt;no need&lt;/strong&gt; to do feature scaling with the normal equation.&lt;/p&gt;

&lt;p&gt;The following is a comparison of gradient descent and the normal equation:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Gradient Descent&lt;/th&gt;
      &lt;th&gt;Normal Equation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Need to choose alpha&lt;/td&gt;
      &lt;td&gt;No need to choose alpha&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Needs many iterations&lt;/td&gt;
      &lt;td&gt;No need to iterate&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$O (kn^2)$&lt;/td&gt;
      &lt;td&gt;$O (n^3)$, need to calculate inverse of $X^TX$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Works well when n is large&lt;/td&gt;
      &lt;td&gt;Slow if n is very large&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With the normal equation, computing the inversion has complexity $O(n^3)$. So if we have a very large number of features, the normal equation will be slow. In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.&lt;/p&gt;

&lt;h3 id=&quot;322-normal-equation-noninvertibility&quot;&gt;3.2.2 Normal Equation Noninvertibility&lt;/h3&gt;

&lt;p&gt;When implementing the normal equation in octave we want to use the ‘pinv’ function rather than &lt;code class=&quot;highlighter-rouge&quot;&gt;inv&lt;/code&gt; . The &lt;code class=&quot;highlighter-rouge&quot;&gt;pinv&lt;/code&gt; function will give you a value of $θ$ even if $X^TX$ is not invertible.&lt;/p&gt;

&lt;p&gt;If $X^TX$ is &lt;strong&gt;noninvertible,&lt;/strong&gt; the common causes might be having :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Redundant features, where two features are very closely related (i.e. they are linearly dependent)&lt;/li&gt;
  &lt;li&gt;Too many features (e.g. m ≤ n). In this case, delete some features or use “regularization” (to be explained in a later lesson).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Apr 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/04/30/ml-week2/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/04/30/ml-week2/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>Machine Learning Week 1</title>
        <description>&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;h2 id=&quot;11-introduction&quot;&gt;1.1 Introduction&lt;/h2&gt;

&lt;h3 id=&quot;111-what-is-machine-learning&quot;&gt;1.1.1 What is Machine Learning?&lt;/h3&gt;

&lt;p&gt;Two definitions of Machine Learning are offered. Arthur Samuel described it as: “the field of study that gives computers the ability to learn without being explicitly programmed.” This is an older, informal definition.&lt;/p&gt;

&lt;p&gt;Tom Mitchell provides a more modern definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”&lt;/p&gt;

&lt;p&gt;Example: playing checkers.&lt;/p&gt;

&lt;p&gt;E = the experience of playing many games of checkers&lt;/p&gt;

&lt;p&gt;T = the task of playing checkers.&lt;/p&gt;

&lt;p&gt;P = the probability that the program will win the next game.&lt;/p&gt;

&lt;p&gt;In general, any machine learning problem can be assigned to one of two broad classifications:&lt;/p&gt;

&lt;p&gt;Supervised learning and Unsupervised learning.&lt;/p&gt;

&lt;h3 id=&quot;112-supervised-learning&quot;&gt;1.1.2 Supervised Learning&lt;/h3&gt;

&lt;p&gt;In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.&lt;/p&gt;

&lt;p&gt;Supervised learning problems are categorized into “regression” and “classification” problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 1:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.&lt;/p&gt;

&lt;p&gt;We could turn this example into a classification problem by instead making our output about whether the house “sells for more or less than the asking price.” Here we are classifying the houses based on price into two discrete categories.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture&lt;/p&gt;

&lt;p&gt;(b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.&lt;/p&gt;

&lt;h3 id=&quot;113-unsupervised-learning&quot;&gt;1.1.3 Unsupervised Learning&lt;/h3&gt;

&lt;p&gt;Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables.&lt;/p&gt;

&lt;p&gt;We can derive this structure by clustering the data based on relationships among the variables in the data.&lt;/p&gt;

&lt;p&gt;With unsupervised learning there is no feedback based on the prediction results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.&lt;/p&gt;

&lt;p&gt;Non-clustering: The “Cocktail Party Algorithm”, allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a &lt;a href=&quot;https://en.wikipedia.org/wiki/Cocktail_party_effect&quot;&gt;cocktail party&lt;/a&gt;).&lt;/p&gt;

&lt;h1 id=&quot;2-linear-regression-with-one-variable&quot;&gt;2. Linear Regression with One Variable&lt;/h1&gt;

&lt;h2 id=&quot;21-model-and-cost-function&quot;&gt;2.1 Model and Cost Function&lt;/h2&gt;

&lt;h3 id=&quot;211-model-representation&quot;&gt;2.1.1 Model Representation&lt;/h3&gt;

&lt;p&gt;To establish notation for future use, we’ll use $x^{(i)}$ to denote the “input” variables (living area in this example), also called input features, and $y{(i)}$ to denote the “output” or target variable that we are trying to predict (price). A pair $(x^{(i)},y^{(i)})$ is called a training example, and the dataset that we’ll be using to learn—a list of m training examples $(x^{(i)},y^{(i)});i=1,…,m$—is called a training set. Note that the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation. We will also use X to denote the space of input values, and Y to denote the space of output values. In this example, X = Y = ℝ.&lt;/p&gt;

&lt;p&gt;To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis. Seen pictorially, the process is therefore like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fqqdi3zexhj30az078wes.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the target variable that we’re trying to predict is continuous, such as in our housing example, we call the learning problem a regression problem. When y can take on only a small number of discrete values (such as if, given the living area, we wanted to predict if a dwelling is a house or an apartment, say), we call it a classification problem.&lt;/p&gt;

&lt;h3 id=&quot;212-the-hypothesis-function&quot;&gt;2.1.2 The Hypothesis Function&lt;/h3&gt;

&lt;p&gt;Our hypothesis function has the general form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}=h_θ(x)=θ_0+θ_1x&lt;/script&gt;

&lt;p&gt;Note that this is like the equation of a straight line. We give to $h_θ(x)$ values for $θ_0$ and $θ_1$ to get our estimated output $\hat{y}$. In other words, we are trying to create a function called $h_θ$ that is trying to map our input data (the x’s) to our output data (the y’s).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose we have the following set of training data:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;input x&lt;/th&gt;
      &lt;th&gt;output y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Now we can make a random guess about our hθ function: $θ_0=2$ and $θ_1=2$. The hypothesis function becomes $h_θ(x)=2+2x$.&lt;/p&gt;

&lt;p&gt;So for input of 1 to our hypothesis, y will be 4. This is off by 3. Note that we will be trying out various values of $θ_0$ and $θ_1$ to try to find values which provide the best possible “fit” or the most representative “straight line” through the data points mapped on the x-y plane.&lt;/p&gt;

&lt;h3 id=&quot;213-cost-function&quot;&gt;2.1.3 Cost Function&lt;/h3&gt;

&lt;p&gt;We can measure the accuracy of our hypothesis function by using a &lt;strong&gt;cost function&lt;/strong&gt;. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x’s and the actual output y’s.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(θ_0,θ_1)=\frac{1}{2m}\sum_{i=1}^{m}(\widehat{y}_i−y_i)^2=\frac{1}{2m}\sum_{i=1}^{m}(h_θ(x_i)−y_i)^2&lt;/script&gt;

&lt;p&gt;To break it apart, it is $\frac{1}{2} \bar x$ where $\bar x$ is the mean of the squares of $h_θ(x_i)−y_i$ , or the difference between the predicted value and the actual value.&lt;/p&gt;

&lt;p&gt;This function is otherwise called the “Squared error function”, or “Mean squared error”. The mean is halved $(\frac{1}{2m})$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $(\frac{1}{2})$ term. The following image summarizes what the cost function does:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fqqdhwkrbhj30i60aatak.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;214-cost-function---intuition-i&quot;&gt;2.1.4 Cost Function - Intuition I&lt;/h3&gt;

&lt;p&gt;If we try to think of it in visual terms, our training data set is scattered on the x-y plane. We are trying to make a straight line (defined by $h_θ(x)$) which passes through these scattered data points.&lt;/p&gt;

&lt;p&gt;Our objective is to get the best possible line. The best possible line will be such so that the average squared vertical distances of the scattered points from the line will be the least. Ideally, the line should pass through all the points of our training data set. In such a case, the value of $J(θ_0,θ_1)$ will be 0. The following example shows the ideal situation where we have a cost function of 0.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fqqe0hm9hyj30bf069dgo.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When $θ_1=1$, we get a slope of 1 which goes through every single data point in our model. Conversely, when $θ_1=0.5$, we see the vertical distance from our fit to the data points increase.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fqqe0kdkqrj30ba06adgi.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This increases our cost function to 0.58. Plotting several other points yields to the following graph:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fqqe0mpd6jj308g081dge.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thus as a goal, we should try to minimize the cost function. In this case, $θ_1=1$ is our global minimum.&lt;/p&gt;

&lt;h3 id=&quot;215-cost-function---intuition-ii&quot;&gt;2.1.5 Cost Function - Intuition II&lt;/h3&gt;

&lt;p&gt;A contour plot is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line. An example of such a graph is the one to the right below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fqqyl9uw92j30ic09qgo3.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Taking any color and going along the ‘circle’, one would expect to get the same value of the cost function. For example, the three green points found on the green line above have the same value for $J(θ_0,θ_1)$ and as a result, they are found along the same line. The circled x displays the value of the cost function for the graph on the left when $θ_0 = 800$ and $θ_1= -0.15$. Taking another h(x) and plotting its contour plot, one gets the following graphs:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fqqylauengj30i50a476m.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When $θ_0 = 360$ and $θ_1 = 0$, the value of $J(θ_0,θ_1)$ in the contour plot gets closer to the center thus reducing the cost function error. Now giving our hypothesis function a slightly positive slope results in a better fit of the data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fqqyl8lrv8j30hy08qgns.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The graph above minimizes the cost function as much as possible and consequently, the result of $θ_1$ and $θ_0$ tend to be around 0.12 and 250 respectively. Plotting those values on our graph to the right seems to put our point in the center of the inner most ‘circle’.&lt;/p&gt;

&lt;h2 id=&quot;22-parameter-learning&quot;&gt;2.2 Parameter Learning&lt;/h2&gt;

&lt;h3 id=&quot;221-gradient-descent&quot;&gt;2.2.1 Gradient Descent&lt;/h3&gt;

&lt;p&gt;So we have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That’s where gradient descent comes in.&lt;/p&gt;

&lt;p&gt;Imagine that we graph our hypothesis function based on its fields $θ_0$ and $θ_1$ (actually we are graphing the cost function as a function of the parameter estimates). We are not graphing x and y itself, but the parameter range of our hypothesis function and the cost resulting from selecting a particular set of parameters.&lt;/p&gt;

&lt;p&gt;We put $θ_0$ on the x axis and $θ_1$ on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fqqzup3o06j30ft08etby.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We will know that we have succeeded when our cost function is at the very bottom of the pits in our graph, i.e. when its value is the minimum. The red arrows show the minimum points in the graph.&lt;/p&gt;

&lt;p&gt;The way we do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter α, which is called the learning rate.&lt;/p&gt;

&lt;p&gt;For example, the distance between each ‘star’ in the graph above represents a step determined by our parameter α. A smaller α would result in a smaller step and a larger α results in a larger step. The direction in which the step is taken is determined by the partial derivative of J(θ0,θ1). Depending on where one starts on the graph, one could end up at different points. The image above shows us two different starting points that end up in two different places.&lt;/p&gt;

&lt;p&gt;The gradient descent algorithm is:&lt;/p&gt;

&lt;p&gt;repeat until convergence:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;θ_j:=θ_j−α\frac {∂}{∂θ_j}J(θ_0,θ_1)&lt;/script&gt;

&lt;p&gt;where j=0,1 represents the feature index number.&lt;/p&gt;

&lt;p&gt;At each iteration j, one should simultaneously update the parameters $θ_1,θ_2,…,θ_n$. Updating a specific parameter prior to calculating another one on the $j^{(th)}$ iteration would yield to a wrong implementation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fqqzulsm5oj30i903s75a.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;222-gradient-descent-intuition&quot;&gt;2.2.2 Gradient Descent Intuition&lt;/h3&gt;

&lt;p&gt;In this video we explored the scenario where we used one parameter $θ_1$ and plotted its cost function to implement a gradient descent. Our formula for a single parameter was :&lt;/p&gt;

&lt;p&gt;Repeat until convergence:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;θ_1:=θ_1−α\frac {d}{dθ_1}J(θ_1)&lt;/script&gt;

&lt;p&gt;Regardless of the slope’s sign for $\frac {d}{dθ_1}J(θ_1)$, $θ_1$ eventually converges to its minimum value. The following graph shows that when the slope is negative, the value of $θ_1$ increases and when it is positive, the value of $θ_1$ decreases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fqqzuqgjgwj30ho0a4myg.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On a side note, we should adjust our parameter α to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fqqzuix1r2j30if0a5gmx.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;how-does-gradient-descent-converge-with-a-fixed-step-size-α&quot;&gt;How does gradient descent converge with a fixed step size $α$?&lt;/h4&gt;

&lt;p&gt;The intuition behind the convergence is that $\frac {d}{dθ_1}J(θ_1)$ approaches 0 as we approach the bottom of our convex function. At the minimum, the derivative will always be 0 and thus we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;θ_1:=θ_1−α∗0&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fqqzuftdncj30i60a2400.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;223-gradient-descent-for-linear-regression&quot;&gt;2.2.3 Gradient Descent For Linear Regression&lt;/h3&gt;

&lt;p&gt;When specifically applied to the case of linear regression, a new form of the gradient descent equation can be derived. We can substitute our actual cost function and our actual hypothesis function and modify the equation to :&lt;/p&gt;

&lt;p&gt;repeat until convergence: {
&lt;script type=&quot;math/tex&quot;&gt;θ_0:=θ_0−α\frac {1}{m}∑_{i=1}^{m}(h_θ(x_i)−y_i)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;θ_1:=θ_1−α\frac {1}{m}∑_{i=1}^{m}((h_θ(x_i)−y_i)*x_i)&lt;/script&gt;
}&lt;/p&gt;

&lt;p&gt;where m is the size of the training set, $θ_0$ a constant that will be changing simultaneously with $θ_1$ and $x_i,y_i$ are values of the given training set (data).&lt;/p&gt;

&lt;p&gt;Note that we have separated out the two cases for $θ_j$ into separate equations for $θ_0$ and $θ_1$; and that for $θ_1$ we are multiplying $x_i$ at the end due to the derivative. The following is a derivation of $\frac{∂}{∂θ_j}J(θ)$ for a single example :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fqr0etb4uzj309s05a3yt.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The point of all this is that if we start with a guess for our hypothesis and then repeatedly apply these gradient descent equations, our hypothesis will become more and more accurate.&lt;/p&gt;

&lt;p&gt;So, this is simply gradient descent on the original cost function J. This method looks at every example in the entire training set on every step, and is called &lt;strong&gt;batch gradient descent&lt;/strong&gt;. Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum. Indeed, J is a convex quadratic function. Here is an example of gradient descent as it is run to minimize a quadratic function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fqr0eu9wxnj308s06ot9c.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The ellipses shown above are the contours of a quadratic function. Also shown is the trajectory taken by gradient descent, which was initialized at (48,30). The x’s in the figure (joined by straight lines) mark the successive values of θ that gradient descent went through as it converged to its minimum.&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Apr 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/04/30/ml-week1/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/04/30/ml-week1/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>Mac OS下大数据工具环境搭建</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;本次配置具体运行环境如下：&lt;/p&gt;

  &lt;p&gt;macOS High Sierra 10.13.3&lt;/p&gt;

  &lt;p&gt;Java JDK 1.8.0&lt;/p&gt;

  &lt;p&gt;Hadoop 3.0.0&lt;/p&gt;

  &lt;p&gt;Scala 2.12.4&lt;/p&gt;

  &lt;p&gt;Spark 2.3.0&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;参考了很多帖子，然而很多并不适用。特此留下此马克，简洁易懂且适用，以便日后之需。&lt;/p&gt;

&lt;h3 id=&quot;准备工作配置ssh查看java路径&quot;&gt;准备工作（配置ssh、查看Java路径）&lt;/h3&gt;

&lt;p&gt;首先，打开系统偏好设置-共享-远程登录，勾选远程登录。然后打开 iTerm&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 查看Java JDK路径，并记录&lt;/span&gt;
/usr/libexec/java_home &lt;span class=&quot;nt&quot;&gt;-V&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 使用 ssh-keygen 生成一对密钥，并将其用于免密登录&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/.ssh
ssh-keygen
&lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;id_rsa.pub &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; authorized_keys
&lt;span class=&quot;c&quot;&gt;# 判断是否成功免密登录&lt;/span&gt;
ssh localhost
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;下面操作均在 &lt;code class=&quot;highlighter-rouge&quot;&gt;ssh localhost&lt;/code&gt; 之后进行&lt;/p&gt;

&lt;h3 id=&quot;1-安装hadoop&quot;&gt;1. 安装Hadoop&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 使用 Homebrew 安装 Hadoop&lt;/span&gt;
brew install hadoop
&lt;span class=&quot;c&quot;&gt;# 进入 Hadoop 路径，此处是 3.0.0 版本，其他版本自行更改命令&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /usr/local/Cellar/hadoop/3.0.0/libexec/etc/hadoop

vi hadoop-env.sh
&lt;span class=&quot;c&quot;&gt;# 使用 /HADOOP_OPTS 查找到被注释的行，添加以下两行，第二行修改为 Java 路径，修改后保存退出&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_OPTS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HADOOP_OPTS&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc= &quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;JAVA_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home&quot;&lt;/span&gt;

vi hdfs-site.xml
&lt;span class=&quot;c&quot;&gt;# &amp;lt;configuration&amp;gt; 标签中添加以下内容，修改后保存退出&lt;/span&gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.http.address&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;0.0.0.0:50070&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

vi mapred-site.xml
&lt;span class=&quot;c&quot;&gt;# &amp;lt;configuration&amp;gt; 标签中添加以下内容，修改后保存退出&lt;/span&gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;mapred.job.tracker&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;localhost:8021&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

vi core-site.xml
&lt;span class=&quot;c&quot;&gt;# &amp;lt;configuration&amp;gt; 标签中添加以下内容，修改后保存退出&lt;/span&gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;/usr/local/Cellar/hadoop/hdfs/tmp&amp;lt;/value&amp;gt;
    &amp;lt;description&amp;gt;A base &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;other temporary directories.&amp;lt;/description&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;fs.default.name&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://localhost:8020&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&lt;span class=&quot;c&quot;&gt;# 格式化后即可&lt;/span&gt;
hdfs namenode &lt;span class=&quot;nt&quot;&gt;-format&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /usr/local/Cellar/hadoop/3.0.0/sbin
&lt;span class=&quot;c&quot;&gt;# 启动 Hadoop，WARN 不用管，打开浏览器，查看 localhost 的 50070 端口和 8088 端口&lt;/span&gt;
./start-dfs.sh
./start-yarn.sh
&lt;span class=&quot;c&quot;&gt;# 停止 Hadoop&lt;/span&gt;
./stop-all.sh

&lt;span class=&quot;c&quot;&gt;# 可以更改环境变量，之后可直接使用 start-dfs.sh 和 start-yarn.sh 来启动Hadoop&lt;/span&gt;
vi ~/.bash_profile
&lt;span class=&quot;c&quot;&gt;# 添加以下在尾部，修改后保存退出&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/Cellar/hadoop/3.0.0
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/sbin:&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/bin

&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-安装scala&quot;&gt;2. 安装Scala&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 使用 Homebrew 安装 Scala&lt;/span&gt;
brew install scala
&lt;span class=&quot;c&quot;&gt;# 测试 Scala 安装成功&lt;/span&gt;
scala
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3-安装spark&quot;&gt;3. 安装Spark&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 使用 Homebrew 安装 Spark&lt;/span&gt;
brew install apache-spark
&lt;span class=&quot;c&quot;&gt;# 测试 Spark 安装成功&lt;/span&gt;
spark-shell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Mon, 19 Mar 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/03/19/big-data-env/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/03/19/big-data-env/</guid>
        
        <category>大数据</category>
        
        
      </item>
    
      <item>
        <title>大学生能力测试</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;对于考研，我更愿意称之为大学生能力测试，因为里面的科目我都不太喜欢&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;今年报考的是北京大学软件与微电子学院，金融大数据方向。从结果来看，这次报考算是偷波鸡，明年是否还是这个情况我也不清楚了。留着当个念想吧，说不定还能帮到今后的考研勇士。&lt;/p&gt;

&lt;h3 id=&quot;一初试&quot;&gt;一、初试&lt;/h3&gt;

&lt;p&gt;初始分也不算太高（政治59，英语71，数学110，专业课109，总分349），各位按照自己的计划进行，可能不太适合所有人。说实话我的专业课我的确考得不太好。&lt;/p&gt;

&lt;h4 id=&quot;1-数学&quot;&gt;1. 数学&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;辅导书：推荐高数十八讲（张宇）、概率论九讲（张宇），以及线性代数辅导讲义（李永乐）。&lt;/li&gt;
  &lt;li&gt;网课：高数、概率论看张宇，线代看李永乐，看基础班、强化班。最后有时间可以看点题班，没时间也无所谓，影响不大。&lt;/li&gt;
  &lt;li&gt;习题集：看课的时候搭配教材和辅导书上的习题（较简单）以及题源1000题（难一些，张宇），看完视频了，最后标记一下错题。后面开始做数学历年真题集（张宇），做完了再整理一下错题。最后有时间可以做张宇的最后八套卷（非常难，分数不用当真），没时间就把整理的历年真题的错题再做一遍。&lt;/li&gt;
  &lt;li&gt;具体思路：建议前期直接看视频学习。先看高数基础班、线代基础班、概率论基础班，看的时候做好笔记，后面复习有用。有时间就做一做教材上的题或题源1000题的基础部分，一般都比较简单。然后看高数提高班、线代提高班、概率论提高班，看一节做完一节题源1000题的题，错题标记出来。学习完这两遍之后，最迟十一月就开始做真题吧，每天一套左右。遇到难的要硬着头皮啃完，理解每道题的解法，把错题记录下来。十二月上旬就开始做错题，有时间再做做最后八套卷，没时间就把错题做完就行了。至此，数学能考的题型你基本都见过了，考前再看看错题，想想思路就行了。&lt;/li&gt;
  &lt;li&gt;至于李林还能不能押到题，很难预测到，有空的话找找资源也不亏。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;2-政治&quot;&gt;2. 政治&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;辅导书和习题集：个人不管肖秀荣怎么样，无脑推荐徐涛。买考研政治核心考案、优题库、习题集，最后八套卷，考研政治考前必背20题（都是徐涛的，除了最后一本书以外，主要都是怼选择题）。考研政治考前必背20题需要20天左右背完，合理安排时间（大概60道小题的内容）。做好错题标记，有空了看一看。&lt;/li&gt;
  &lt;li&gt;网课：直接看徐涛的&lt;strong&gt;强化班&lt;/strong&gt;。最好暑假开始看视频，挤不出来时间的话九月份开始也行。&lt;/li&gt;
  &lt;li&gt;具体思路：也建议直接上视频，并从九月份开始准备。我是从十月份开始的，时间略紧。算好视频的数量，每天看几个视频，然后做对应的题（徐涛推荐看完一门课做一门课的题，自己抉择吧），做完把错题标记一下。看完强化班之后，根据其他科目准备的情况，有时间可以看看冲刺班。然后注意他的解答题讲解视频，尤其是马哲的解答题需要自己练习。后面等小黄书（考研政治考前必背20题）出了，最迟考试前三天把内容背完，最后根据年度热门事件选择性地背熟某几个考试概率高的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3-英语&quot;&gt;3. 英语&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;关于背单词：我当时看了恋恋有词，但是我觉得看了遗忘的比较多，而且比较费时间。我就下载的沪江开心英语这个APP，每天150个词，1个小时左右。每个词有解释的听解释，效率对我而言稍高一些。第二天复习的时候注意换个题型考察自己。一些可以用到作文里的经典例句可以提前摘抄下来，以备练习作文时套用。&lt;/li&gt;
  &lt;li&gt;辅导书：做了黄剑的阅读理解150提高版，但是感觉书的质量离真题还是差了一些，要是愿意做也可以当个参考。还用了黄剑的历年真题和写作书（他的书全都是黄色的）。&lt;/li&gt;
  &lt;li&gt;具体思路：每天花半个小时到一个小时背150个单词，考前都不能停下。暑假开始做考研英语真题，一套卷子拆成两三天慢慢做（不用做写作），最好用铅笔写，做完之后看解析，想自己思路哪里不对。全部做完了把笔迹擦掉，隔几天再做一遍，一两天一份（不用做写作）。做完了就一天一节黄剑阅读理解，这个书很怪，个人答案有些奇怪。不过也不用管，反正保持每天阅读就行了。考前一个月看一看他的作文书，背一背小作文模板，练练大作文，上考场就基本不慌了。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;4-专业课&quot;&gt;4. 专业课&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;辅导书：看你报考学校的要求，一般是操作系统、数据结构、计算机网络和组成原理。王道论坛的一套书（这个去他们官方的淘宝店买，后面会送模拟题）质量还行，根据自己需要购买。如果感觉数据结构有些困难，可以买一本天勤的数据结构搭配着看，天勤的数据结构比较细、容易理解。&lt;/li&gt;
  &lt;li&gt;习题：除了书上的题，没时间的话做王道的最后八套，有时间王道和天勤都做了。做好错题整理，考前都有用，特别是计算机网络的杂碎知识点。&lt;/li&gt;
  &lt;li&gt;失败的思路：直接看辅导书做题，不懂的看书。来来回回看了四遍辅导书。&lt;/li&gt;
  &lt;li&gt;觉得可能正确的思路：课本仔细看一遍，然后再看辅导书，不懂的再看书。需要记住书上算法的具体实现。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;5-关于网课视频&quot;&gt;5. 关于网课视频&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;一般买书送的视频都不太靠谱，我的视频基本都是在大傻考研网这个公众号找的（他也有微博号），这个更新的比较快。&lt;/li&gt;
  &lt;li&gt;如果想要更及时的视频，请上&lt;strong&gt;B站&lt;/strong&gt;，多搜搜老师的名字有惊喜，特别是徐涛的视频。&lt;/li&gt;
  &lt;li&gt;如果现在找不到资源，数学可以看去年的，尤其是线性代数。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;6-关于流量&quot;&gt;6. 关于流量&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;开个一年的百度云会员，200多块钱，去图书馆三楼机房下视频或看B站视频。&lt;/li&gt;
  &lt;li&gt;最好搞个内网穿透，把这一年流量问题解决掉，但是稍微有些麻烦，没时间的话请忽略。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;7-其他&quot;&gt;7. 其他&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;晚上七点半准时选第二天图书馆的座位，如果需要三楼的电脑，请早起抢座。&lt;/li&gt;
  &lt;li&gt;考试前一周是四六级考试，请抢好座位。&lt;/li&gt;
  &lt;li&gt;年末背政治犯困，可移步一楼站着背书，一楼可能有点冷风，穿厚点，别感冒了。&lt;/li&gt;
  &lt;li&gt;注意坐姿，眼镜干涩备好眼药水。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;二复试&quot;&gt;二、复试&lt;/h3&gt;

&lt;p&gt;考完初试，休息几天，就抓紧准备复试吧。看看报考方向需要的知识，最好看那么一两本完整的入门书，不至于到时候慌了阵脚。多做做OJ，即使没有进复试也是在为以后积累，这个怎么着也不亏。&lt;/p&gt;

&lt;p&gt;具体过程。进去之后敲门、向老师们问好。老师英语问看过什么大数据的书、什么金融的书，既然没怎么看过金融的书为什么报这个方向（那就暑假在补呗……）。然后就开始聊项目了。讲了讲Adaboost算法原理和具体实现，但是具体实现没有说得特别清楚，有些紧张。总的来说和上午的几个小伙伴差不多，问题都不算太刁钻，都是顺着回答的不断深入地问。&lt;/p&gt;

&lt;h3 id=&quot;三后记&quot;&gt;三、后记&lt;/h3&gt;

&lt;p&gt;想想这半年多的确还是辛苦，从八月回学校真正开始看视频到十二月结束考试，走过了一些弯路，当然也得到了很多人的帮助。在此感谢爸爸妈妈、各位叔叔阿姨、各位老师同学的理解和支持。感谢舍友们对寝室两个考研党的关照。也感谢肥猫一直陪我学习，要是没有肥猫的帮助，我不可能坚持下来。本来想着如果上不了，我也不打算再来一次了。结果3月16号复试结束，3月19日晚上就收到了拟录取通知，然而肥猫运气稍差了一些，还在努力地争取着自己的未来。&lt;/p&gt;

&lt;p&gt;顺境需谨慎，不顺才是是常态，愿你我都有光明的未来。&lt;/p&gt;
</description>
        <pubDate>Sat, 17 Mar 2018 01:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/03/16/about-eep/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/03/16/about-eep/</guid>
        
        <category>考研</category>
        
        
      </item>
    
      <item>
        <title>折腾树莓派</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;抱歉，路由器已经满足不了我了&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;自从考完研就心水树莓派，于是马上就买了个折腾折腾，想当个软路由，但是效果并没有想象中的那么好。暂时先搁着，闲暇时候折腾折腾，当个小玩具。慢慢更新，免得以后迷路。&lt;/p&gt;

&lt;h3 id=&quot;1-准备工作&quot;&gt;1. 准备工作&lt;/h3&gt;

&lt;p&gt;一根网线，一台电脑（只说 MAC OS 环境），一个 Raspberry Pi 3B，一个读卡器，一个8G及以上的 TF 卡，一个 Wi-Fi 环境，一个 USB 网线转换器&lt;/p&gt;

&lt;h3 id=&quot;2-钦定系统-raspbian&quot;&gt;2. 钦定系统 Raspbian&lt;/h3&gt;

&lt;h4 id=&quot;21-烧录&quot;&gt;2.1 烧录&lt;/h4&gt;

&lt;p&gt;下载官网的固件后，解压出 &lt;code class=&quot;highlighter-rouge&quot;&gt;.img&lt;/code&gt; 格式的镜像文件后，插上读卡器。打开 iTerm / Terminal，输入以下指令查看当前挂载设备&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;df &lt;span class=&quot;nt&quot;&gt;-h&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;找到你U盘的路径 &lt;code class=&quot;highlighter-rouge&quot;&gt;[usb_device_path]&lt;/code&gt; 然后输入以下指令卸载设备&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;diskutil unmount &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;usb_device_path]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;然后输入以下指令再找路径 &lt;code class=&quot;highlighter-rouge&quot;&gt;[device_path]&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;diskutil list
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;然后就可以烧录了&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dd &lt;span class=&quot;nv&quot;&gt;ds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;4m &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=[&lt;/span&gt;path_to_img] &lt;span class=&quot;nv&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=[&lt;/span&gt;device_path]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;等待10分钟左右即可，烧录完成后进入U盘路径，并创建空文件 ssh 后保存退出&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;path_to_device]
touch ssh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;22-连接&quot;&gt;2.2 连接&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;系统偏好设置-&amp;gt;共享-&amp;gt;互联网共享&lt;/code&gt; 把连接的Wi-Fi共享到USB端口，然后再打开 iTerm / Terminal 后输入以下指令&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;arp &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;就可以找到树莓派的IP地址，一般是 &lt;code class=&quot;highlighter-rouge&quot;&gt;192.168.2.3&lt;/code&gt; ，再使用SSH连接树莓派，密码是raspberry&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh root@[ip_address]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;至此即可使用 ssh 控制树莓派&lt;/p&gt;

&lt;h3 id=&quot;3-路由系统-lede&quot;&gt;3. 路由系统 LEDE&lt;/h3&gt;

&lt;h4 id=&quot;31-烧录&quot;&gt;3.1 烧录&lt;/h4&gt;

&lt;p&gt;下载官网镜像，选择 &lt;code class=&quot;highlighter-rouge&quot;&gt;arm_cortex-a53_neon-vfpv4&lt;/code&gt; 这个目录下面的固件，解压出 &lt;code class=&quot;highlighter-rouge&quot;&gt;.img&lt;/code&gt; 格式的镜像文件后，插上读卡器。打开 iTerm / Terminal，输入以下指令查看当前挂载设备&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;df &lt;span class=&quot;nt&quot;&gt;-h&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;找到你U盘的路径 &lt;code class=&quot;highlighter-rouge&quot;&gt;[usb_device_path]&lt;/code&gt; 然后输入以下指令卸载设备&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;diskutil unmount &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;usb_device_path]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;然后输入以下指令再找路径 &lt;code class=&quot;highlighter-rouge&quot;&gt;[device_path]&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;diskutil list
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;然后就可以烧录了&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dd &lt;span class=&quot;nv&quot;&gt;ds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2m &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=[&lt;/span&gt;path_to_img] &lt;span class=&quot;nv&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=[&lt;/span&gt;device_path]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;等待5分钟左右即可&lt;/p&gt;

&lt;h4 id=&quot;32-连接&quot;&gt;3.2 连接&lt;/h4&gt;

&lt;p&gt;现在官方的 LEDE 固件已经集成 LUCI ，并开启了 DHCP 。网线连到树莓派的网线口，关掉Wi-Fi，有IP地址后，登录 &lt;code class=&quot;highlighter-rouge&quot;&gt;192.168.1.1&lt;/code&gt; 即可进行路由器设置&lt;/p&gt;

&lt;h4 id=&quot;33-设置&quot;&gt;3.3 设置&lt;/h4&gt;

&lt;p&gt;打开 LUCI 界面，开启树莓派的无线，然后用 Wi-Fi 连接树莓派。（未完待续）&lt;/p&gt;
</description>
        <pubDate>Sat, 06 Jan 2018 04:40:00 +0800</pubDate>
        <link>http://localhost:4000/2018/01/05/raspberry-pi/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/01/05/raspberry-pi/</guid>
        
        <category>树莓派</category>
        
        
      </item>
    
  </channel>
</rss>
