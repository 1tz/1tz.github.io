<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>俄罗斯韬娃</title>
    <description>An undergraduate in Beijing Normal University</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 23 Jun 2018 20:26:39 +0800</pubDate>
    <lastBuildDate>Sat, 23 Jun 2018 20:26:39 +0800</lastBuildDate>
    <generator>Jekyll v3.6.2</generator>
    
      <item>
        <title>台湾游记</title>
        <description>&lt;h1 id=&quot;20180527-20180606台北-台中-台南-高雄-垦丁-花莲-台北我韬韬&quot;&gt;2018.05.27-2018.06.06，台北-台中-台南-高雄-垦丁-花莲-台北，我&amp;amp;韬韬。&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;出发之前，我和韬韬开玩笑，去一趟台湾回来注定要变成肥仔。韬韬信誓旦旦地说，他去台湾的第一天晚上要牛排鸡排各一份，再配上一杯50岚，美滋滋。结果，在台湾的这几天，我们往往一天只吃两顿，事先做好的餐厅、夜市攻略也吃了不到一半。或许，相比于眼花缭乱的美食，台湾更吸引我们的是这里精致的风景和浓浓的人情味。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;day-1-台北&quot;&gt;Day 1 台北&lt;/h2&gt;

&lt;p&gt;我们是中午11点半左右到的桃园机场，在机场的ATM机换了新台币，坐紫线捷运直达台北车站。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;寄存行李🙋🏻‍我有话说：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们从机场直奔台北车站的目的只有一个：寄存行李。即使去的时候是台湾旅游的淡季，台北车站地下的行李寄存箱仍然鲜有空位。这时，不妨到地上的台铁行李房看看，那里空间大，无需担心没有空位的问题，且价格低廉，按天计价。唯一不足之处在于行李房有固定的营业时间，须在关闭前取走行李。&lt;/p&gt;

&lt;h4 id=&quot;潘家老牌牛肉面&quot;&gt;🍱潘家老牌牛肉面&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;在台北车站附近的一条巷子里，没有正经的店面，更像是路边摊。&lt;/p&gt;

&lt;p&gt;我和韬韬都点的招牌牛肉面，牛肉、牛筋、牛肚三合一。个人觉得无论是汤头还是食材，味道都不错。食物的大小也切得合适，正好一口的量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslc5hv31hj31kw16okjm.jpg&quot; alt=&quot;潘家老牌牛肉面的摊位&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslc5swzv0j31kw1kwnpd.jpg&quot; alt=&quot;招牌牛肉面&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;北投&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;台北的北投区以温泉闻名，通往北投的捷运上多是提着袋子来这里泡温泉的本地人和少数游客。除了温泉，北投温泉博物馆、北投图书馆、地热谷也是游客不容错过的景点。&lt;/p&gt;

&lt;h4 id=&quot;北投温泉博物馆&quot;&gt;📍北投温泉博物馆&lt;/h4&gt;

&lt;p&gt;这是出发前我一直心心念念的博物馆。结果，到达时正逢博物馆整修，预计得今年年底才会再度开放了。&lt;/p&gt;

&lt;h4 id=&quot;北投图书馆&quot;&gt;📚北投图书馆&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;图书馆的整体设计风格深得我心。矮矮的书架，随处可见的小台灯，桌子和凳子的多元组合，让所有人都可以在这里找到属于自己的阅读方式。不过，值得注意的是，图书馆虽美，在拍摄上却有严格的要求，即不允许拍摄到人，自拍也不行。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fsld0rrg8pj31kw11x1l1.jpg&quot; alt=&quot;北投图书馆的外观1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fsld0o4c8uj31kw120x6r.jpg&quot; alt=&quot;北投图书馆的外观2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslc6xwyqfj30sg0lcqaf.jpg&quot; alt=&quot;北投图书馆的小书架&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslc7b3u9ej31kw16oqv6.jpg&quot; alt=&quot;一些童话书&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;地热谷&quot;&gt;📍地热谷&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;北投一带温泉的源头，但由于水温过高，并不适合直接在这里泡。走近地热谷，就能强烈感受到扑面而来的热气。水呈翠绿色，水面上源源不断升起白色的烟雾，仿佛置身于仙境。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fsj1aql1twg30qo0f0npe.gif&quot; alt=&quot;地热谷&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcly1fsgfbp8uk2j31kw1kwtwy.jpg&quot; alt=&quot;试图点水却被烫死的蜻蜓&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslc83fffuj31kw16oqv7.jpg&quot; alt=&quot;被地热谷的水冲刷绿的石头&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;️千禧汤&quot;&gt;♨️千禧汤&lt;/h4&gt;

&lt;p&gt;​    肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​    韬韬⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;当地小有名气的露天温泉。内部是日式建筑风格，配有厕所、淋浴房、吹风机、储物柜等设施，一人40新台币，须穿着泳装泡汤。&lt;/p&gt;

&lt;p&gt;虽然是炎炎夏日，但来千禧汤泡温泉的人并不少，多是当地的大爷大妈。馆内共有5个池子，两个冷池，一个30度左右，一个40度左右，一个50度左右。我们当时泡的40度左右的那个，水温正好，是平时洗热水澡的温度，非常舒服。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;去千禧汤🙋🏻‍我有话说：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;内部的设施多是收费的，如电吹风、淋热水浴、储物柜，因而携带充分的零钱是必要的。&lt;/p&gt;

&lt;p&gt;此外，池子之间多由石头相连，夏天的石头又烫又硌脚，因而携带拖鞋也是非常必要的。&lt;/p&gt;

&lt;h4 id=&quot;象山&quot;&gt;🌃象山&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;当日最惊喜的景点，没有之一！&lt;/p&gt;

&lt;p&gt;登象山不纯粹为了爬山，而是为了看台北夜景！我和韬韬一人喝了一杯奶茶，晚饭都没吃就兴冲冲开始爬象山。我们是晚上去的，沿路设有路灯，且夜爬象山的人挺多，因而也不感到害怕。爬到第一个观景台时，人很多，视线也不是太好，因而一咬牙又继续爬到第二个观景台。登临第二个观景台的台阶会陡一些，但是人少很多，视野也好得多。从第二观景台再往上走几步就是摄影平台，平台上铺设有平整的木板，供游客歇脚的长椅和直饮水设备，适合歇脚、看夜景和疯狂拍照。&lt;/p&gt;

&lt;p&gt;不得不说，在象山上看夜景比在台北101大楼强太多，而且无须高昂的门票。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;象山看景🙋🏻‍我有话说：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果登象山是冲着看台北101大楼的夜景去的，那么一定要赶在晚上十点之前到达。台北101大楼的灯光只开到晚上十点，我们下山时正好目睹101大楼熄灯，一看时间正好十点。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcly1fsgfbznbicj31kw16onpd.jpg&quot; alt=&quot;登象山前的能量补充之50岚（韬韬：这时我们的手还是白的）&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslc8lm38ej31kw11x4qr.jpg&quot; alt=&quot;象山上的台北夜景1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fslc94sp9sj31kw11x7wj.jpg&quot; alt=&quot;象山上的台北夜景2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslc9afo01j31kw11xb2b.jpg&quot; alt=&quot;象山上的台北夜景3&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;day-2-台北&quot;&gt;Day 2 台北&lt;/h2&gt;

&lt;h4 id=&quot;永和豆浆大王&quot;&gt;🍳永和豆浆大王&lt;/h4&gt;

&lt;p&gt;​    肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​    韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;比较理想的一家早餐店，店内品种蛮多。强烈推荐他家的荷包蛋！现煎！淋汁！非常好吃！&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslca0q0upj30sg0lcjxb.jpg&quot; alt=&quot;永和豆浆大王&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;台北故宫博物院&quot;&gt;📍台北故宫博物院&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;来台湾怎么能不来台北故宫博物馆！&lt;/p&gt;

&lt;p&gt;台北故宫博物院的展品小而精。其中，以翠玉白菜、肉形石、毛公鼎为镇馆之宝，在二楼有单独的展厅。&lt;/p&gt;

&lt;p&gt;出发前的半个月，我和韬韬在台北故宫博物院的官网上预约了定时导览，即由志愿者带领着报名的游客一边讲解一边欣赏展品。当时为我们讲解的是一位奶奶，她对瓷器比较感兴趣，因而就依着朝代一边为我们讲解瓷器的发展史一边欣赏展品，对于外行人而言还是挺长见识的。&lt;/p&gt;

&lt;p&gt;奶奶在讲解的过程中留给我最深的一句话是「不同民族之间相互接触和交流的过程中，彼此的文化会相互渗透，并具体表现在他们使用的各种器具上。」于历朝历代如此，于台湾亦是如此。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslcah6spzj31kw11xx6r.jpg&quot; alt=&quot;翠玉白菜&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslcabde9ej31kw11xu0z.jpg&quot; alt=&quot;肉形石&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;富春居&quot;&gt;🍱富春居&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;导览员推荐的吃简餐的地方。餐厅是日式建筑风格，前台非常热情，人少时会主动安排坐包间。食物味道不错，在景区内能吃到这种价位和口味的简餐，也算物美价廉。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslcatee26j31kw155npe.jpg&quot; alt=&quot;富春居&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;搭乘公车🙋🏻‍我有话说：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;来台北故宫博物院，免不了要搭乘公车。在台湾搭乘公车，有几点必须注意：1 进站的公车只有看见乘客招手才会停车；2 公车的刷卡方式有三种，即上车刷卡、下车刷卡和上下车刷卡，具体的刷卡方式可在上车时留意司机头上的牌子；3 大多数公车不报站，可借助导航APP定位，如Bus+；4 下车前按下车铃以提示司机下站停车。&lt;/p&gt;

&lt;h4 id=&quot;中正纪念堂&quot;&gt;📍中正纪念堂&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;中正纪念堂为纪念蒋介石而建造，看点之一是整点的宪兵交接仪式。中正纪念堂的前方是自由广场，一群被喂得白白胖胖的鸽子在那里来回踱步，两旁宫殿般的建筑分别是音乐厅和戏剧院，是台湾文艺展演的最高殿堂。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslcb91os0j31kw11pe82.jpg&quot; alt=&quot;中正纪念堂&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;二二八和平纪念公园&quot;&gt;📍二二八和平纪念公园&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;来公园的目的很肤浅，就是为了看小松鼠。公园里人不多，很安静，除了来回跳窜的小松鼠，认真踱步的鸽子也随处可见。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslcbmsxbij31kw11xqvb.jpg&quot; alt=&quot;二二八和平纪念公园&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;迪化街&quot;&gt;📍迪化街&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️&lt;/p&gt;

&lt;p&gt;我们是下午接近傍晚的时候去的，街上行人不多，沿途以卖特产的店铺居多。个人觉得不是太有意思，也许沿途的建筑风格值得欣赏。&lt;/p&gt;

&lt;h4 id=&quot;大稻埕码头&quot;&gt;🛳大稻埕码头&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;大稻埕码头是个比较神奇的地方，一边是静静流淌的淡水河，一边是成群结队飞驰而过的机车。码头附近很安静，有几间卖饮品、小吃的小屋。在这里吹吹风、看看夕阳，也是一个不错的选择。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslcc6mtg9j31kw11xb2b.jpg&quot; alt=&quot;大稻埕码头&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;阿宗面线&quot;&gt;🍜阿宗面线&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;毫不夸张，这可能是我这辈子吃过的最好吃的面线！面线的口感和厦门的面线糊有点不同，比较有韧性，木鱼花的调味使得整碗面线特别香！强烈推荐！&lt;/p&gt;

&lt;p&gt;顺便一提，这家店是没有座位的，大家都是现场买了捧着碗在门口站着吃。夏天天气热，大家一边吃得津津有味，一边满头大汗，也是挺有意思的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslcdmms8gj31kw1kwkjl.jpg&quot; alt=&quot;阿宗面线&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;台北101大楼&quot;&gt;🌃台北101大楼&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;这是一个不来不甘心，来了又失望的地方。看点也许是电梯爬楼的速度？也许是88层巨大的阻尼器？也许是户外的观景台？&lt;/p&gt;

&lt;p&gt;最后说一句，若是想看台北夜景，还是去象山吧。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslcg9qwa4j31kw11xhdv.jpg&quot; alt=&quot;台北101大楼&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;day-3-台中&quot;&gt;Day 3 台中&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;论旅游景点的质量，台中完胜台北！高美湿地、日月潭，每个景点都美得令人窒息，令人不舍离去。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;🚇在台北车站乘坐高铁到的台中高铁站，因为台中的繁华地带集中在火车站附近，因而我们下了高铁又坐区间车前往台中火车站。如此折腾，似乎还不如一开始就坐台铁从台北车站到台中火车站？&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslcgjqe96j30sg0lcn1k.jpg&quot; alt=&quot;台中高铁站&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;宫原眼科&quot;&gt;🍨宫原眼科&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;原是一家诊所，后被一家凤梨酥企业改造为甜品店兼伴手礼店。甜品店里的冰淇淋球口味很多，味道不错；伴手礼店设计复古，一股魔法图书馆风，里面卖茶书哦。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslchc6g44j31kw16oe82.jpg&quot; alt=&quot;宫原眼科1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslchr86cuj31kw11xe85.jpg&quot; alt=&quot;宫原眼科2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fslch9kbdkj31kw23v7wi.jpg&quot; alt=&quot;宫原眼科3&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;台中肉员&quot;&gt;🍱台中肉员&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;肉圆算是台中非常有特色的小吃了，店里客人以本地人居多。这家店只卖肉圆、冬粉汤和鱼丸汤三种，味道都不错。肉圆的外皮很Q很有嚼劲，里面包着肉馅，配着蘸酱吃味道很好。冬粉汤和鱼丸汤口味偏清淡，配着肉圆吃正好。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fslci1c43tj30sg0sg7bz.jpg&quot; alt=&quot;台中肉员&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;高美湿地&quot;&gt;📍高美湿地&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;去高美湿地有两种办法，一是包车，二是从台中火车站坐火车到清水再转大巴。我们选择的第二种，也还算方便。&lt;/p&gt;

&lt;p&gt;进入湿地，首先要走过一段木栈道。木栈道两旁的泥里散落着一群白色小螃蟹，在阳光下挥舞着小钳子，非常有意思。值得注意的是，木栈道经过的这部分湿地是不允许游客进入的，有非常严格的罚款处罚，即使东西掉了也不能捡。继续往前走，离木栈道远一些的地方，偶尔可以看见一群白色的鸟低着头觅食。据说这些都是候鸟，填饱了肚子又会继续它们的旅程。走到木栈道的终点，就可以下湿地了。至于这里的景色有多美，难以言表，还是看图吧。&lt;/p&gt;

&lt;p&gt;其实很多人是来这里看夕阳的，但是因为台铁班次的原因，只能在落日前离开。如果有机会还想再来，看完落日再走的那种！&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fslcih2yitj31kw11x4qq.jpg&quot; alt=&quot;高美湿地1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslcin9b89j31kw11xe82.jpg&quot; alt=&quot;高美湿地2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslciv69uvj31kw11xqv7.jpg&quot; alt=&quot;高美湿地3&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;逢甲夜市&quot;&gt;📍逢甲夜市&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;逢甲夜市很大，里面有吃的有喝的，有玩的有买的，完全就是一个生活区。吃了大名鼎鼎的官芝霖大肠包小肠，可能是我对它的预期不对，以为有大肠或小肠，结果拿到手里发现是糯米包香肠，而且还点了蒜味，蒜特别辣，所以觉得味道一般。印象比较深的是果汁，混合了好多种水果榨的，酸酸甜甜，出奇的好喝，而鸡排、牛排、猪排这些小吃，味道还行，但没有到惊艳的地步。&lt;/p&gt;

&lt;h4 id=&quot;台中植光花园酒店sof-hotel&quot;&gt;🏨台中植光花园酒店（SOF hotel）&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;这是我们在台中住的酒店，因为实在太喜欢了，就在这里标注顺便推荐一下。&lt;/p&gt;

&lt;p&gt;酒店的整体构架是混泥土风格，酒店的里里外外种了好多植物，在酒店的正中间更是种了树！仿佛置身森林的感觉。&lt;/p&gt;

&lt;p&gt;因为植物较多，担心有蚊虫，酒店还给了一个驱蚊器，非常贴心！住的几天也没有受到蚊虫的困扰。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslcj5h5tuj30sg0lcah7.jpg&quot; alt=&quot;SOF&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;day-4-台中&quot;&gt;Day 4 台中&lt;/h2&gt;

&lt;h4 id=&quot;日月潭&quot;&gt;📍日月潭&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;来台中怎么能不来日月潭！日月潭那么美！&lt;/p&gt;

&lt;p&gt;先说交通，我们买的台湾好行的套票，包含从台中火车站到日月潭的来回车票、日月潭的班轮、日月潭骑行券、日月潭观光巴士车票等。时间关系，我们只用了前三部分的票，但是也不亏。&lt;/p&gt;

&lt;p&gt;来之前对日月潭并没有太高的期待，太过热门的景点总是言过其实。但在下车的那一瞬间，日月潭真的惊艳到我了。日月潭真的太美了，那种朦朦胧胧的美，那种水天一色的美，那种人与自然融为一体的美。&lt;/p&gt;

&lt;p&gt;日月潭的玩法有好多种。最大众的就是坐班轮在三个码头间穿梭，听听船长介绍这里的故事；体力还不错的，可以租辆自行车从水社出发，一路骑到月潭，目睹日月潭最美的一段风景和当地人的水上运动；还可以坐日月潭景区的观光巴士在海拔较高的景点下车，俯瞰日月潭的全貌。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslcjjhehmj31kw11x1kz.jpg&quot; alt=&quot;日月潭1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslcjsvsefj31kw11xx6s.jpg&quot; alt=&quot;日月潭2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslck2ulrxj31kw11xx6s.jpg&quot; alt=&quot;日月潭3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslckaud54j31kw11xnpg.jpg&quot; alt=&quot;日月潭4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslckk0i9aj31kw11xx6s.jpg&quot; alt=&quot;日月潭5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslckpoej8j31kw11x7wj.jpg&quot; alt=&quot;日月潭6&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;阿婆茶叶蛋&quot;&gt;🍱阿婆茶叶蛋&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;在玄光码头上，来日月潭必吃的小吃。这里茶叶蛋的独特之处在于煮的时候加了香菇作为佐料，吃起来特别香。&lt;/p&gt;

&lt;h4 id=&quot;台湾香蕉新乐园&quot;&gt;🍱台湾香蕉新乐园&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;这是一家起初听名字有点摸不着头脑，进去后完全被震撼的餐厅。餐厅内部被设计成一条街市，中间是路，两旁是林立的商店，有美发店、照相馆、小卖部等，都是上个世纪的面貌，浓浓的复古风。买了单还可以上二楼参观。二楼主要陈列了一些早年的玩具，更有一些是限量版的，想必店主应该是个十足的收藏家。&lt;/p&gt;

&lt;p&gt;关注点都落在装修上了。至于菜品，我觉得都不错，印象比较深的是肉燥饭，对于从来不吃肥肉的我而言，真的非常好吃，一点都不油腻。而夏天来一壶冰什锦水果茶，酸酸甜甜的，也非常舒服。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslcl3bcrgj31kw11xb2c.jpg&quot; alt=&quot;台湾香蕉新乐园1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslcl89uyrj31kw11xu0z.jpg&quot; alt=&quot;台湾香蕉新乐园2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslclcs6a0j31kw11xqv7.jpg&quot; alt=&quot;台湾香蕉新乐园3&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;day-5-台南--高雄&quot;&gt;Day 5 台南 &amp;amp; 高雄&lt;/h2&gt;

&lt;p&gt;这次我们吸取了教训，从台中直接坐台铁到达台南。&lt;/p&gt;

&lt;p&gt;众所周知，台南以美食和庙宇闻名。这里的食物是全台湾最好吃的，这里的宗教信仰文化可能也是全台湾最密集且最丰富的。&lt;/p&gt;

&lt;p&gt;寄存行李时，火车站的大叔提醒我们多使用公共交通工具，坐计程车容易被敲竹杆。&lt;/p&gt;

&lt;h4 id=&quot;赤莰担仔面&quot;&gt;🍱赤莰担仔面&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;这原是一间老房子，后经过修缮和利用，成为一家古色古香的餐馆。店内食物以小吃为主，因为选择困难，我们当时点了个套餐，又加了点卤料。大力推荐它家的担仔面，无法用言语形容的好吃！鱼羹也不错，稠稠的，混合着酸笋的酸味，味道很不错。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslclo6x0wj30sg0sgn49.jpg&quot; alt=&quot;赤莰担仔面1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslclsg2nuj30sg0sg12i.jpg&quot; alt=&quot;赤莰担仔面2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fslclw8dgej30sg0sgn4i.jpg&quot; alt=&quot;赤莰担仔面3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslclzk8vpj30sg0lcgvd.jpg&quot; alt=&quot;赤莰担仔面4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslcm3l5xyj30sg0sgn3i.jpg&quot; alt=&quot;赤莰担仔面5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fslcm8k0aej30sg0sgai2.jpg&quot; alt=&quot;赤莰担仔面6&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;赤崁楼&quot;&gt;📍赤崁楼&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;台南两大看点：历史建筑和庙宇，赤崁楼属于前者。园区主要包含海神庙和文昌阁两栋建筑，还有当年荷兰建筑的遗址。登上海神庙和文昌阁可以望望远，但高度有限，风景也有限。值得一提的是，文昌阁二楼的一块大板子上挂满了考生的准考证和写着愿望的小牌子，想必每逢考试升学，这里都香火很旺。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslcmn44baj31kw11xnph.jpg&quot; alt=&quot;赤崁楼&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;双全老店红茶&quot;&gt;🍱双全老店红茶&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;走着走着就渴了，于是寻了一家饮品店。这是一家老店，在小巷子里，很多当地人会开着摩托车来买，一次都是一大瓶一大瓶的带。我和韬韬两个人买了一大瓶，店家给了两个杯子，于是我们俩就在人家店门口你一杯我一杯地喝了起来，仿佛在街边买醉。&lt;/p&gt;

&lt;p&gt;客人可以自主选择红茶的甜度。红茶刚入口有点涩有点苦，停顿片刻后嘴里会有回甘，清凉解渴。不过，喝太多会边际效应递减hhh，韬韬亲身体会。&lt;/p&gt;

&lt;h4 id=&quot;林百货&quot;&gt;📍林百货&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;据说这是全台南第一家有电梯的百货大楼。大楼一共五层，以文创产品为主。产品都还蛮吸引人，我入手了一个奶茶提袋hhh。&lt;/p&gt;

&lt;p&gt;本来想在台南大吃一顿，结果肚子不争气，吃了一家店就饱了。也许是台南的天气太热了，影响食欲。&lt;/p&gt;

&lt;p&gt;逛完林百货，三点多的样子，我们又乘坐台铁前往高雄。&lt;/p&gt;

&lt;p&gt;高雄是个港口城市，本以为会是一个同台北相媲美的大城市模样，但真实情况却有些偏差。在高雄，街上行人并不多，非上下高峰期的捷运车厢常常空荡荡的。不过，高雄给我的感觉更自由些。在台北，公共场所总是安安静静的，大家说话总压低着声音，生怕吵着别人。而高雄则不同，也许是因为人少，也许是因为港口的海风，大家大声畅谈，风会被吵闹声都带走的。&lt;/p&gt;

&lt;h4 id=&quot;美丽岛站&quot;&gt;📍美丽岛站&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;这是高雄两条捷运线的换乘站，一出捷运的闸机就能看见，五光十色的。中间伫立着两根圆柱，一红一蓝，撑起一片巨大的穹顶，仿佛教堂的彩色玻璃般神秘而美丽。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslcn2vnvlj31kw11xu10.jpg&quot; alt=&quot;美丽岛站&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;西子湾&quot;&gt;📍西子湾&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;到西子湾时，六点左右，太阳已经开始向海平面偏移，我们便决意在这里守着看日落。即使是傍晚时分，西子湾的人并不多，大家三五成群地聚在石头围栏旁一边聊着天，一边等待日落。临近日落，来了一群学生，一位男孩抱着吉他，在日落时分开始唱歌。伴着歌声，看着太阳一点一点下沉，也是别有一番风味。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslcne8pmnj31kw11xx6s.jpg&quot; alt=&quot;西子湾&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;旗津半岛&quot;&gt;📍旗津半岛&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;旗津半岛之于高雄，有点鼓浪屿之于厦门的感觉，需乘坐渡轮前往。船票可现金可刷卡，刷卡有优惠。乘船过程中可远远看见85大楼。&lt;/p&gt;

&lt;p&gt;我们抵达旗津半岛时天已经擦黑，路上行人熙熙攘攘，几乎都是冲着海产街去的，我们也不例外。除此之外，旗津半岛似乎也没有太多可玩之处，就是普通的街市。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslcntahezj31kw11x1l0.jpg&quot; alt=&quot;旗津半岛&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;鸭角活海产店&quot;&gt;🍱鸭角活海产店&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;在海产街上，但得往里走一点。虽然不似街口那几家海产店，专门有人在门口招揽生意，但这家的生意仍不错，当地人居多。点菜的方式很大方，门口摆放着各类海产品，直接挑选，服务员会向你推荐做法。若实在无从下手，也可向服务员说明预算，由他推荐菜品。他会非常严格地为你控制预算，超过了还会提醒你，非常良心。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslco9ecuoj30sg0sgwo1.jpg&quot; alt=&quot;鸭角活海产店1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fslcod3ogoj30sg0sgaij.jpg&quot; alt=&quot;鸭角活海产店2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fslcoh01n8j30sg0sg4a5.jpg&quot; alt=&quot;鸭角活海产店3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslcol4h6wj30sg0sgtg9.jpg&quot; alt=&quot;鸭角活海产店4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslcoogrkyj30sg0sggxi.jpg&quot; alt=&quot;鸭角活海产店5&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;六合夜市&quot;&gt;🍱六合夜市&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;吃海鲜没吃饱，回来后继续逛夜市。夜市挺热闹。「烤肉之家」的生意不错，去时还排了会儿队。他家的茭白是整个直接烤的，脆脆的，还有点汤汁，味道不错。&lt;/p&gt;

&lt;h2 id=&quot;day-6-垦丁&quot;&gt;Day 6 垦丁&lt;/h2&gt;

&lt;p&gt;一说到垦丁，浮现在眼前的必然是阳光、沙滩和一望无际的湛蓝海洋。垦丁的确如此。但在垦丁游玩时，一定要注意防晒。我和韬韬在垦丁玩了一天，纷纷晒伤，红彤彤的一片，巨疼。&lt;/p&gt;

&lt;p&gt;我们一早赶到高雄的高铁左营站，乘坐垦丁快线前往垦丁。到达垦丁时，已经是中午，烈日炎炎。我们住的民宿楼下就能租机车。在小哥的一番劝说下，认同骑机车游走于各个景点更方便的观点，租了一辆，费用按天算。&lt;/p&gt;

&lt;p&gt;刚租到机车时，韬韬特别兴奋，一边骑着车一边说「肥猫，我带你去兜风」，莫名其妙的偶像剧代入感hhh。&lt;/p&gt;

&lt;h4 id=&quot;巷子内海鲜热炒合菜&quot;&gt;🍱巷子内海鲜热炒合菜&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;这里的导航特别不准，我和韬韬绕了好几圈，愣着没找着，倒是韬韬的骑车水平有了很大的提升。最后放弃寻找四处瞎晃时意外发现，也算缘分。&lt;/p&gt;

&lt;p&gt;比较独特的一道菜是「情人的眼泪」，炒的一种黑木耳口感的食材，韬韬觉得很好吃。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslcp02sm2j30sg0sgdpg.jpg&quot; alt=&quot;巷子内海鲜热炒合菜&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;龙坑生态保护区&quot;&gt;📍龙坑生态保护区&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;原先是个保护区， 仅供科研使用。近几年向游客开放，每天控制固定的接待量，需提前预约。&lt;/p&gt;

&lt;p&gt;参观的形式是一位志愿者带领着一个团的人，一边进入保护区，一边进行讲解。在到达真正的龙坑前，要穿过一片林子，沿路有些花花草草，志愿者会选取具有代表性的植物进行讲解，节奏很慢，听着很舒服。林子的尽头是一片珊瑚礁，也就是所谓的龙坑。像是一块块石头堆砌而成，但上面有些小坑，都奇形怪状的，工作人员为此发挥了各种想象。珊瑚礁不允许踩踏，因而建起了一条弯弯曲曲的木栈道。沿着木栈道一路向上走，可远眺大海。左边是巴士海峡，右边是太平洋，中间是它们的交汇处，视野极广。远眺是平静的湛蓝海面，水光接天，近看是不断拍打着岸边珊瑚礁的浪花，气势磅礴，配上迎面吹来的阵阵海风，非常凉爽且惬意。对比鹅銮鼻公园和台湾最南点，在龙坑生态保护区看到的海是最美的。&lt;/p&gt;

&lt;p&gt;看完龙坑，休息片刻，志愿者便带着我们原路返回。中途，他突发奇想，说带我们去看个东西。只见地上一堆散落的贝壳，边上放着一个塑料桶，桶里装满了各种饮料瓶等塑料垃圾。志愿者告诉我们，这些塑料垃圾都是寄居蟹带来的。海洋垃圾太多，寄居蟹就会误把这些垃圾当作自己的家，带着它们上岸，但这些垃圾并不能很好地保护它们。于是他们为寄居蟹在这里设置了一个中转站，捡了一些贝壳，这样寄居蟹经过时就可以给自己换个家，更好地生存下去，算是寄居蟹保育工作的一部分。志愿者再三向我们重申保护环境的重要性，这也是这个保护区向外开放最重要的意义所在吧。&lt;/p&gt;

&lt;p&gt;龙坑预约🙋🏻‍我有话说：&lt;/p&gt;

&lt;p&gt;我们原先是在官网上预约的，但官网比较麻烦的一个问题是，只有两种选择，一是自己开个团，二是等着别人来拼团。后者可平摊导览费用，但得满一定的人数才开团。我们选的后者。但临近预约日仍迟迟未成团，眼看几乎要泡汤，偶然发现淘宝上有这类拼团服务，100%成团，但不保证最低费用。能花尽量少的钱玩到喜欢的景点也算值得。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslcpd6gd5j31kw11xnpg.jpg&quot; alt=&quot;龙坑1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslcpnp6lfj31kw11xe86.jpg&quot; alt=&quot;龙坑2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslcptrlx4j31kw11xx6s.jpg&quot; alt=&quot;龙坑3&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;台湾最南点&quot;&gt;📍台湾最南点&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️&lt;/p&gt;

&lt;p&gt;出龙坑时正好看见路标，便顺路来看看。来台湾最南点无非两个目的，一是在与台湾最南点的标志合个影，二是在观景台上看看巴士海峡与太平洋交汇处的壮阔海景。但个人觉得在龙坑木栈道上看到的更震撼些。&lt;/p&gt;

&lt;h4 id=&quot;鹅銮鼻公园&quot;&gt;📍鹅銮鼻公园&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;鹅銮鼻公园的看点之一是这里的白色灯塔，很多人在这里拍照。其次是海边的观景台。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslcq41393j31kw2dchdv.jpg&quot; alt=&quot;鹅銮鼻灯塔&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;船帆石&quot;&gt;📍船帆石&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;因形似船帆而得名，是垦丁的标志性景观。垦丁公园前往鹅銮鼻公园沿途有一段木栈道，是观赏船帆石的绝佳位置。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fslcqiauv4j31kw11xu0z.jpg&quot; alt=&quot;船帆石&quot; /&gt;&lt;/p&gt;

&lt;p&gt;本想去关山看日落的，但没赶上，太阳在我们到达前匆匆下山，我们骑着机车在路上与太阳说了再见，寻思着找个海滩走一走，就近选择了南湾。&lt;/p&gt;

&lt;p&gt;不一会儿，天色就完全暗下来，我们重新骑上机车，前往恒春镇觅食。&lt;/p&gt;

&lt;h4 id=&quot;乡村冬粉鸭&quot;&gt;🍱乡村冬粉鸭&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;这家店的主角就是鸭子。大家的普遍吃法是点一碗冬粉，再来点卤料。冬粉汤的味道比较清淡，卤料的味道很香，有一股淡淡的烟熏味。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fsj36pde68j30sg0sgad6.jpg&quot; alt=&quot;乡村冬粉鸭1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fsj36uz43jj30sg0sgq61.jpg&quot; alt=&quot;乡村冬粉鸭2&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;垦丁大街&quot;&gt;🍱垦丁大街&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;从恒春镇回来，经过一天的暴晒，两个人的腿和手臂早已红彤彤一片，严重晒伤，也没什么心情逛吃垦丁大街了。&lt;/p&gt;

&lt;p&gt;但是！垦丁大街真的很热闹，比恒春镇更热闹。人群以游客为主。路过几个卖生蚝的摊子，价格很便宜！据韬韬回忆，9个生蚝只要100新台币。韬韬到现在还在后悔当时没有大吃几个生蚝，并恳请大家走过路过替他多吃几个。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslcr7ahlqj30sg0sgtft.jpg&quot; alt=&quot;垦丁大街&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;day-7-垦丁&quot;&gt;Day 7 垦丁&lt;/h2&gt;

&lt;p&gt;一早乘坐垦丁街车前往海生馆。&lt;/p&gt;

&lt;p&gt;垦丁街车的班次不是很密集，乘客不多。沿途停靠的站点很多，但如果没人上下车就不停靠，因而开起来还挺快。如果能准确知晓发车时间，乘坐起来还是比较方便的。&lt;/p&gt;

&lt;h4 id=&quot;海生馆&quot;&gt;📍海生馆&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;海生馆的一大特色是它的夜宿项目，即一边观赏海洋动物一边入睡，想想都很浪漫！但费用颇高，贫穷使我们忍痛舍弃了这个难得的体验。&lt;/p&gt;

&lt;p&gt;海生馆分为三个展馆，分别为台湾水域馆、珊瑚王国馆、世界水域馆。台湾水域馆的一大特色在于设置有触摸池，每天定点开放，大人小孩洗干净手，就可以亲自摸摸海星、海胆等海洋生物，并有专人在旁边进行讲解和指导。珊瑚王国馆的一大特色在于有一条长长的海底隧道，可以看见各种海洋生物从你的头顶、身边穿过。据说这也是夜宿项目的热门地点。在珊瑚王国馆的尽头，还有微笑的白鲸不时窜到玻璃前与人们互动。世界水域馆中三层楼高的海藻森林非常震撼，而人们也常常被这里蠢萌的企鹅吸引而驻足观看。&lt;/p&gt;

&lt;p&gt;此外，海生馆的各个展馆分不同时间设置有喂食解说，值得观看。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslcrrkic7j31kw16i1kz.jpg&quot; alt=&quot;海生馆1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fsldfbyy8gj31kw0htu0x.jpg&quot; alt=&quot;海生馆2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从海生馆出来，乘坐垦丁街车再转垦丁快线，到达枋寮火车站，乘坐台铁前往花莲。值得注意的是，台铁的空调开得较足，因而随身携带一件薄外套可能会更适宜。&lt;/p&gt;

&lt;p&gt;本来以为台铁便当需要提前预定，或者在火车站直接购买，便在枋寮火车站附近匆匆吃了一餐。上了台铁发现，台铁服务员是会在饭点推着小车来兜售便当的。台铁便当真的非常好吃，价格也实惠，60新台币一份，有肉有菜，都很实在。相比大陆火车上的快餐实在是物美价廉。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fsj382y6f5j30sg0sgwik.jpg&quot; alt=&quot;台铁便当1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fsj38a6j31j30sg0sgwhx.jpg&quot; alt=&quot;台铁便当2&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;day-8-花莲&quot;&gt;Day 8 花莲&lt;/h2&gt;

&lt;p&gt;花莲是一个与大海紧密相连的城市，而这里的人们也一直在想方设法与大海、大自然产生联系。在这里，你可以乘船驶入太平洋，看海面上不时冲出水面的飞鱼和跃出水面炫技的海豚，浪花飞溅留下的白色晶体是太平洋最朴实的礼物；在这里，你可以沿着秀姑峦溪一路泛舟到达太平洋入海口，合着泥的溪水迸溅在身上，开出的是一张张灿烂的笑颜；在这里，你还可以勇闯太鲁阁，踏上或平缓或惊险的步道，与各类动物同处一片天地，各得其所。&lt;/p&gt;

&lt;h4 id=&quot;清水断崖&quot;&gt;📍清水断崖&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;清水断崖是一个我出发前非常期待，到达后仍非常喜欢的景点，透露着独到的美。一面是几近垂直的断崖，一面是澎湃的浪花，远处的海平面水光接天，身处其中只觉畅快与平静。&lt;/p&gt;

&lt;p&gt;前往清水断崖，包车或许会是一个更好的选择。我们当时是从花莲火车站乘坐区间车到达崇德站，然后沿着苏花公路一路向北，穿过崇德隧道，出口处即到达观景台。大概2公里的路程。路程不算远，但由于一路没有人行道，与各类土方车、大巴车、小轿车同道行走，因而走得胆战心惊，生怕一不注意就被撞飞。万幸的是最终平安往返，清水断崖的景观也没让我们失望。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslct0fjptj31kw11xhdz.jpg&quot; alt=&quot;清水断崖1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fslct6oqnwj31kw11x1kz.jpg&quot; alt=&quot;清水断崖2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslctflqe0j31kw11xb2b.jpg&quot; alt=&quot;清水断崖3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslctm1asxj31kw23vnpe.jpg&quot; alt=&quot;清水断崖4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从清水断崖回来，我们直奔“金三角”商圈！准备大吃一顿！&lt;/p&gt;

&lt;h4 id=&quot;公正包子&quot;&gt;🍱公正包子&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;人气是真的旺！到的时候店门口早已排起长龙。小笼包的口味较普通，没有吃出特别之处，倒是肉羹汤的味道不错。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslctxd5e0j31kw16o7wi.jpg&quot; alt=&quot;公正包子&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;戴记扁食&quot;&gt;🍱戴记扁食&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;花莲的扁食与厦门的扁食还是有些区别的，特别是在在扁肉的处理方式上，厦门的吃起来更偏肉羹口感，而花莲的则更似肉蓉口感，肉粒分明。扁食一颗水饺大小，泡在汤里活似一只只白色金鱼。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslcu70718j30sg0sgqal.jpg&quot; alt=&quot;戴记扁食店门&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fsj3bgl3xrj30sg0sg0u6.jpg&quot; alt=&quot;戴记扁食&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;一心泡泡冰&quot;&gt;🍹一心泡泡冰&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;饭后甜点。这是除了7-11唯一一家我们吃了两次的店。第一次吃的芒果雪花冰，下面是雪花冰，上面铺一层芒果块，最上面放一颗冰淇淋球。芒果很甜，在炎热的夏天来一大碗冰吃起来很舒服。但是吃完后，韬韬仍对店名的「泡泡冰」念念不忘，于是来了第二次，一人要了一碗泡泡冰。泡泡冰和泡泡并没有什么太大的关系（这是韬韬一直惦记着泡泡冰的原因之一，他以为是汽水冻的😅），有很多种口味，冰淇淋球模样。我点了龙眼味和百香果味，味道很浓，龙眼甜甜的，百香果酸酸的，里面有小小的果粒。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslcug01x7j30sg0sgn37.jpg&quot; alt=&quot;芒果雪花冰&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;赏鲸&quot;&gt;🐳赏鲸&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;赏鲸是我和韬韬都非常期待的一个项目。我们提前半个月左右在多罗满的官网上订的票。&lt;/p&gt;

&lt;p&gt;当天，多罗满的工作人员开车来民宿接我们，在多罗满公司接受如何穿救生衣，以及常见的鲸目生物的讲解。顺便一提，海豚也是鲸的一种，出海时看到的大多是海豚，抹香鲸等大型鲸则相当罕见，遇见全靠运气。&lt;/p&gt;

&lt;p&gt;一切准备就绪就出海啦！轮船不大，分上下两层。二层不会被轮船行驶时飞溅的浪花打湿，且视野较好，因而我们当时找了个二层靠边的座位坐下，满怀期待与海豚相遇。一路上，志愿者会对周围的景观进行介绍，比如出花莲港进入太平洋时经过的两个红、绿灯塔，太阳西斜时在海平面上照射出的金色大道等。&lt;/p&gt;

&lt;p&gt;我们的轮船行驶了好久，仍没有看见海豚的踪影，倒是偶然发现了几只飞鱼。它们冲出海平面在空中停留一会儿再坠入海中，就像空中飞翔的鸟儿，非常有意思。本来以为这次赏鲸之旅会以失败告终，最后在回港的路上正好碰上了一大群飞旋海豚！它们或成群结队地先后跃出海平面，或抬起脑袋用力往海平面砸去，更有兴奋的，在跃出海平面的同时在空中旋转几周，炫耀着自己的绝技，令船上的大人小孩们尖叫连连，大呼过瘾。海豚没有在轮船边逗留太久便接二连三朝着别的方向游走了；为了不打扰海豚的生活，轮船也没有在原地停留太久便离开了。曾经充满欢声笑语的海面再度归于平静。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fsj1az37hgg30hs0a0e83.gif&quot; alt=&quot;赏鲸1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tKfTcly1fsj1ae300vg30hs0a0hdz.gif&quot; alt=&quot;赏鲸2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;个人觉得，相比于将海洋生物囚禁于海洋馆中好吃好喝地供养着，不如将它们放归大海，让它们拥有真正的自由，并以出海配合志愿者讲解的形式进行观赏。原来，我们人类与大海、与海豚的距离可以那么近，真的是一次超级难忘的经历。强力推荐！&lt;/p&gt;

&lt;p&gt;赏完鲸，多罗满的工作人员直接将我们送往自强夜市。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fslcuv2womj31kw1kwe4h.jpg&quot; alt=&quot;自强夜市&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;北港春卷&quot;&gt;🍱北港春卷&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;人气超级旺的一家店。这里的春卷和厦门的类似，用一层薄薄的白色面皮包裹各种菜吃。做春卷的是一位老爷爷，一次摊开二十张面皮，一份春卷两张，一次做十份。即使后面排队的人再多，动作也慢悠悠的，非常典型的台湾人性格。春卷的味道很不错。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fsj3efw2jsj31kw1kwnpd.jpg&quot; alt=&quot;北港春卷摊位&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;妙不可言果汁&quot;&gt;🍹妙不可言果汁&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;也是人气超级旺的一家饮品店。水果的种类很多，还能做各种混合，因而选择特别多，令人眼花缭乱。果汁的味道不错，酸酸甜甜的。&lt;/p&gt;

&lt;h4 id=&quot;来来盐酥鸡&quot;&gt;🍱来来盐酥鸡&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;来台湾前一直惦记着吃鸡排，结果来了台湾八天，一块炸的鸡肉都没吃过。点了一份盐酥鸡，比大陆的盐酥鸡好吃太多！鸡肉一块挺大，能够吃出里面是真的鸡肉，不是大团面粉，表皮炸得酥酥的，是期待中的味道！&lt;/p&gt;

&lt;h4 id=&quot;蒋家官财饭&quot;&gt;🍱蒋家官财饭&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;在夜市的尽头，设有座位。官财饭其实是将一块炸过的吐司切开，在里面放入食材，像汉堡一样夹着吃。如果外带，店家会送一杯奶茶或红茶，在店里吃则奶茶、红茶自取。我点的凤梨虾球味，有虾球和凤梨块，再浇上沙拉，味道不错，一整个吃光光。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fsj3ednagsj31kw23vnpd.jpg&quot; alt=&quot;凤梨虾球官财饭&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;day-9-花莲&quot;&gt;Day 9 花莲&lt;/h2&gt;

&lt;h4 id=&quot;太鲁阁国家公园&quot;&gt;📍太鲁阁国家公园&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;太鲁阁给我的感觉比较「野」。虽然开辟了好多条步道，但仍最大限度地保留了当地的原始风貌，随处可见的大型蜘蛛网，原住民聚集地的保留，以及步道上提示各种野生动物出没的标志，都是最好的证明。太鲁阁的步道分三种，景观型步道的难度较低，老少咸宜，无需提前申请；健行型、登山型的难度较大，需要提前提交申请，并支付一定的费用。值得一提的是，在一些地质比较不稳定的地方，可能还需要佩戴安全帽。&lt;/p&gt;

&lt;p&gt;对于自由行又不想花费太多钱的人（也就是我们），台湾好行的一日套票是个不错的选择。巴士从花莲火车站出发，途径七星潭、砂卡礑、布洛湾、燕子口等地，可以一路玩到终点站天祥，再从天祥直接坐车返回。中途可无限次上下车。&lt;/p&gt;

&lt;p&gt;景观型步道中，砂卡礑步道、燕子口步道最为热门，均以平地为主。砂卡礑步道左临溪流右靠岩壁，身临其中，耳边是淙淙的溪水声，眼前是阳光下闪闪发光的溪流和不时飞过的白鹭鸶。此外，对岸岩壁上曲折多变的纹理也是一大看点。砂卡礑步道须原路返回。从太管处出发，有两条路可到达砂卡礑步道。一是沿着砂卡礑隧道走到头，二是从砂卡礑隧道的侧门出，穿过小錐麓步道，出口不远处即为砂卡礑步道的入口。顺便一提，小錐麓步道以阶梯为主，临近终点处有一座绳索桥，相比于砂卡礑步道，趣味性更强，难度也较大。好在步道并不长。&lt;/p&gt;

&lt;p&gt;燕子口步道绝大部分设在岩洞中，左边走车，右边走人。透过右手边开凿出的岩洞可望见对岸岩壁上一个个巴掌大小的洞，有的洞甚至还有水流出。燕子不时在峡谷中来回穿梭。值得一提的是，燕子口存在岩石掉落的危险，佩戴一顶安全帽会更有保障。&lt;/p&gt;

&lt;p&gt;除了以上三条步道，我们还去了布洛湾，冲着它的小剧场去的。但是去时恰逢布洛湾区域的休息日，小剧场没开，工作人员也不在，冷冷清清的。周围有几条步道，都不长，风景有限，在这里玩得并不尽兴。&lt;/p&gt;

&lt;p&gt;就餐问题可在天祥解决，有饭馆的家常菜、7-11的方便食品可供选择。&lt;/p&gt;

&lt;p&gt;插些题外话。个人觉得，徒步是来太鲁阁玩的重头戏。在此之前，我和韬韬苦练了半个多月，周末暴走奥森，平日绕着操场狂走。但是去太鲁阁时，因为出发得晚，对各条步道的情况了解不够，导致时间规划上出了点问题，走得并不尽兴。因而，在出发前做足功课，或者在游玩前向工作人员寻求必要的路线指导会使游玩过程更加顺畅。此外，这次太鲁阁之旅使我对健行型、登山型步道心生向往，下次再来争取征服它们💪&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslcvc69h3j30u60k4jxq.jpg&quot; alt=&quot;太鲁阁国家公园1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslcveut7aj30u60k4acz.jpg&quot; alt=&quot;太鲁阁国家公园2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslcvhxm54j30u60k449p.jpg&quot; alt=&quot;太鲁阁国家公园3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fslcvljp3dj30u60k47ed.jpg&quot; alt=&quot;太鲁阁国家公园4&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;黄车炸弹葱油饼&quot;&gt;🍱黄车炸弹葱油饼&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;现在想起来还有点嘴馋。葱油饼特别香，外面酥酥脆脆的，里面包了个鸡蛋，没有完全煎熟，软软的，特别好吃。&lt;/p&gt;

&lt;p&gt;我们六点半左右到的，幸运的买到了最后两个。当时突然下起了雨，两个人躲在对面的店铺门口吃，目睹好几个人来扑了空，生意是真的好。&lt;/p&gt;

&lt;h4 id=&quot;液香扁食&quot;&gt;🍱液香扁食&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;液香扁食和戴记扁食是花莲最有名的两家扁食店。两家口味大同小异，韬韬觉得完全没区别，我坚持认为液香更好吃。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslcvvk9umj31kw1kw7wh.jpg&quot; alt=&quot;液香扁食&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;day-10-花莲&quot;&gt;Day 10 花莲&lt;/h2&gt;

&lt;h4 id=&quot;泛舟&quot;&gt;🚣‍泛舟&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;泛舟是花莲比较有代表性的户外运动之一。从秀姑峦溪上游出发，沿着溪水一路向下，到长虹桥结束。那里已经非常接近太平洋的入海口了。&lt;/p&gt;

&lt;p&gt;工作人员一大早把我们从民宿接到泛舟中心，先接受安全教育，如救生衣如何穿、舟如何划、落水了怎么办等。接着，6-8人一组，一组一舟，就正式下水了。一开始溪水较平稳，我们以为泛舟就是用桨来使舟前进，顿觉无趣又疲惫。但到了水流较为湍急的部分，舟会上下左右晃动，人在其中颠三倒四，仿佛坐过山车一般，任凭冰凉的溪水打在身上，才真正发现泛舟的乐趣所在！泛舟的全过程会有救生员跟着，在大家泛舟无力时会推大家一把，或者索性用绳索拉着舟一路乘风破浪，好不舒服。&lt;/p&gt;

&lt;p&gt;泛舟过程会中有个午休时间，有免费的便当提供，每人一块大猪排，再配些蔬菜，味道还不错（但是和我们一起的花莲人表示并不好吃😅）。泛舟结束后，有淋浴房、吹风机等设施，非常人性化，可以干干净净地离开。&lt;/p&gt;

&lt;p&gt;泛舟结束后，我们直奔花莲火车站，乘坐台铁前往台北。在花莲火车站有特供便当出售，内容比普通台铁便当更丰富，不妨尝试看看。&lt;/p&gt;

&lt;p&gt;到达台北，看见马路上川流不息的各色车辆，不禁有一种回到大城市的感慨。&lt;/p&gt;

&lt;p&gt;因为住的酒店临近宁夏夜市，俩人收拾妥当后，决议前往宁夏夜市做最后的挣扎（第二天就要离开了😢）。&lt;/p&gt;

&lt;h4 id=&quot;猪肝荣仔&quot;&gt;🍱猪肝荣仔&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;有猪肝汤、猪肚汤和综合汤。为了两种都尝尝，我们点了综合汤。猪肝一点都不腥，汤的味道很好。当着老板娘的面一整碗吃光！&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslcw5q1eoj31kw1kwhdt.jpg&quot; alt=&quot;猪肝荣仔综合汤&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;刘芋仔&quot;&gt;🍱刘芋仔&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;我只吃了一口，还蛮香，其余都是韬韬一个人吃的，还是让韬韬来发言吧：&lt;/p&gt;

&lt;p&gt;「韬韬：芋圆包裹鸭蛋黄，放进油锅中炸至金黄。外皮酥脆，芋圆入口绵柔，蛋黄的咸味恰如其分。」&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fsj3ez0sbgj31kw1kwx6p.jpg&quot; alt=&quot;刘芋仔&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最后，韬韬又买了一杯50岚，我买了两盒释迦（超甜），心满意足地回了酒店。&lt;/p&gt;

&lt;h2 id=&quot;day-11-台北&quot;&gt;Day 11 台北&lt;/h2&gt;

&lt;h4 id=&quot;台湾大学&quot;&gt;📍台湾大学&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;台湾大学的建筑透露着一股浓浓的历史韵味。我们从正门进去，一路直走到图书馆。路上看见一位老教授带领着五六个学生围绕着一棵树做着讲解，而图书馆前方宽敞的绿色草坪上则不乏穿着学士服拍照的毕业生。校园里很安静，偶尔有三五个学生结伴边走路边聊着天，也不觉得吵，可能是人少的缘故。而在图书馆周围更是特意树立了个牌子，说明这里不允许喧哗和举办任何活动，充分体现了对学习、知识的尊重。&lt;/p&gt;

&lt;p&gt;去的那天天气实在太热，我们逛到图书馆便匆匆离开。如果能有机会在这里学习一段时间，应该会是一段很美好的经历。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fslcwm3gwbj31kw1kw1ky.jpg&quot; alt=&quot;台湾大学图书馆走廊&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;诚品敦南店&quot;&gt;📍诚品敦南店&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;这里简直是书虫们的天堂！书的种类很多，而且每本书几乎都有已经拆封、可供阅读的样书。书店设有座位，可坐着阅读。最棒的是书店是24小时营业的，想象着在这里彻夜阅读，也会是一次很不错的人生体验。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fslcwyi7z3j31kw16ox6p.jpg&quot; alt=&quot;诚品敦南店地下的一个展览&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;永康牛肉面&quot;&gt;🍱永康牛肉面&lt;/h4&gt;

&lt;p&gt;​     肥猫⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;​     韬韬⭐️⭐️⭐️⭐️&lt;/p&gt;

&lt;p&gt;人气超级旺。我们中午去的，还排了会儿队，客人多是一些提着行李箱的游客，应该是刚下飞机就赶来了。&lt;/p&gt;

&lt;p&gt;韬韬点的半筋半肉面，我点的半筋半肉粉。牛筋和牛肉都很大块，吃起来很厚实，块数也不少。当然，价格也不便宜，一碗面五六十人民币。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fslcxl66tsj31kw1kwu0y.jpg&quot; alt=&quot;半肉半筋粉&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;后记&quot;&gt;后记&lt;/h2&gt;

&lt;p&gt;终于以记流水账的形式把台湾之旅的全程记录下来。一来为以后的回忆留个线索，二来如果能帮助到有需要的人，也算意外的收获了。&lt;/p&gt;

&lt;p&gt;台湾给我的印象真的特别好。永远微笑着脸的服务人员，从他们身上得到的服务和信息往往超过我的预期。他们还很爱闲聊，比如在台湾办登机时，柜台的工作人员看见我们在台湾呆了十一天，问我们是不是环岛了，还和我们聊了聊都去了哪里，那种感觉就好像他是我们的一个朋友，有一种莫名的家的温暖。此外，我还很欣赏他们凡事都慢慢来的脾气，仿佛这世上没有什么是来不及的，如果有，小跑一下就好了。&lt;/p&gt;

&lt;p&gt;最后，以孤独星球里的一句话作为结尾吧，个人觉得以它形容台湾实在再恰当不过：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;太平洋的风不眠不休拂过这座岛屿，吹散着历史的离愁别恨，却吹不散浓浓的人情味，落在每个旅人的心头。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果有机会，我想再去很多次。&lt;/p&gt;
</description>
        <pubDate>Fri, 22 Jun 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/06/22/journey-tw/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/06/22/journey-tw/</guid>
        
        <category>游记</category>
        
        
      </item>
    
      <item>
        <title>Machine Learning Week 6</title>
        <description>&lt;h1 id=&quot;8-advice-for-applying-machine-learning&quot;&gt;8. Advice for Applying Machine Learning&lt;/h1&gt;

&lt;h2 id=&quot;81-evaluating-a-learning-algorithm&quot;&gt;8.1 Evaluating a Learning Algorithm&lt;/h2&gt;

&lt;h3 id=&quot;811-evaluating-a-hypothesis&quot;&gt;8.1.1 Evaluating a Hypothesis&lt;/h3&gt;

&lt;p&gt;Once we have done some trouble shooting for errors in our predictions by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Getting more training examples&lt;/li&gt;
  &lt;li&gt;Trying smaller sets of features&lt;/li&gt;
  &lt;li&gt;Trying additional features&lt;/li&gt;
  &lt;li&gt;Trying polynomial features&lt;/li&gt;
  &lt;li&gt;Increasing or decreasing $\lambda$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can move on to evaluate our new hypothesis.&lt;/p&gt;

&lt;p&gt;A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a &lt;strong&gt;training set&lt;/strong&gt; and a &lt;strong&gt;test set&lt;/strong&gt;. Typically, the training set consists of 70 % of your data and the test set is the remaining 30 %.&lt;/p&gt;

&lt;p&gt;The new procedure using these two sets is then:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Learn $\Theta$ and minimize $J_{train}(\Theta)$ using the training set&lt;/li&gt;
  &lt;li&gt;Compute the test set error $J_{test}(\Theta)$&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;the-test-set-error&quot;&gt;The test set error&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;For linear regression: $J_{test}(\Theta) = \dfrac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_\Theta(x^{(i)}&lt;em&gt;{test}) - y^{(i)}&lt;/em&gt;{test})^2$&lt;/li&gt;
  &lt;li&gt;For classification ~ Misclassification error (aka 0/1 misclassification error):&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
err(h_\Theta(x),y) = \begin{matrix} 1 &amp; \mbox{if } h_\Theta(x) \geq 0.5\ and\ y = 0\ or\ h_\Theta(x) &lt; 0.5\ and\ y = 1\newline 0 &amp; \mbox otherwise \end{matrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;This gives us a binary 0 or 1 error result based on a misclassification. The average test error for the test set is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Test Error} = \dfrac{1}{m_{test}} \sum^{m_{test}}_{i=1} err(h_\Theta(x^{(i)}_{test}), y^{(i)}_{test})&lt;/script&gt;

&lt;p&gt;This gives us the proportion of the test data that was misclassified.&lt;/p&gt;

&lt;h3 id=&quot;812-model-selection-and-trainvalidationtest-sets&quot;&gt;8.1.2 Model Selection and Train/Validation/Test Sets&lt;/h3&gt;

&lt;p&gt;Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. It could over fit and as a result your predictions on the test set would be poor. The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than the error on any other data set.&lt;/p&gt;

&lt;p&gt;Given many models with different polynomial degrees, we can use a systematic approach to identify the ‘best’ function. In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result.&lt;/p&gt;

&lt;p&gt;One way to break down our dataset into the three sets is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training set: 60%&lt;/li&gt;
  &lt;li&gt;Cross validation set: 20%&lt;/li&gt;
  &lt;li&gt;Test set: 20%&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can now calculate three separate error values for the three different sets using the following method:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Optimize the parameters in Θ using the training set for each polynomial degree.&lt;/li&gt;
  &lt;li&gt;Find the polynomial degree d with the least error using the cross validation set.&lt;/li&gt;
  &lt;li&gt;Estimate the generalization error using the test set with $J_{test}(\Theta^{(d)})$, (d = theta from polynomial with lower error);&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This way, the degree of the polynomial d has not been trained using the test set.&lt;/p&gt;

&lt;h2 id=&quot;82-bias-vs-variance&quot;&gt;8.2 Bias vs. Variance&lt;/h2&gt;

&lt;h3 id=&quot;821-diagnosing-bias-vs-variance&quot;&gt;8.2.1 Diagnosing Bias vs. Variance&lt;/h3&gt;

&lt;p&gt;In this section we examine the relationship between the degree of the polynomial d and the underfitting or overfitting of our hypothesis.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We need to distinguish whether &lt;strong&gt;bias&lt;/strong&gt; or &lt;strong&gt;variance&lt;/strong&gt; is the problem contributing to bad predictions.&lt;/li&gt;
  &lt;li&gt;High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The training error will tend to &lt;strong&gt;decrease&lt;/strong&gt; as we increase the degree d of the polynomial.&lt;/p&gt;

&lt;p&gt;At the same time, the cross validation error will tend to &lt;strong&gt;decrease&lt;/strong&gt; as we increase d up to a point, and then it will &lt;strong&gt;increase&lt;/strong&gt; as d is increased, forming a convex curve.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;High bias (underfitting)&lt;/strong&gt;: both $J_{train}(\Theta)$ and $ J_{CV}(\Theta)$ will be high. Also, $J_{CV}(\Theta) \approx J_{train}(\Theta)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;High variance (overfitting)&lt;/strong&gt;: $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be much greater than $J_{train}(\Theta)$.&lt;/p&gt;

&lt;p&gt;The is summarized in the figure below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79gy1frl0tbtmzjj308c073q3l.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;822-regularization-and-biasvariance&quot;&gt;8.2.2 Regularization and Bias/Variance&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; [The regularization term below and through out the video should be $\frac \lambda {2m} \sum _{j=1}^n \theta_j ^2$ and &lt;strong&gt;NOT&lt;/strong&gt; $\frac \lambda {2m} \sum _{j=1}^m \theta_j ^2$]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1frl1fgdjz8j30jj0akq4m.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the figure above, we see that as $\lambda$ increases, our fit becomes more rigid. On the other hand, as $\lambda$ approaches 0, we tend to over overfit the data. So how do we choose our parameter $\lambda$ to get it ‘just right’ ? In order to choose the model and the regularization term $\lambda$, we need to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Create a list of lambdas (i.e. $\lambda∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24}$);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a set of models with different degrees or any other variants.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Iterate through the $\lambda$s and for each $\lambda$ go through all the models to learn some $\Theta$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compute the cross validation error using the learned $\Theta$ (computed with $\lambda$) on the $J_{CV}(\Theta)$ &lt;strong&gt;without&lt;/strong&gt; regularization or $\lambda$ = 0.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Select the best combo that produces the lowest error on the cross validation set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using the best combo $\Theta$ and $\lambda$, apply it on $ J_{test}(\Theta)$ to see if it has a good generalization of the problem.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;823-learning-curves&quot;&gt;8.2.3 Learning Curves&lt;/h3&gt;

&lt;p&gt;Training an algorithm on a very few number of data points (such as 1, 2 or 3) will easily have 0 errors because we can always find a quadratic curve that touches exactly those number of points. Hence:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As the training set gets larger, the error for a quadratic function increases.&lt;/li&gt;
  &lt;li&gt;The error value will plateau out after a certain m, or training set size.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Experiencing high bias:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Low training set size&lt;/strong&gt;: causes $J_{train}(\Theta)$ to be low and $J_{CV}(\Theta)$ to be high.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Large training set size&lt;/strong&gt;: causes both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ to be high with $J_{train}(\Theta)\approx J_{CV}(\Theta)$.&lt;/p&gt;

&lt;p&gt;If a learning algorithm is suffering from &lt;strong&gt;high bias&lt;/strong&gt;, getting more training data will not &lt;strong&gt;(by itself)&lt;/strong&gt; help much.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1frl2eobr1kj308c056q3e.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Experiencing high variance:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Low training set size&lt;/strong&gt;: $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be high.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Large training set size&lt;/strong&gt;: $J_{train}(\Theta)$ increases with training set size and $J_{CV}(\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\Theta) &amp;lt; J_{CV}(\Theta)$ but the difference between them remains significant.&lt;/p&gt;

&lt;p&gt;If a learning algorithm is suffering from &lt;strong&gt;high variance&lt;/strong&gt;, getting more training data is likely to help.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1frl2epp3moj308c04rdgb.jpg&quot; alt=&quot;img&quot; /&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1frl2etwmn4j308c04rdgb.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;824-deciding-what-to-do-next-revisited&quot;&gt;8.2.4 Deciding What to Do Next Revisited&lt;/h3&gt;

&lt;p&gt;Our decision process can be broken down as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Getting more training examples:&lt;/strong&gt; Fixes high variance&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Trying smaller sets of features:&lt;/strong&gt; Fixes high variance&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Adding features:&lt;/strong&gt; Fixes high bias&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Adding polynomial features:&lt;/strong&gt; Fixes high bias&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Decreasing $\lambda$:&lt;/strong&gt; Fixes high bias&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Increasing $\lambda$:&lt;/strong&gt; Fixes high variance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;825-diagnosing-neural-networks&quot;&gt;8.2.5 Diagnosing Neural Networks&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A neural network with fewer parameters is &lt;strong&gt;prone to underfitting&lt;/strong&gt;. It is also &lt;strong&gt;computationally cheaper&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;A large neural network with more parameters is &lt;strong&gt;prone to overfitting&lt;/strong&gt;. It is also &lt;strong&gt;computationally expensive&lt;/strong&gt;. In this case you can use regularization (increase $\lambda$) to address the overfitting.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using your cross validation set. You can then select the one that performs best.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model Complexity Effects:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.&lt;/li&gt;
  &lt;li&gt;Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.&lt;/li&gt;
  &lt;li&gt;In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;83-building-a-spam-classifier&quot;&gt;8.3 Building a Spam Classifier&lt;/h2&gt;

&lt;h3 id=&quot;831-prioritizing-what-to-work-on&quot;&gt;8.3.1 Prioritizing What to Work On&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;System Design Example:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Given a data set of emails, we could construct a vector for each email. Each entry in this vector represents a word. The vector normally contains 10,000 to 50,000 entries gathered by finding the most frequently used words in our data set. If a word is to be found in the email, we would assign its respective entry a 1, else if it is not found, that entry would be a 0. Once we have all our x vectors ready, we train our algorithm and finally, we could use it to classify if an email is a spam or not.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1frm85bqkz7j30ki09mdk2.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So how could you spend your time to improve the accuracy of this classifier?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Collect lots of data (for example “honeypot” project but doesn’t always work)&lt;/li&gt;
  &lt;li&gt;Develop sophisticated features (for example: using email header data in spam emails)&lt;/li&gt;
  &lt;li&gt;Develop algorithms to process your input in different ways (recognizing misspellings in spam).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is difficult to tell which of the options will be most helpful.&lt;/p&gt;

&lt;h3 id=&quot;832-error-analysis&quot;&gt;8.3.2 Error Analysis&lt;/h3&gt;

&lt;p&gt;The recommended approach to solving machine learning problems is to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start with a simple algorithm, implement it quickly, and test it early on your cross validation data.&lt;/li&gt;
  &lt;li&gt;Plot learning curves to decide if more data, more features, etc. are likely to help.&lt;/li&gt;
  &lt;li&gt;Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, assume that we have 500 emails and our algorithm misclassifies a 100 of them. We could manually analyze the 100 emails and categorize them based on what type of emails they are. We could then try to come up with new cues and features that would help us classify these 100 emails correctly. Hence, if most of our misclassified emails are those which try to steal passwords, then we could find some features that are particular to those emails and add them to our model. We could also see how classifying each word according to its root changes our error rate:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79gy1frm8mnk8l5j30jg0a143a.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is very important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm’s performance. For example if we use stemming, which is the process of treating the same word with different forms (fail/failing/failed) as one word (fail), and get a 3% error rate instead of 5%, then we should definitely add it to our model. However, if we try to distinguish between upper case and lower case letters and end up getting a 3.2% error rate instead of 3%, then we should avoid using this new feature. Hence, we should try new things, get a numerical value for our error rate, and based on our result decide whether we want to keep the new feature or not.&lt;/p&gt;

&lt;h2 id=&quot;84-handling-skewed-data&quot;&gt;8.4 Handling Skewed Data&lt;/h2&gt;

&lt;h3 id=&quot;841-error-metrics-for-skewed-classes&quot;&gt;8.4.1 Error Metrics for Skewed Classes&lt;/h3&gt;

&lt;p&gt;Precision and recall are defined according to:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1frm90iwi53j30ae0akweg.jpg&quot; alt=&quot;If predicted class and actual class are both 1, then a test example is a True Positive. If predicted class and actual class are both 0, then a test example is a True Negative. If predicted class is 0 actual class is 1, then a test example is a False Negative. If predicted class is 1 and actual class is 0, then a test example is a False Positive.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Precision&lt;/strong&gt; (Of all patients where we predicted $y=1$, what fraction actually has cancer?)
&lt;script type=&quot;math/tex&quot;&gt;\text{Precision} = \frac{\text{True positives}}{\text{# predicted as positive}} = \frac{\text{True positives}}{\text{True positives + False positives}}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recall&lt;/strong&gt; (Of all patients that actually have cancer, what fraction did we correctly detect as having cancer?)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Recall} = \frac{\text{True positives}}{\text{# actual positives}} = \frac{\text{True positives}}{\text{True positives + False negatives}}&lt;/script&gt;

&lt;h3 id=&quot;842-trading-off-precision-an-recall&quot;&gt;8.4.2 Trading Off Precision an Recall&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79gy1frm9eyehi3j30q70eqdim.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79gy1frm9m7r3jpj30q80ergnj.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Tue, 22 May 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/05/22/ml-week6/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/22/ml-week6/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>Machine Learning Week 5</title>
        <description>&lt;h1 id=&quot;7-neural-networks-learning&quot;&gt;7. Neural Networks: Learning&lt;/h1&gt;

&lt;h2 id=&quot;71-backpropagation&quot;&gt;7.1 Backpropagation&lt;/h2&gt;

&lt;h3 id=&quot;711-cost-function&quot;&gt;7.1.1 Cost Function&lt;/h3&gt;

&lt;p&gt;Let’s first define a few variables that we will need to use:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;L = total number of layers in the network&lt;/li&gt;
  &lt;li&gt;$s_l$ = number of units (not counting bias unit) in layer l&lt;/li&gt;
  &lt;li&gt;K = number of output units/classes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recall that in neural networks, we may have many output nodes. We denote $h_\Theta(x)_k$ as being a hypothesis that results in the $k^{th}$ output. Our cost function for neural networks is going to be a generalization of the one we used for logistic regression. Recall that the cost function for regularized logistic regression was:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(θ)=−\frac{1}{m}∑_{i=1}^m[y^{(i)} log(h_\theta(x^{(i)}))+(1−y^{(i)}) log(1−h_\theta(x^{(i)}))]+\frac\lambda{2m}∑_{j=1}^nθ_j^2&lt;/script&gt;

&lt;p&gt;For neural networks, it is going to be slightly more complicated:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gather*} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather*}&lt;/script&gt;

&lt;p&gt;We have added a few nested summations to account for our multiple output nodes. In the first part of the equation, before the square brackets, we have an additional nested summation that loops through the number of output nodes.&lt;/p&gt;

&lt;p&gt;In the regularization part, after the square brackets, we must account for multiple theta matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term.&lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the double sum simply adds up the logistic regression costs calculated for each cell in the output layer&lt;/li&gt;
  &lt;li&gt;the triple sum simply adds up the squares of all the individual Θs in the entire network.&lt;/li&gt;
  &lt;li&gt;the i in the triple sum does &lt;strong&gt;not&lt;/strong&gt; refer to training example i&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;712-backpropagation-algorithm&quot;&gt;7.1.2 Backpropagation Algorithm&lt;/h3&gt;

&lt;p&gt;“Backpropagation” is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression. Our goal is to compute:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_\Theta J(\Theta)&lt;/script&gt;

&lt;p&gt;That is, we want to minimize our cost function J using an optimal set of parameters in theta. In this section we’ll look at the equations we use to compute the partial derivative of J(Θ):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta)&lt;/script&gt;

&lt;p&gt;To do so, we use the following algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tKfTcgy1fredq4s50ij30om0d9gs6.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Back propagation Algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Given training set $\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Set $\Delta^{(l)}_{i,j} := 0$ for all (l,i,j), (hence you end up having a matrix full of zeros)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For training example t =1 to m:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Set $a^{(1)} := x^{(t)}$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Perform forward propagation to compute $a^{(l)}$ for l=2,3,…,L&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcgy1frkciksidkj30d8074q53.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Using $y^{(t)}$, compute $\delta^{(L)} = a^{(L)} - y^{(t)}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Where L is our total number of layers and $a^{(L)}$ is the vector of outputs of the activation units for the last layer. So our “error values” for the last layer are simply the differences of our actual results in the last layer and the correct outputs in y. To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Compute $\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}$ using $\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .&lt;em&gt;\ a^{(l)}\ .&lt;/em&gt;\ (1 - a^{(l)})$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The delta values of layer l are calculated by multiplying the delta values in the next layer with the theta matrix of layer l. We then element-wise multiply that with a function called g’, or g-prime, which is the derivative of the activation function g evaluated with the input values given by $z^{(l)}$.&lt;/p&gt;

&lt;p&gt;The g-prime derivative terms can also be written out as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g'(z^{(l)}) = a^{(l)}\ .*\ (1 - a^{(l)})&lt;/script&gt;

&lt;ol&gt;
  &lt;li&gt;$\Delta^{(l)}&lt;em&gt;{i,j} := \Delta^{(l)}&lt;/em&gt;{i,j} + a_j^{(l)} \delta_i^{(l+1)}$ or with vectorization, $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Hence we update our new $\Delta$ matrix.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$D^{(l)}&lt;em&gt;{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}&lt;/em&gt;{i,j} + \lambda\Theta^{(l)}_{i,j}\right)$, if j≠0.&lt;/li&gt;
  &lt;li&gt;$D^{(l)}&lt;em&gt;{i,j} := \dfrac{1}{m}\Delta^{(l)}&lt;/em&gt;{i,j}$ If j=0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The capital-delta matrix D is used as an “accumulator” to add up our values as we go along and eventually compute our partial derivative. Thus we get $\frac \partial {\partial \Theta_{ij}^{(l)}} J(\Theta)= D_{ij}^{(l)}$&lt;/p&gt;

&lt;h3 id=&quot;713-backpropagation-intuition&quot;&gt;7.1.3 Backpropagation Intuition&lt;/h3&gt;

&lt;p&gt;Recall that the cost function for a neural network is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gather*}J(\Theta) = - \frac{1}{m} \sum_{t=1}^m\sum_{k=1}^K \left[ y^{(t)}_k \ \log (h_\Theta (x^{(t)}))_k + (1 - y^{(t)}_k)\ \log (1 - h_\Theta(x^{(t)})_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_l+1} ( \Theta_{j,i}^{(l)})^2\end{gather*}&lt;/script&gt;

&lt;p&gt;If we consider simple non-multiclass classification (k = 1) and disregard regularization, the cost is computed with:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;cost(t)=y^{(t)}log(h_\Theta(x^{(t)}))+(1-y^{(t)})log(1-h_\Theta(x^{(t)}))&lt;/script&gt;

&lt;p&gt;Intuitively, $\delta_j^{(l)}$ is the “error” for $a^{(l)}_j$ (unit j in layer l). More formally, the delta values are actually the derivative of the cost function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta_j^{(l)}=\frac \partial {\partial z_j^{(l)}}cost(t)&lt;/script&gt;

&lt;p&gt;Recall that our derivative is the slope of a line tangent to the cost function, so the steeper the slope the more incorrect we are. Let us consider the following neural network below and see how we could calculate some $\delta_j^{(l)}$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcly1frekhy8t1bj30k00b9wi3.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the image above, to calculate $\delta_2^{(2)}$, we multiply the weights $\Theta_{12}^{(2)}$ and $\Theta_{22}^{(2)}$ by their respective $\delta$ values found to the right of each edge. So we get $\delta_2^{(2)}= \Theta_{12}^{(2)}&lt;em&gt;\delta_1^{(3)}+\Theta_{22}^{(2)}&lt;/em&gt;\delta_2^{(3)}$. To calculate every single possible $\delta_j^{(l)}$, we could start from the right of our diagram. We can think of our edges as our $\Theta_{ij}$. Going from right to left, to calculate the value of $\delta_j^{(l)}$, you can just take the over all sum of each weight times the $\delta$ it is coming from. Hence, another example would be $\delta_2^{(3)}=\Theta_{12}^{(3)}*\delta_1^{(4)}$.&lt;/p&gt;

&lt;h2 id=&quot;72-backpropagation-in-practice&quot;&gt;7.2 Backpropagation in Practice&lt;/h2&gt;

&lt;h3 id=&quot;721-implementation-note-unrolling-parameters&quot;&gt;7.2.1 Implementation Note: Unrolling Parameters&lt;/h3&gt;

&lt;p&gt;With neural networks, we are working with sets of matrices:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*} \Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}, \dots \newline D^{(1)}, D^{(2)}, D^{(3)}, \dots \end{align*}&lt;/script&gt;

&lt;p&gt;In order to use optimizing functions such as “fminunc()”, we will want to “unroll” all the elements and put them into one long vector:&lt;/p&gt;

&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;thetaVector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Theta1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Theta2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Theta3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:);&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;deltaVector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11, then we can get back our original matrices from the “unrolled” versions as follows:&lt;/p&gt;

&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Theta1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thetaVector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;110&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Theta2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thetaVector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;111&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;220&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Theta3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thetaVector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;221&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;231&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To summarize:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tKfTcly1frekyv4avsj30ey06aq57.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;722-gradient-checking&quot;&gt;7.2.2 Gradient Checking&lt;/h3&gt;

&lt;p&gt;Gradient checking will assure that our backpropagation works as intended. We can approximate the derivative of our cost function with:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}&lt;/script&gt;

&lt;p&gt;With multiple theta matrices, we can approximate the derivative &lt;strong&gt;with respect to&lt;/strong&gt; Θ_jΘj as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}&lt;/script&gt;

&lt;p&gt;A small value for ${\epsilon}$ (epsilon) such as ${\epsilon = 10^{-4}}$, guarantees that the math works out properly. If the value for $\epsilon$ is too small, we can end up with numerical problems.&lt;/p&gt;

&lt;p&gt;Hence, we are only adding or subtracting epsilon to the $\Theta_j$ matrix. In octave we can do it as follows:&lt;/p&gt;

&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;thetaPlus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;thetaPlus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;thetaMinus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;thetaMinus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gradApprox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;J&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thetaPlus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;J&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thetaMinus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))/(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We previously saw how to calculate the deltaVector. So once we compute our gradApprox vector, we can check that gradApprox $ \approx$ deltaVector.&lt;/p&gt;

&lt;p&gt;Once you have verified &lt;strong&gt;once&lt;/strong&gt; that your backpropagation algorithm is correct, you don’t need to compute gradApprox again. The code to compute gradApprox can be very slow.&lt;/p&gt;

&lt;h3 id=&quot;723-random-initialization&quot;&gt;7.2.3 Random Initialization&lt;/h3&gt;

&lt;p&gt;Initializing all theta weights to zero does not work with neural networks. When we backpropagate, all nodes will update to the same value repeatedly. Instead we can randomly initialize our weights for our \ThetaΘmatrices using the following method:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcly1frgukh5tezj30fb07qjsk.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hence, we initialize each $\Theta^{(l)}_{ij}$ to a random value between $[-\epsilon,\epsilon]$. Using the above formula guarantees that we get the desired bound. The same procedure applies to all the $\Theta$’s. Below is some working code you could use to experiment.&lt;/p&gt;

&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;If&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dimensions&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Theta1&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Theta2&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x11&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Theta3&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x11&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Theta1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INIT_EPSILON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INIT_EPSILON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Theta2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INIT_EPSILON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INIT_EPSILON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Theta3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INIT_EPSILON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INIT_EPSILON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;rand(x,y) is just a function in octave that will initialize a matrix of random real numbers between 0 and 1.&lt;/p&gt;

&lt;p&gt;(Note: the epsilon used above is unrelated to the epsilon from Gradient Checking)&lt;/p&gt;

&lt;h3 id=&quot;724-putting-it-together&quot;&gt;7.2.4 Putting it Together&lt;/h3&gt;

&lt;p&gt;First, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers in total you want to have.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Number of input units = dimension of features $x^{(i)}$&lt;/li&gt;
  &lt;li&gt;Number of output units = number of classes&lt;/li&gt;
  &lt;li&gt;Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units)&lt;/li&gt;
  &lt;li&gt;Defaults: 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Training a Neural Network&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Randomly initialize the weights&lt;/li&gt;
  &lt;li&gt;Implement forward propagation to get $h_\Theta(x^{(i)})$ for any $x^{(i)}$&lt;/li&gt;
  &lt;li&gt;Implement the cost function&lt;/li&gt;
  &lt;li&gt;Implement backpropagation to compute partial derivatives&lt;/li&gt;
  &lt;li&gt;Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.&lt;/li&gt;
  &lt;li&gt;Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When we perform forward and back propagation, we loop on every training example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for i = 1:m,
   Perform forward propagation and backpropagation using example (x(i),y(i))
   (Get activations a(l) and delta terms d(l) for l = 2,...,L
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following image gives us an intuition of what is happening as we are implementing our neural network:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tKfTcly1frgukfnqmcj30eu08aq5g.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ideally, you want $h_\Theta(x^{(i)}) \approx y^{(i)}$. This will minimize our cost function. However, keep in mind that $J(\Theta)$ is not convex and thus we can end up in a local minimum instead.&lt;/p&gt;
</description>
        <pubDate>Mon, 14 May 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/05/14/ml-week5/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/14/ml-week5/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>Machine Learning Week 4</title>
        <description>&lt;h1 id=&quot;6-neural-networks-representation&quot;&gt;6. Neural Networks: Representation&lt;/h1&gt;

&lt;h2 id=&quot;61-neural-networks&quot;&gt;6.1 Neural Networks&lt;/h2&gt;

&lt;h3 id=&quot;611-model-representation-i&quot;&gt;6.1.1 Model Representation I&lt;/h3&gt;

&lt;p&gt;Let’s examine how we will represent a hypothesis function using neural networks. At a very simple level, neurons are basically computational units that take inputs (&lt;strong&gt;dendrites&lt;/strong&gt;) as electrical inputs (called “spikes”) that are channeled to outputs (&lt;strong&gt;axons&lt;/strong&gt;). In our model, our dendrites are like the input features $x_1\cdots x_n$, and the output is the result of our hypothesis function. In this model our $x_0$ input node is sometimes called the “bias unit.” It is always equal to 1. In neural networks, we use the same logistic function as in classification, $\frac{1}{1 + e^{-\theta^Tx}}$, yet we sometimes call it a sigmoid (logistic) &lt;strong&gt;activation&lt;/strong&gt; function. In this situation, our “theta” parameters are sometimes called “weights”.&lt;/p&gt;

&lt;p&gt;Visually, a simplistic representation looks like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline \end{bmatrix}\rightarrow\begin{bmatrix}\ \ \ \newline \end{bmatrix}\rightarrow h_\theta(x)&lt;/script&gt;

&lt;p&gt;Our input nodes (layer 1), also known as the “input layer”, go into another node (layer 2), which finally outputs the hypothesis function, known as the “output layer”.&lt;/p&gt;

&lt;p&gt;We can have intermediate layers of nodes between the input and output layers called the “hidden layers.”&lt;/p&gt;

&lt;p&gt;In this example, we label these intermediate or “hidden” layer nodes $a^2_0 \cdots a^2_n$and call them “activation units.”&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; a_i^{(j)} = \text{&quot;activation&quot; of unit $i$ in layer $j$} \newline&amp; \Theta^{(j)} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$}\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;If we had one hidden layer, it would look like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline x_3\end{bmatrix}\rightarrow\begin{bmatrix}a_1^{(2)} \newline a_2^{(2)} \newline a_3^{(2)} \newline \end{bmatrix}\rightarrow h_\theta(x)&lt;/script&gt;

&lt;p&gt;The values for each of the “activation” nodes is obtained as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*}&lt;/script&gt;

&lt;p&gt;This is saying that we compute our activation nodes by using a 3×4 matrix of parameters. We apply each row of the parameters to our inputs to obtain the value for one activation node. Our hypothesis output is the logistic function applied to the sum of the values of our activation nodes, which have been multiplied by yet another parameter matrix $\Theta^{(2)}$ containing the weights for our second layer of nodes.&lt;/p&gt;

&lt;p&gt;Each layer gets its own matrix of weights, $\Theta^{(j)}$.&lt;/p&gt;

&lt;p&gt;The dimensions of these matrices of weights is determined as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{If network has }{s_j}\text{ units in layer j and }{s_{j+1}}\text{ units in layer j+1, then }{\Theta^{(j)}}\text{ will be of dimension }{s_{j+1} \times (s_j + 1)}\text{ .}&lt;/script&gt;

&lt;p&gt;The +1 comes from the addition in $\Theta^{(j)}$ of the “bias nodes,” $x_0$ and $\Theta_0^{(j)}$. In other words the output nodes will not include the bias nodes while the inputs will. The following image summarizes our model representation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcly1fr2jxyyawgj30jk0ay772.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Example: If layer 1 has 2 input nodes and layer 2 has 4 activation nodes. Dimension of $\Theta^{(1)}$ is going to be 4×3 where $ s_j = 2$ and $s_{j+1} = 4$, so $s_{j+1} \times (s_j + 1) = 4 \times 3$.&lt;/p&gt;

&lt;h3 id=&quot;612-model-representation-ii&quot;&gt;6.1.2 Model Representation II&lt;/h3&gt;

&lt;p&gt;To re-iterate, the following is an example of a neural network:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*}&lt;/script&gt;

&lt;p&gt;In this section we’ll do a vectorized implementation of the above functions. We’re going to define a new variable $z_k^{(j)}$zk(j) that encompasses the parameters inside our g function. In our previous example if we replaced by the variable z for all the parameters we would get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}a_1^{(2)} = g(z_1^{(2)}) \newline a_2^{(2)} = g(z_2^{(2)}) \newline a_3^{(2)} = g(z_3^{(2)}) \newline \end{align*}&lt;/script&gt;

&lt;p&gt;In other words, for layer j=2 and node k, the variable z will be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_k^{(2)}=\Theta_{k,0}^{(1)}x_0+\Theta_{k,1}^{(1)}x_1+\cdots+\Theta_{k,n}^{(1)}x_n&lt;/script&gt;

&lt;p&gt;The vector representation of x and $z^{j}$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}x = \begin{bmatrix}x_0 \newline x_1 \newline\cdots \newline x_n\end{bmatrix} &amp;z^{(j)} = \begin{bmatrix}z_1^{(j)} \newline z_2^{(j)} \newline\cdots \newline z_n^{(j)}\end{bmatrix}\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Setting $x = a^{(1)}$, we can rewrite the equation as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z^{(j)}=\Theta^{(j-1)}a^{(j-1)}&lt;/script&gt;

&lt;p&gt;We are multiplying our matrix $\Theta^{(j-1)}$ with dimensions $s_j\times (n+1)$ (where s_jsj is the number of our activation nodes) by our vector $a^{(j-1)}$ with height (n+1). This gives us our vector $z^{(j)}$ with height $s_j$. Now we can get a vector of our activation nodes for layer j as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a^{(j+1)}=\Theta^{(j)}a^{(j)}&lt;/script&gt;

&lt;p&gt;We get this final z vector by multiplying the next theta matrix after $\Theta^{(j-1)}$ with the values of all the activation nodes we just got. This last theta matrix $\Theta^{(j)}$ will have only &lt;strong&gt;one row&lt;/strong&gt; which is multiplied by one column $a^{(j)}$ so that our result is a single number. We then get our final result with:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\Theta(x)=a^{(j+1)}=g(z^{(j+1)})&lt;/script&gt;

&lt;p&gt;Notice that in this &lt;strong&gt;last step&lt;/strong&gt;, between layer j and layer j+1, we are doing &lt;strong&gt;exactly the same thing&lt;/strong&gt; as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypotheses.&lt;/p&gt;

&lt;h2 id=&quot;62-applications&quot;&gt;6.2 Applications&lt;/h2&gt;

&lt;h3 id=&quot;621-examples-and-intuitions-i&quot;&gt;6.2.1 Examples and Intuitions I&lt;/h3&gt;

&lt;p&gt;A simple example of applying neural networks is by predicting $x_1$ AND $x_2$, which is the logical ‘and’ operator and is only true if both $x_1$ and $x_2$ are 1.&lt;/p&gt;

&lt;p&gt;The graph of our functions will look like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}\begin{bmatrix}x_0 \newline x_1 \newline x_2\end{bmatrix} \rightarrow\begin{bmatrix}g(z^{(2)})\end{bmatrix} \rightarrow h_\Theta(x)\end{align*}&lt;/script&gt;

&lt;p&gt;Remember that $x_0$ is our bias variable and is always 1.&lt;/p&gt;

&lt;p&gt;Let’s set our first theta matrix as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Theta^{(1)}=[-30\ 20\ 20]&lt;/script&gt;

&lt;p&gt;This will cause the output of our hypothesis to only be positive if both $x_1$ and $x_2$ are 1. In other words:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; h_\Theta(x) = g(-30 + 20x_1 + 20x_2) \newline \newline &amp; x_1 = 0 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-30) \approx 0 \newline &amp; x_1 = 0 \ \ and \ \ x_2 = 1 \ \ then \ \ g(-10) \approx 0 \newline &amp; x_1 = 1 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-10) \approx 0 \newline &amp; x_1 = 1 \ \ and \ \ x_2 = 1 \ \ then \ \ g(10) \approx 1\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;So we have constructed one of the fundamental operations in computers by using a small neural network rather than using an actual AND gate. Neural networks can also be used to simulate all the other logical gates. The following is an example of the logical operator ‘OR’, meaning either $x_1$ is true or $x_2$ is true, or both:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tKfTcly1fr2qx34y7mj30gb07raaw.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where g(z) is the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tKfTcly1fr2qx34y7mj30gb07raaw.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;622-examples-and-intuitions-ii&quot;&gt;6.2.2 Examples and Intuitions II&lt;/h3&gt;

&lt;p&gt;The $Θ^{(1)}$ matrices for AND, NOR, and OR are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}AND:\newline\Theta^{(1)} &amp;=\begin{bmatrix}-30 &amp; 20 &amp; 20\end{bmatrix} \newline NOR:\newline\Theta^{(1)} &amp;= \begin{bmatrix}10 &amp; -20 &amp; -20\end{bmatrix} \newline OR:\newline\Theta^{(1)} &amp;= \begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix} \newline\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can combine these to get the XNOR logical operator (which gives 1 if $x_1$ and $x_2$ are both 0 or both 1).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}\begin{bmatrix}x_0 \newline x_1 \newline x_2\end{bmatrix} \rightarrow\begin{bmatrix}a_1^{(2)} \newline a_2^{(2)} \end{bmatrix} \rightarrow\begin{bmatrix}a^{(3)}\end{bmatrix} \rightarrow h_\Theta(x)\end{align*}&lt;/script&gt;

&lt;p&gt;For the transition between the first and second layer, we’ll use a $Θ^{(1)}$ matrix that combines the values for AND and NOR:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\Theta^{(1)} =\begin{bmatrix}-30 &amp; 20 &amp; 20 \newline 10 &amp; -20 &amp; -20\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;For the transition between the second and third layer, we’ll use a $Θ^{(2)}$ matrix that uses the value for OR:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Theta^{(2)}=[-10\ 20\ 20]&lt;/script&gt;

&lt;p&gt;Let’s write out the values for all our nodes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; a^{(2)} = g(\Theta^{(1)} \cdot x) \newline&amp; a^{(3)} = g(\Theta^{(2)} \cdot a^{(2)}) \newline&amp; h_\Theta(x) = a^{(3)}\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;And there we have the XNOR operator using a hidden layer with two nodes! The following summarizes the above algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcly1fr2rikyh1ij30hb09h40o.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;623-multiclass-classification&quot;&gt;6.2.3 Multiclass Classification&lt;/h3&gt;

&lt;p&gt;To classify data into multiple classes, we let our hypothesis function return a vector of values. Say we wanted to classify our data into one of four categories. We will use the following example to see how this classification is done. This algorithm takes as input an image and classifies it accordingly:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcly1fr2rxa2aybj30h309en0f.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can define our set of resulting classes as y:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y^{(i)}=\begin {bmatrix}1\\0\\0\\0\end {bmatrix} ,\begin {bmatrix}0\\1\\0\\0\end {bmatrix} ,\begin {bmatrix}0\\0\\1\\0\end {bmatrix} ,\begin {bmatrix}0\\0\\0\\1\end {bmatrix}&lt;/script&gt;

&lt;p&gt;Each $y^{(i)}$ represents a different image corresponding to either a car, pedestrian, truck, or motorcycle. The inner layers, each provide us with some new information which leads to our final hypothesis function. The setup looks like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin {bmatrix}x_0\\x_1\\x_2\\\cdots\\x_n\end {bmatrix} \rightarrow \begin {bmatrix}a_0^{(2)}\\a_1^{(2)}\\a_2^{(2)}\\\cdots\end {bmatrix} \rightarrow \begin {bmatrix}a_0^{(3)}\\a_1^{(3)}\\a_2^{(3)}\\\cdots\end {bmatrix} \rightarrow \cdots \rightarrow \begin {bmatrix}h_\Theta(x)_1\\h_\Theta(x)_2\\h_\Theta(x)_3\\h_\Theta(x)_4\end {bmatrix}&lt;/script&gt;

&lt;p&gt;Our resulting hypothesis for one set of inputs may look like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\Theta(x) =\begin{bmatrix}0 \newline 0 \newline 1 \newline 0 \newline\end{bmatrix}&lt;/script&gt;

&lt;p&gt;In which case our resulting class is the third one down, or $h_\Theta(x)_3$, which represents the motorcycle.&lt;/p&gt;
</description>
        <pubDate>Mon, 07 May 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/05/07/ml-week4/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/07/ml-week4/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>学习如何学习</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;活到老，学到老&lt;/p&gt;

  &lt;p&gt;学习如何学习是Coursera的一门课，详见&lt;a href=&quot;https://www.coursera.org/learn/learning-how-to-learn&quot;&gt;此处&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;去掉了一些鸡汤，留下来了一点鸡胸，慢慢嚼吧&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;1-集中与发散思维&quot;&gt;1. 集中与发散思维&lt;/h2&gt;

&lt;p&gt;人有两种思维模式：专注模式和发散模式。这两种思维模式只能同时激活其中之一。大脑需要练习这两种模式的切换，以针对不同的问题。这样，才能发挥这两种思维模式的各自优势。通常，发散模式在大脑较为放松的时候被激活，而专注模式在完成一系列相关过程时被激活。&lt;/p&gt;

&lt;h2 id=&quot;2-拖延症记忆与睡眠&quot;&gt;2. 拖延症、记忆与睡眠&lt;/h2&gt;

&lt;h3 id=&quot;21-拖延症&quot;&gt;2.1 拖延症&lt;/h3&gt;

&lt;p&gt;当人们遇到一件极其不愿意做的事情时，大脑中与疼痛感相关的区域被激活。自然而然，大脑就会寻找停止这种负面刺激的方式，即将注意力转移到其他事情上。但是当人们能够面对这些不愿意做的事情时，这种不适感很快就消失了。使用番茄工作法可以解决拖延症，一个番茄时钟完成后一定要给自己奖励、放松放松，这样才能不断激励自己。&lt;/p&gt;

&lt;h3 id=&quot;22-记忆&quot;&gt;2.2 记忆&lt;/h3&gt;

&lt;p&gt;记忆主要分为工作记忆和长期记忆两种。工作记忆是大脑即时、有意识处理信息时的记忆，它只能储存大约四个组块的信息。长期记忆则是一个记忆仓库，能够存储很多记忆。将工作记忆转入长期记忆仓库，需要间隔重复。&lt;/p&gt;

&lt;p&gt;数学和理论科学中的思想和概念普遍抽象。越是抽象的东西，越要反复练习、加强，记忆就会不断加深。学习不是一蹴而就的，学习一段时间后，休息一下，把注意力转移到其他东西上。这期间，大脑的发散模式在后台不断固化对概念的理解。这样建立起来的知识体系才是牢固的。&lt;/p&gt;

&lt;h3 id=&quot;23-睡眠&quot;&gt;2.3 睡眠&lt;/h3&gt;

&lt;p&gt;睡眠能够祛除大脑产生的有毒物质。在睡梦中，大脑会把学习到的东西反复演练，清楚一些记忆中不太重要的部分，增强想要记住的记忆。&lt;/p&gt;

&lt;h2 id=&quot;3-组块要领&quot;&gt;3. 组块——要领&lt;/h2&gt;

&lt;h3 id=&quot;31-何为组块&quot;&gt;3.1 何为组块&lt;/h3&gt;

&lt;p&gt;组块就是把信息碎片拼接起来的的过程。组块化让新的信息更容易记忆，也更容易将其整合到大框架内。&lt;/p&gt;

&lt;h3 id=&quot;32-如何构成组块&quot;&gt;3.2 如何构成组块&lt;/h3&gt;

&lt;p&gt;首先，需要对待组块化的信息保持注意力。然后，要求对建立组块的对象有基本了解，只有在自己实际操作和完全掌握的情况下才能建立起组块。其次，要获取背景知识。背景认识意味着学会在特定的时候使用正确的方法。最后，熟能生巧。通过练习，不论是由上而下的认识，还是由下而上的组块化，都能够让你更熟练地运用这个组块。&lt;/p&gt;

&lt;h3 id=&quot;33-能力错觉&quot;&gt;3.3 能力错觉&lt;/h3&gt;

&lt;p&gt;阅读会让人产生能力错觉。仅仅扫一眼答案而不代入题目中并没有什么作用。同样，做笔记时，高亮和下划线标记需要谨慎，否则也会让自己误认为已经记住这些概念。如果要做标记，试着在勾画前找到中心思想，并试着尽量减少划线和高亮的内容，每段不超过一句。除此之外，空白处总结关键概念是一种很好的办法 。&lt;/p&gt;

&lt;p&gt;回顾，即在心中检索关键概念，可以使学习更加专注和高效，也能够避免部分能力错觉。回顾需要间隔时间，需要潜意识在大脑中不断巩固。在不同场景回顾学习的东西，与会帮助加深对学习内容的理解。&lt;/p&gt;

&lt;h2 id=&quot;4-看到全局&quot;&gt;4. 看到全局&lt;/h2&gt;

&lt;h3 id=&quot;41-组块库的价值&quot;&gt;4.1 组块库的价值&lt;/h3&gt;

&lt;p&gt;组块越多、运用越熟练，能更容易地解决问题。即使是理解不同领域的新概念，组块还能起到迁移的作用。组块能够在不熟悉新知识的情况下，给发散模式带来直觉，再由专注模式小心求证。&lt;/p&gt;

&lt;h3 id=&quot;42-过度学习抑制思维定式与交叉&quot;&gt;4.2 过度学习、抑制、思维定式与交叉&lt;/h3&gt;

&lt;p&gt;当学会一个东西时，在那段时间内不断重复并不能加强关于它的长期记忆，反而容易造成能力错觉——让人误以为掌握了所有内容，但是实际只了解个皮毛。这时，应该把精力集中于困难的部分，使用刻意训练。&lt;/p&gt;

&lt;p&gt;原先的思维模式，加上专注模式，会形成思维定势，这会阻止你走向可能发现解法的新区域。所以在学习新事物的时候，你必须摒弃错误的旧思想和方法。&lt;/p&gt;

&lt;p&gt;当你掌握了某一技巧的基本概念时，你应该开始将其交叉于不同类别的问题、方法、概念、过程和学科中。交叉学习让大脑更具有灵活性和创造性，让大脑学会独立思考。&lt;/p&gt;

&lt;h2 id=&quot;5-拖延症&quot;&gt;5. 拖延症&lt;/h2&gt;

&lt;h3 id=&quot;51-我们都需要治疗拖延症&quot;&gt;5.1 我们都需要治疗拖延症&lt;/h3&gt;

&lt;p&gt;拖延症如同饮鸩止渴，短时间看似解决了问题，实则更糟。&lt;/p&gt;

&lt;h3 id=&quot;52-习惯&quot;&gt;5.2 习惯&lt;/h3&gt;

&lt;p&gt;习惯可以分成四个阶段：&lt;/p&gt;

&lt;p&gt;信号（cue）。本身无所谓好坏，重要的是我们对信号做出的反应&lt;/p&gt;

&lt;p&gt;惯式（routine）。大脑接收到信号后做出的习惯反应。&lt;/p&gt;

&lt;p&gt;奖励（reward）。任何一种习惯的养成都是因为这种习惯可以给我们带来好处。&lt;/p&gt;

&lt;p&gt;信念（belief）。坚信习惯，习惯就回变得强大，想要改变习惯，就必须改变潜藏其中的信念。&lt;/p&gt;

&lt;h3 id=&quot;53-过程与结果&quot;&gt;5.3 过程与结果&lt;/h3&gt;

&lt;p&gt;拖延的一个原因就是，人们经常把注意力放在结果上。为防止拖延，应把注意力集中在过程上。过程与一个个小习惯相关，这些小习惯可以帮助完成不愉快但是必须完成的任务。越是注重结果，越会适得其反。&lt;/p&gt;

&lt;p&gt;使用番茄时钟，为自己制定这一时间段的工作过程，而不是必须完成的任务，可以让人专注于过程。当然，使用降噪耳机为自己提供安静的学习环境也可以帮助集中注意力。&lt;/p&gt;

&lt;h3 id=&quot;54-利用僵尸意识来帮助自己&quot;&gt;5.4 利用僵尸意识来帮助自己&lt;/h3&gt;

&lt;p&gt;习惯的第一个要素是信号。克服一个习惯的诀窍就是改变你对某个信号的反应，唯一需要毅力的地方就是改变对这个信号的反应。拖延的问题在于自己意识不到自己正在拖延。你可以关闭对你时常有诱惑力的东西，来隔离那些最具杀伤力的信号。&lt;/p&gt;

&lt;p&gt;第二个要素是惯式。拖延时，经常把注意力转移到一些不那么痛苦的事情上去，大脑便会自动进入这个惯式。你必须主动重设旧习惯的反应信号，其关键是制订一个计划、养成一个新习惯。&lt;/p&gt;

&lt;p&gt;第三个是奖励。需要在摆脱拖延后，给自己一些奖励：毫无罪意地刷剧、看一场电影或是喝一瓶肥仔快乐水。当完成事情的效果越好，这件事情带来的愉悦感也越强，好好奖励奖励自己吧。&lt;/p&gt;

&lt;p&gt;最后是信念。可能事情越棘手时，越想回到拖延的常态中，但是新习惯的养成，能够让你坚持下去。&lt;/p&gt;

&lt;h3 id=&quot;55-应对生活与学习&quot;&gt;5.5 应对生活与学习&lt;/h3&gt;

&lt;p&gt;前一天晚上写下第二天的计划，这样就不会占用你的工作记忆。&lt;/p&gt;

&lt;p&gt;设定番茄时钟，劳逸结合，设置结束时间同样重要。&lt;/p&gt;

&lt;p&gt;最好睡前回忆复习一遍学习的新鲜事物。&lt;/p&gt;

&lt;h2 id=&quot;6-复兴式学习释放潜力&quot;&gt;6. 复兴式学习、释放潜力&lt;/h2&gt;

&lt;h3 id=&quot;61-怎样成为更好的学习者&quot;&gt;6.1 怎样成为更好的学习者&lt;/h3&gt;

&lt;p&gt;我们不能局限于学习狭义的课堂上的知识，学习如何学习是一种任何人都能掌握的技巧。&lt;/p&gt;

&lt;h3 id=&quot;62-创建生动的视觉比喻或类比&quot;&gt;6.2 创建生动的视觉比喻或类比&lt;/h3&gt;

&lt;p&gt;学习中引入比喻或类比，有助于记忆和理解概念，还能摆脱思维定式，将其运用在其他领域。&lt;/p&gt;

&lt;h3 id=&quot;63-团队合作的价值&quot;&gt;6.3 团队合作的价值&lt;/h3&gt;

&lt;p&gt;人的右脑能够帮助反观全局，发现其不合常理的地方，不断质疑。人的左脑则让人容易变得自满。最容易欺骗的人是自己。找到自己盲点和误区的最好方式之一，就是与同领域的人精诚合作。但是也要注意学习小组的效率，尽量减少闲聊、避免迟到。&lt;/p&gt;

&lt;h3 id=&quot;64-清单&quot;&gt;6.4 清单&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;你是否认真努力地去理解过课文？ 仅仅是找出课文里有解答过程的例题不算&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;你是否跟同学讨论过作业中的问题或是至少和其他人对过答案？&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;你是否尝试过在和同学讨论之前，先列出每道作业的解题大纲？&lt;/li&gt;
  &lt;li&gt;你是否积极参与作业小组中的讨论、贡献自己的观点并提出问题？&lt;/li&gt;
  &lt;li&gt;当你遇到问题的时候，是否会去咨询讲师或助教？&lt;/li&gt;
  &lt;li&gt;交作业的时候，你是否已经弄清了所有问题的答案？&lt;/li&gt;
  &lt;li&gt;对于作业中不明白的问题，你是否在课上提出疑问寻求解答？&lt;/li&gt;
  &lt;li&gt;如果你有辅导书，在考试前你是否已经认真通读它，并且相信自己弄明白了书上所有的问题？&lt;/li&gt;
  &lt;li&gt;你是否尝试略过具体计算，直接快速写出一些问题的解题思路？&lt;/li&gt;
  &lt;li&gt;你是否和同学一起复习过辅导书上的内容和其他问题并相互提问？&lt;/li&gt;
  &lt;li&gt;如果考前有复习课，你是否参加过，并对自己不确定的部分提出疑问？&lt;/li&gt;
  &lt;li&gt;考试前睡眠时间安排是否合理？&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;65-先难后易&quot;&gt;6.5 先难后易&lt;/h3&gt;

&lt;p&gt;难题能够唤起发散模式中的创造力，一两分钟内无法解决就迅速切换，做几道简单的后再来挑战。这个方法也可以有效帮你避开思维定势，或是避免陷入错误思维的泥沼，因为你将有机会从不同的角度看待这些问题。&lt;/p&gt;

&lt;h3 id=&quot;66-有用的有关测验的最后提示&quot;&gt;6.6 有用的、有关测验的最后提示&lt;/h3&gt;

&lt;p&gt;在考试或测试前一天，最后快速浏览一遍资料、温习所学知识。不要为自己似乎没在大考前一天拼命复习而自责，不要为自己似乎没在大考前一天拼命复习而自责。如果你好好准备了，考前稍稍放松就是一种自然反应，就像下意识地节省脑力一样。考试时，你也应该谨记：你的大脑还会欺骗你。你应该时刻擦亮眼睛 ，转移注意力并再次检查，从全局角度核对你的答案。多问问自己：这样做真的合乎逻辑吗？在你检查时，由后至前的顺序往往更能给大脑一个新鲜的视角，让你更容易揪出错误。当你充分准备、勤奋练习、储备知识，考试时再注意技巧，你就会发现好运气会逐渐地出现在你面前。&lt;/p&gt;
</description>
        <pubDate>Thu, 03 May 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/05/03/lhtl/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/05/03/lhtl/</guid>
        
        <category>学习</category>
        
        
      </item>
    
      <item>
        <title>Machine Learning Week 3</title>
        <description>&lt;h1 id=&quot;4-logistic-regression&quot;&gt;4. Logistic Regression&lt;/h1&gt;

&lt;h2 id=&quot;41-classification-and-representation&quot;&gt;4.1 Classification and Representation&lt;/h2&gt;

&lt;h3 id=&quot;411-classification&quot;&gt;4.1.1 Classification&lt;/h3&gt;

&lt;p&gt;To attempt classification, one method is to use linear regression and map all predictions greater than 0.5 as a 1 and all less than 0.5 as a 0. However, this method doesn’t work well because classification is not actually a linear function.&lt;/p&gt;

&lt;p&gt;The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values. For now, we will focus on the &lt;strong&gt;binary classification&lt;/strong&gt; &lt;strong&gt;problem&lt;/strong&gt; in which y can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.) For instance, if we are trying to build a spam classifier for email, then $x^{(i)}$ may be some features of a piece of email, and y may be 1 if it is a piece of spam mail, and 0 otherwise. Hence, y∈{0,1}. 0 is also called the negative class, and 1 the positive class, and they are sometimes also denoted by the symbols “-” and “+.” Given $x^{(i)}$, the corresponding $y^{(i)}$ is also called the label for the training example.&lt;/p&gt;

&lt;h3 id=&quot;412-hypothesis-representation&quot;&gt;4.1.2 Hypothesis Representation&lt;/h3&gt;

&lt;p&gt;We could approach the classification problem ignoring the fact that y is discrete-valued, and use our old linear regression algorithm to try to predict y given x. However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn’t make sense for hθ(x) to take values larger than 1 or smaller than 0 when we know that y ∈ {0, 1}. To fix this, let’s change the form for our hypotheses hθ(x)to satisfy 0≤hθ(x)≤1. This is accomplished by plugging θTx into the Logistic Function.&lt;/p&gt;

&lt;p&gt;Our new form uses the “Sigmoid Function,” also called the “Logistic Function”:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; h_\theta (x) = g ( \theta^T x ) \newline \newline&amp; z = \theta^T x \newline&amp; g(z) = \dfrac{1}{1 + e^{-z}}\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The following image shows us what the sigmoid function looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcly1fquoucaittj30my03k74g.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The function g(z), shown here, maps any real number to the (0, 1) interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification.&lt;/p&gt;

&lt;p&gt;hθ(x) will give us the &lt;strong&gt;probability&lt;/strong&gt; that our output is 1. For example, $h_θ(x)=0.7$ gives us a probability of 70% that our output is 1. Our probability that our prediction is 0 is just the complement of our probability that it is 1 (e.g. if probability that it is 1 is 70%, then the probability that it is 0 is 30%).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align*} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;413-decision-boundary&quot;&gt;4.1.3 Decision Boundary&lt;/h3&gt;

&lt;p&gt;In order to get our discrete 0 or 1 classification, we can translate the output of the hypothesis function as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; h_\theta(x) \geq 0.5 \rightarrow y = 1 \newline&amp; h_\theta(x) &lt; 0.5 \rightarrow y = 0 \newline\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The way our logistic function g behaves is that when its input is greater than or equal to zero, its output is greater than or equal to 0.5:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; g(z) \geq 0.5 \newline&amp; when \; z \geq 0\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Remember.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}z=0, e^{0}=1 \Rightarrow g(z)=1/2\newline z \to \infty, e^{-\infty} \to 0 \Rightarrow g(z)=1 \newline z \to -\infty, e^{\infty}\to \infty \Rightarrow g(z)=0 \end{align*}&lt;/script&gt;

&lt;p&gt;So if our input to g is θTX, then that means:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; h_\theta(x) = g(\theta^T x) \geq 0.5 \newline&amp; when \; \theta^T x \geq 0\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;From these statements we can now say:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; \theta^T x \geq 0 \Rightarrow y = 1 \newline&amp; \theta^T x &lt; 0 \Rightarrow y = 0 \newline\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The &lt;strong&gt;decision boundary&lt;/strong&gt; is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; \theta = \begin{bmatrix}5 \newline -1 \newline 0\end{bmatrix} \newline &amp; y = 1 \; if \; 5 + (-1) x_1 + 0 x_2 \geq 0 \newline &amp; 5 - x_1 \geq 0 \newline &amp; - x_1 \geq -5 \newline&amp; x_1 \leq 5 \newline \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In this case, our decision boundary is a straight vertical line placed on the graph where $x_1=5$, and everything to the left of that denotes y = 1, while everything to the right denotes y = 0.&lt;/p&gt;

&lt;p&gt;Again, the input to the sigmoid function g(z) (e.g. $θ^TX$) doesn’t need to be linear, and could be a function that describes a circle (e.g. $z=θ_0+θ_1x^2_1+θ_2x^2_2$) or any shape to fit our data.&lt;/p&gt;

&lt;h2 id=&quot;42-logistic-regression-model&quot;&gt;4.2 Logistic Regression Model&lt;/h2&gt;

&lt;h3 id=&quot;421-cost-function&quot;&gt;4.2.1 Cost Function&lt;/h3&gt;

&lt;p&gt;We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.&lt;/p&gt;

&lt;p&gt;Instead, our cost function for logistic regression looks like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; &amp; \text{if y = 1} \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; &amp; \text{if y = 0}\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;When y = 1, we get the following plot for $J(θ)$ vs $h_θ(x)$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fqx182bk1vj308c06s0t7.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similarly, when y = 0, we get the following plot for $J(θ)$ vs $h_θ(x)$:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fqx1839j8fj308b07sdg8.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; \mathrm{Cost}(h_\theta(x),y) = 0 \text{ if } h_\theta(x) = y \newline &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 0 \; \mathrm{and} \; h_\theta(x) \rightarrow 1 \newline &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 1 \; \mathrm{and} \; h_\theta(x) \rightarrow 0 \newline \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;If our correct answer ‘y’ is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity.&lt;/p&gt;

&lt;p&gt;If our correct answer ‘y’ is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity.&lt;/p&gt;

&lt;p&gt;Note that writing the cost function in this way guarantees that J(θ) is convex for logistic regression.&lt;/p&gt;

&lt;h3 id=&quot;422-simplified-cost-function-and-gradient-descent&quot;&gt;4.2.2 Simplified Cost Function and Gradient Descent&lt;/h3&gt;

&lt;p&gt;We can compress our cost function’s two conditional cases into one case:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Cost(h_θ(x),y)=−ylog(h_θ(x))−(1−y)log(1−h_θ(x))&lt;/script&gt;

&lt;p&gt;Notice that when y is equal to 1, then the second term (1−y)log(1−hθ(x)) will be zero and will not affect the result. If y is equal to 0, then the first term −ylog(hθ(x)) will be zero and will not affect the result.&lt;/p&gt;

&lt;p&gt;We can fully write out our entire cost function as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]&lt;/script&gt;

&lt;p&gt;A vectorized implementation is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} &amp; h = g(X\theta)\newline &amp; J(\theta) = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right) \end{align*} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h4&gt;

&lt;p&gt;Remember that the general form of gradient descent is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \newline &amp; \rbrace\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can work out the derivative part using calculus to get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} &amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline &amp; \rbrace \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Notice that this algorithm is identical to the one we used in linear regression. We still have to simultaneously update all values in theta.&lt;/p&gt;

&lt;p&gt;A vectorized implementation is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;θ:=θ−\frac{α}{m}X^T(g(Xθ)−\hat{y})&lt;/script&gt;

&lt;h3 id=&quot;423-advanced-optimization&quot;&gt;4.2.3 Advanced Optimization&lt;/h3&gt;

&lt;p&gt;“Conjugate gradient”, “BFGS”, and “L-BFGS” are more sophisticated, faster ways to optimize $θ$ that can be used instead of gradient descent. We suggest that you should not write these more sophisticated algorithms yourself (unless you are an expert in numerical computing) but use the libraries instead, as they’re already tested and highly optimized. Octave provides them.&lt;/p&gt;

&lt;p&gt;We first need to provide a function that evaluates the following two functions for a given input value θ:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(θ)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{∂}{∂θ_j}J(θ)&lt;/script&gt;

&lt;p&gt;We can write a single function that returns both of these:&lt;/p&gt;

&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jVal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;costFunction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;jVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;code to compute J(theta)...];&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;gradient&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;code to compute derivative of J(theta)...];&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we can use octave’s “fminunc()” optimization algorithm along with the “optimset()” function that creates an object containing the options we want to send to “fminunc()”.&lt;/p&gt;

&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;optimset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'GradObj'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'on'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'MaxIter'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;initialTheta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optTheta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;functionVal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exitFlag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fminunc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;costFunction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialTheta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
     &lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We give to the function “fminunc()” our cost function, our initial vector of theta values, and the “options” object that we created beforehand.&lt;/p&gt;

&lt;h2 id=&quot;43-multiclass-classification&quot;&gt;4.3 Multiclass Classification&lt;/h2&gt;

&lt;h3 id=&quot;431multiclass-classification-one-vs-all&quot;&gt;4.3.1Multiclass Classification: One-vs-all&lt;/h3&gt;

&lt;p&gt;Now we will approach the classification of data when we have more than two categories. Instead of y = {0,1} we will expand our definition so that y = {0,1…n}.&lt;/p&gt;

&lt;p&gt;Since y = {0,1…n}, we divide our problem into n+1 (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that ‘y’ is a member of one of our classes.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; y \in \lbrace0, 1 ... n\rbrace \newline&amp; h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \newline&amp; h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \newline&amp; \cdots \newline&amp; h_\theta^{(n)}(x) = P(y = n | x ; \theta) \newline&amp; \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\newline\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We are basically choosing one class and then lumping all the others into a single second class. We do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.&lt;/p&gt;

&lt;p&gt;The following image shows how one could classify 3 classes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fqx182rhj0j30d507agmp.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To summarize:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Train a logistic regression classifier $h_θ(x)$ for each class￼ to predict the probability that ￼ ￼y = i￼ ￼.&lt;/p&gt;

&lt;p&gt;To make a prediction on a new x, pick the class ￼that maximizes $h_θ(x)$&lt;/p&gt;

&lt;h1 id=&quot;5-regularization&quot;&gt;5. Regularization&lt;/h1&gt;

&lt;h2 id=&quot;51-solving-the-problem-of-overfitting&quot;&gt;5.1 Solving the Problem of Overfitting&lt;/h2&gt;

&lt;h3 id=&quot;511-the-problem-of-overfitting&quot;&gt;5.1.1 The Problem of Overfitting&lt;/h3&gt;

&lt;p&gt;Consider the problem of predicting y from x ∈ R. The leftmost figure below shows the result of fitting a $y = θ_0+θ_1x$ to a dataset. We see that the data doesn’t really lie on straight line, and so the fit is not very good.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fqy3d4x38dj30f0046dg2.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Instead, if we had added an extra feature $x^2$ , and fit $y=θ_0+θ_1x+θ_2x^2$ , then we obtain a slightly better fit to the data (See middle figure). Naively, it might seem that the more features we add, the better. However, there is also a danger in adding too many features: The rightmost figure is the result of fitting a 5th order polynomial $y=∑^{5}_{j=0}θ_jx^j$. We see that even though the fitted curve passes through the data perfectly, we would not expect this to be a very good predictor of, say, housing prices (y) for different living areas (x). Without formally defining what these terms mean, we’ll say the figure on the left shows an instance of &lt;strong&gt;underfitting&lt;/strong&gt;—in which the data clearly shows structure not captured by the model—and the figure on the right is an example of &lt;strong&gt;overfitting&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Underfitting, or high bias, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. At the other extreme, overfitting, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.&lt;/p&gt;

&lt;p&gt;This terminology is applied to both linear and logistic regression. There are two main options to address the issue of overfitting:&lt;/p&gt;

&lt;p&gt;1) Reduce the number of features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Manually select which features to keep.&lt;/li&gt;
  &lt;li&gt;Use a model selection algorithm (studied later in the course).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2) Regularization&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Keep all the features, but reduce the magnitude of parameters $θ_j$.&lt;/li&gt;
  &lt;li&gt;Regularization works well when we have a lot of slightly useful features.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;512-cost-function&quot;&gt;5.1.2 Cost Function&lt;/h3&gt;

&lt;p&gt;If we have overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost.&lt;/p&gt;

&lt;p&gt;Say we wanted to make the following function more quadratic:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;θ_0+θ_1x+θ_2x^2+θ_3x^3+θ_4x^4&lt;/script&gt;

&lt;p&gt;We’ll want to eliminate the influence of $θ_3x^3$ and $θ_4x^4$ . Without actually getting rid of these features or changing the form of our hypothesis, we can instead modify our &lt;strong&gt;cost function&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;min_θ \frac{1}{2m}∑^m_{i=1}(h_θ(x^{(i)})−y^{(i)})^2+1000⋅θ^2_3+1000⋅θ^2_4&lt;/script&gt;

&lt;p&gt;We’ve added two extra terms at the end to inflate the cost of $θ_3$ and $θ_4$. Now, in order for the cost function to get close to zero, we will have to reduce the values of $θ_3$ and $θ_4$ to near zero. This will in turn greatly reduce the values of $θ_3x^3$ and $θ_4x^4$ in our hypothesis function. As a result, we see that the new hypothesis (depicted by the pink curve) looks like a quadratic function but fits the data better due to the extra small terms $θ_3x^3$ and $θ_4x^4$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcly1fr0rrwonzxj30gh091wfl.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We could also regularize all of our theta parameters in a single summation as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;min_\theta\ \dfrac{1}{2m}\  [ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2 ]&lt;/script&gt;

&lt;p&gt;The $\lambda$, or lambda, is the &lt;strong&gt;regularization parameter&lt;/strong&gt;. It determines how much the costs of our theta parameters are inflated.&lt;/p&gt;

&lt;p&gt;Using the above cost function with the extra summation, we can smooth the output of our hypothesis function to reduce overfitting. If lambda is chosen to be too large, it may smooth out the function too much and cause underfitting. Hence, what would happen if $λ=0$ or is too small ? If $\lambda$ is very small, $\theta$ will not be restricted. So it will cause overfitting.&lt;/p&gt;

&lt;h3 id=&quot;513-regularized-linear-regression&quot;&gt;5.1.3 Regularized Linear Regression&lt;/h3&gt;

&lt;p&gt;We can apply regularization to both linear regression and logistic regression. We will approach linear regression first.&lt;/p&gt;

&lt;h4 id=&quot;gradient-descent-1&quot;&gt;Gradient Descent&lt;/h4&gt;

&lt;p&gt;We will modify our gradient descent function to separate out $\theta_0$ from the rest of the parameters because we do not want to penalize $\theta_0$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} &amp; \text{Repeat}\ \lbrace \newline &amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline &amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline &amp; \rbrace \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The term $\frac{λ}{m}θ_j$ performs our regularization. With some manipulation our update rule can also be represented as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;θ_j:=θ_j(1−α\frac{λ}{m})−α\frac{1}{m}∑^m_{i=1}(h_θ(x^{(i)})−y^{(i)})x^{(i)}_j&lt;/script&gt;

&lt;p&gt;The first term in the above equation, $1−α\frac{λ}m$ will always be less than 1. Intuitively you can see it as reducing the value of $θ_j$ by some amount on every update. Notice that the second term is now exactly the same as it was before.&lt;/p&gt;

&lt;h4 id=&quot;normal-equation&quot;&gt;&lt;strong&gt;Normal Equation&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;Now let’s approach regularization using the alternate method of the non-iterative normal equation.&lt;/p&gt;

&lt;p&gt;To add in regularization, the equation is the same as our original, except that we add another term inside the parentheses:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline&amp; \text{where}\ \ L = \begin{bmatrix} 0 &amp; &amp; &amp; &amp; \newline &amp; 1 &amp; &amp; &amp; \newline &amp; &amp; 1 &amp; &amp; \newline &amp; &amp; &amp; \ddots &amp; \newline &amp; &amp; &amp; &amp; 1 \newline\end{bmatrix}\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;L is a matrix with 0 at the top left and 1’s down the diagonal, with 0’s everywhere else. It should have dimension (n+1)×(n+1). Intuitively, this is the identity matrix (though we are not including $x_0$), multiplied with a single real number λ.&lt;/p&gt;

&lt;p&gt;Recall that if m &amp;lt; n, then $X^TX$ is non-invertible. However, when we add the term λ⋅L, then $X^TX + λ⋅L$ becomes invertible.&lt;/p&gt;

&lt;h3 id=&quot;514-regularized-logistic-regression&quot;&gt;5.1.4 Regularized Logistic Regression&lt;/h3&gt;

&lt;p&gt;We can regularize logistic regression in a similar way that we regularize linear regression. As a result, we can avoid overfitting. The following image shows how the regularized function, displayed by the pink line, is less likely to overfit than the non-regularized function represented by the blue line:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcly1fr0rr7x64lj30do07edh2.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;cost-function&quot;&gt;Cost Function&lt;/h4&gt;

&lt;p&gt;Recall that our cost function for logistic regression was:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(θ)=−\frac1m∑^m_{i=1}[y^{(i)} log(h_θ(x^{(i)}))+(1−y^{(i)}) log(1−h_θ(x^{(i)}))]&lt;/script&gt;

&lt;p&gt;We can regularize this equation by adding a term to the end:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2&lt;/script&gt;

&lt;p&gt;The second sum, $∑^n_{j=1}θ^2_j$ &lt;strong&gt;means to explicitly exclude&lt;/strong&gt; the bias term, $θ_0$. I.e. the θ vector is indexed from 0 to n (holding n+1 values, $θ_0$ through $θ_n$), and this sum explicitly skips $θ_0$, by running from 1 to n, skipping 0. Thus, when computing the equation, we should continuously update the two following equations:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fqy6zag3yhj30de06h75k.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Apr 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/04/30/ml-week3/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/04/30/ml-week3/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>Machine Learning Week 2</title>
        <description>&lt;h1 id=&quot;3-linear-regression-with-multiple-variables&quot;&gt;3. Linear Regression with Multiple Variables&lt;/h1&gt;

&lt;h2 id=&quot;31-multivariate-linear-regression&quot;&gt;3.1 Multivariate Linear Regression&lt;/h2&gt;

&lt;h3 id=&quot;311-multiple-features&quot;&gt;3.1.1 Multiple Features&lt;/h3&gt;

&lt;p&gt;Linear regression with multiple variables is also known as “multivariate linear regression”.&lt;/p&gt;

&lt;p&gt;We now introduce notation for equations where we can have any number of input variables.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}x_j^{(i)} &amp;= \text{value of feature } j \text{ in the }i^{th}\text{ training example} \newline x^{(i)}&amp; = \text{the column vector of all the feature inputs of the }i^{th}\text{ training example} \newline m &amp;= \text{the number of training examples} \newline n &amp;= \left| x^{(i)} \right| ; \text{(the number of features)} \end{align*}​ %]]&gt;&lt;/script&gt;

&lt;p&gt;The multivariable form of the hypothesis function accommodating these multiple features is as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_θ(x)=θ_0+θ_1x_1+θ_2x_2+θ_3x_3+⋯+θ_nx_n&lt;/script&gt;

&lt;p&gt;In order to develop intuition about this function, we can think about $θ_0$ as the basic price of a house, $θ_1$ as the price per square meter, $θ_2$ as the price per floor, etc. $x_1$ will be the number of square meters in the house, $x_2$ the number of floors, etc.&lt;/p&gt;

&lt;p&gt;Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
h_θ(x)=\begin{bmatrix}θ_0&amp;θ_1&amp;…&amp;θ_n\end{bmatrix}\begin{bmatrix}x_0\\x_1\\…\\x_n\end{bmatrix}=θ^Tx %]]&gt;&lt;/script&gt;

&lt;p&gt;This is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more.&lt;/p&gt;

&lt;p&gt;Remark: Note that for convenience reasons in this course we assume $x^{(i)}_0=1$ for $(i∈1,…,m)$.&lt;/p&gt;

&lt;p&gt;[&lt;strong&gt;Note&lt;/strong&gt;: So that we can do matrix operations with theta and x, we will set $x^{(i)}&lt;em&gt;0 = 1$, for all values of i. This makes the two vectors ‘theta’ and $x&lt;/em&gt;{(i)}$ match each other element-wise  (that is, have the same number of elements: n+1).]&lt;/p&gt;

&lt;p&gt;The training examples are stored in X row-wise, like such:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}X = \begin{bmatrix}x^{(1)}_0 &amp; x^{(1)}_1  \newline x^{(2)}_0 &amp; x^{(2)}_1  \newline x^{(3)}_0 &amp; x^{(3)}_1 \end{bmatrix}&amp;,\theta = \begin{bmatrix}\theta_0 \newline \theta_1 \newline\end{bmatrix}\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;You can calculate the hypothesis as a column vector of size (m x 1) with:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_θ(X)=Xθ&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;For the rest of these notes, and other lecture notes, X will represent a matrix of training examples&lt;/strong&gt; $x_{(i)}$ &lt;strong&gt;stored row-wise.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;312-cost-function&quot;&gt;3.1.2 Cost function&lt;/h3&gt;

&lt;p&gt;For the parameter vector θ (of type $ℝ^{n+1}$ or in $ℝ^{(n+1)×1}$, the cost function is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(θ)=\frac {1}{2m}∑_{i=1}^m(hθ(x(i))−y(i))^2&lt;/script&gt;

&lt;p&gt;The vectorized version is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(θ)=\frac{1}{2m}(Xθ−\hat y )^T(Xθ−\hat y )&lt;/script&gt;

&lt;p&gt;Where $\hat{y}$  denotes the vector of all y values.&lt;/p&gt;

&lt;h3 id=&quot;313-gradient-descent-for-multiple-variables&quot;&gt;3.1.3 Gradient Descent for Multiple Variables&lt;/h3&gt;

&lt;p&gt;The gradient descent equation itself is generally the same form; we just have to repeat it for our ‘n’ features:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} &amp; \text{repeat until convergence:} \; \lbrace \newline \; &amp; \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_0^{(i)}\newline \; &amp; \theta_1 := \theta_1 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_1^{(i)} \newline \; &amp; \theta_2 := \theta_2 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_2^{(i)} \newline &amp; \cdots \newline \rbrace \end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In other words:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}&amp; \text{repeat until convergence:} \; \lbrace \newline \; &amp; \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \; &amp; \text{for j := 0...n}\newline \rbrace\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The following image compares gradient descent with one variable to gradient descent with multiple variables:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tKfTcly1fqs784zggaj30g508ttam.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;314-gradient-descent-in-practice-i---feature-scaling&quot;&gt;3.1.4 Gradient Descent in Practice I - Feature Scaling&lt;/h3&gt;

&lt;p&gt;We can speed up gradient descent by having each of our input values in roughly the same range. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.&lt;/p&gt;

&lt;p&gt;The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;−1 ≤ x_{(i)} ≤ 1&lt;/script&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;−0.5 ≤ x_{(i)} ≤ 0.5&lt;/script&gt;

&lt;p&gt;These aren’t exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.&lt;/p&gt;

&lt;p&gt;Two techniques to help with this are &lt;strong&gt;feature scaling&lt;/strong&gt; and &lt;strong&gt;mean normalization&lt;/strong&gt;. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;xi:=\frac {x_i−μ_i}{s_i}&lt;/script&gt;

&lt;p&gt;Where $μ_i$ is the &lt;strong&gt;average&lt;/strong&gt; of all the values for feature (i) and $s_i$ is the range of values (max - min), or si is the standard deviation.&lt;/p&gt;

&lt;p&gt;Note that dividing by the range, or dividing by the standard deviation, give different results. The quizzes in this course use range - the programming exercises use standard deviation.&lt;/p&gt;

&lt;p&gt;For example, if xi represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, $xi:=\frac {price−1000}{1900}$.&lt;/p&gt;

&lt;h3 id=&quot;315-gradient-descent-in-practice-ii---learning-rate&quot;&gt;3.1.5 Gradient Descent in Practice II - Learning Rate&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Debugging gradient descent.&lt;/strong&gt; Make a plot with &lt;em&gt;number of iterations&lt;/em&gt; on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Automatic convergence test.&lt;/strong&gt; Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as $10^{−3}$. However in practice it’s difficult to choose this threshold value.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcly1fqscb3p3vsj30eg07wt9v.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcly1fqscb4de3zj30e807pjsd.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To summarize:&lt;/p&gt;

&lt;p&gt;If α is too small: slow convergence.&lt;/p&gt;

&lt;p&gt;If α is too large: ￼may not decrease on every iteration and thus may not converge.&lt;/p&gt;

&lt;h3 id=&quot;316-features-and-polynomial-regression&quot;&gt;3.1.6 Features and Polynomial Regression&lt;/h3&gt;

&lt;p&gt;We can improve our features and the form of our hypothesis function in a couple different ways.&lt;/p&gt;

&lt;p&gt;We can &lt;strong&gt;combine&lt;/strong&gt; multiple features into one. For example, we can combine x1 and x2 into a new feature x3 by taking $x_1⋅x_2$.&lt;/p&gt;

&lt;h4 id=&quot;polynomial-regression&quot;&gt;Polynomial Regression&lt;/h4&gt;

&lt;p&gt;Our hypothesis function need not be linear (a straight line) if that does not fit the data well.&lt;/p&gt;

&lt;p&gt;We can &lt;strong&gt;change the behavior or curve&lt;/strong&gt; of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).&lt;/p&gt;

&lt;p&gt;For example, if our hypothesis function is $h_θ(x)=θ_0+θ_1x_1$ then we can create additional features based on $x_1$, to get the quadratic function $h_θ(x)=θ_0+θ_1x_1+θ_2x^2_1$ or the cubic function $h_θ(x)=θ_0+θ_1x_1+θ_2x^2_1+θ_3x^3_1$&lt;/p&gt;

&lt;p&gt;In the cubic version, we have created new features $x_2$ and $x_3$ where $x_2=x^2_1$ and $x_3=x^3_1$.&lt;/p&gt;

&lt;p&gt;To make it a square root function, we could do: $h_θ(x)=θ_0+θ_1x_1+θ_2\sqrt{x_1}$&lt;/p&gt;

&lt;p&gt;One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important.&lt;/p&gt;

&lt;p&gt;eg. if $x_1$ has range 1 - 1000 then range of $x^2_1$ becomes 1 - 1000000 and that of $x^3_1$ becomes 1 - 1000000000&lt;/p&gt;

&lt;h2 id=&quot;32-computing-parameters-analytically&quot;&gt;3.2 Computing Parameters Analytically&lt;/h2&gt;

&lt;h3 id=&quot;321-normal-equation&quot;&gt;3.2.1 Normal Equation&lt;/h3&gt;

&lt;p&gt;Gradient descent gives one way of minimizing J. Let’s discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. In the “Normal Equation” method, we will minimize J by explicitly taking its derivatives with respect to the $θ_j$ ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;θ=(X^TX)^{−1}X^Ty&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcly1fqsdxv5p9zj30gq09dgn1.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is &lt;strong&gt;no need&lt;/strong&gt; to do feature scaling with the normal equation.&lt;/p&gt;

&lt;p&gt;The following is a comparison of gradient descent and the normal equation:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Gradient Descent&lt;/th&gt;
      &lt;th&gt;Normal Equation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Need to choose alpha&lt;/td&gt;
      &lt;td&gt;No need to choose alpha&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Needs many iterations&lt;/td&gt;
      &lt;td&gt;No need to iterate&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$O (kn^2)$&lt;/td&gt;
      &lt;td&gt;$O (n^3)$, need to calculate inverse of $X^TX$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Works well when n is large&lt;/td&gt;
      &lt;td&gt;Slow if n is very large&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With the normal equation, computing the inversion has complexity $O(n^3)$. So if we have a very large number of features, the normal equation will be slow. In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.&lt;/p&gt;

&lt;h3 id=&quot;322-normal-equation-noninvertibility&quot;&gt;3.2.2 Normal Equation Noninvertibility&lt;/h3&gt;

&lt;p&gt;When implementing the normal equation in octave we want to use the ‘pinv’ function rather than &lt;code class=&quot;highlighter-rouge&quot;&gt;inv&lt;/code&gt; . The &lt;code class=&quot;highlighter-rouge&quot;&gt;pinv&lt;/code&gt; function will give you a value of $θ$ even if $X^TX$ is not invertible.&lt;/p&gt;

&lt;p&gt;If $X^TX$ is &lt;strong&gt;noninvertible,&lt;/strong&gt; the common causes might be having :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Redundant features, where two features are very closely related (i.e. they are linearly dependent)&lt;/li&gt;
  &lt;li&gt;Too many features (e.g. m ≤ n). In this case, delete some features or use “regularization” (to be explained in a later lesson).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.&lt;/p&gt;
</description>
        <pubDate>Sat, 28 Apr 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/04/28/ml-week2/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/04/28/ml-week2/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>Machine Learning Week 1</title>
        <description>&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;h2 id=&quot;11-introduction&quot;&gt;1.1 Introduction&lt;/h2&gt;

&lt;h3 id=&quot;111-what-is-machine-learning&quot;&gt;1.1.1 What is Machine Learning?&lt;/h3&gt;

&lt;p&gt;Two definitions of Machine Learning are offered. Arthur Samuel described it as: “the field of study that gives computers the ability to learn without being explicitly programmed.” This is an older, informal definition.&lt;/p&gt;

&lt;p&gt;Tom Mitchell provides a more modern definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”&lt;/p&gt;

&lt;p&gt;Example: playing checkers.&lt;/p&gt;

&lt;p&gt;E = the experience of playing many games of checkers&lt;/p&gt;

&lt;p&gt;T = the task of playing checkers.&lt;/p&gt;

&lt;p&gt;P = the probability that the program will win the next game.&lt;/p&gt;

&lt;p&gt;In general, any machine learning problem can be assigned to one of two broad classifications:&lt;/p&gt;

&lt;p&gt;Supervised learning and Unsupervised learning.&lt;/p&gt;

&lt;h3 id=&quot;112-supervised-learning&quot;&gt;1.1.2 Supervised Learning&lt;/h3&gt;

&lt;p&gt;In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.&lt;/p&gt;

&lt;p&gt;Supervised learning problems are categorized into “regression” and “classification” problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 1:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.&lt;/p&gt;

&lt;p&gt;We could turn this example into a classification problem by instead making our output about whether the house “sells for more or less than the asking price.” Here we are classifying the houses based on price into two discrete categories.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture&lt;/p&gt;

&lt;p&gt;(b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.&lt;/p&gt;

&lt;h3 id=&quot;113-unsupervised-learning&quot;&gt;1.1.3 Unsupervised Learning&lt;/h3&gt;

&lt;p&gt;Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables.&lt;/p&gt;

&lt;p&gt;We can derive this structure by clustering the data based on relationships among the variables in the data.&lt;/p&gt;

&lt;p&gt;With unsupervised learning there is no feedback based on the prediction results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.&lt;/p&gt;

&lt;p&gt;Non-clustering: The “Cocktail Party Algorithm”, allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a &lt;a href=&quot;https://en.wikipedia.org/wiki/Cocktail_party_effect&quot;&gt;cocktail party&lt;/a&gt;).&lt;/p&gt;

&lt;h1 id=&quot;2-linear-regression-with-one-variable&quot;&gt;2. Linear Regression with One Variable&lt;/h1&gt;

&lt;h2 id=&quot;21-model-and-cost-function&quot;&gt;2.1 Model and Cost Function&lt;/h2&gt;

&lt;h3 id=&quot;211-model-representation&quot;&gt;2.1.1 Model Representation&lt;/h3&gt;

&lt;p&gt;To establish notation for future use, we’ll use $x^{(i)}$ to denote the “input” variables (living area in this example), also called input features, and $y{(i)}$ to denote the “output” or target variable that we are trying to predict (price). A pair $(x^{(i)},y^{(i)})$ is called a training example, and the dataset that we’ll be using to learn—a list of m training examples $(x^{(i)},y^{(i)});i=1,…,m$—is called a training set. Note that the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation. We will also use X to denote the space of input values, and Y to denote the space of output values. In this example, X = Y = ℝ.&lt;/p&gt;

&lt;p&gt;To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis. Seen pictorially, the process is therefore like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fqqdi3zexhj30az078wes.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the target variable that we’re trying to predict is continuous, such as in our housing example, we call the learning problem a regression problem. When y can take on only a small number of discrete values (such as if, given the living area, we wanted to predict if a dwelling is a house or an apartment, say), we call it a classification problem.&lt;/p&gt;

&lt;h3 id=&quot;212-the-hypothesis-function&quot;&gt;2.1.2 The Hypothesis Function&lt;/h3&gt;

&lt;p&gt;Our hypothesis function has the general form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}=h_θ(x)=θ_0+θ_1x&lt;/script&gt;

&lt;p&gt;Note that this is like the equation of a straight line. We give to $h_θ(x)$ values for $θ_0$ and $θ_1$ to get our estimated output $\hat{y}$. In other words, we are trying to create a function called $h_θ$ that is trying to map our input data (the x’s) to our output data (the y’s).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Suppose we have the following set of training data:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;input x&lt;/th&gt;
      &lt;th&gt;output y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Now we can make a random guess about our hθ function: $θ_0=2$ and $θ_1=2$. The hypothesis function becomes $h_θ(x)=2+2x$.&lt;/p&gt;

&lt;p&gt;So for input of 1 to our hypothesis, y will be 4. This is off by 3. Note that we will be trying out various values of $θ_0$ and $θ_1$ to try to find values which provide the best possible “fit” or the most representative “straight line” through the data points mapped on the x-y plane.&lt;/p&gt;

&lt;h3 id=&quot;213-cost-function&quot;&gt;2.1.3 Cost Function&lt;/h3&gt;

&lt;p&gt;We can measure the accuracy of our hypothesis function by using a &lt;strong&gt;cost function&lt;/strong&gt;. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x’s and the actual output y’s.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(θ_0,θ_1)=\frac{1}{2m}\sum_{i=1}^{m}(\widehat{y}_i−y_i)^2=\frac{1}{2m}\sum_{i=1}^{m}(h_θ(x_i)−y_i)^2&lt;/script&gt;

&lt;p&gt;To break it apart, it is $\frac{1}{2} \bar x$ where $\bar x$ is the mean of the squares of $h_θ(x_i)−y_i$ , or the difference between the predicted value and the actual value.&lt;/p&gt;

&lt;p&gt;This function is otherwise called the “Squared error function”, or “Mean squared error”. The mean is halved $(\frac{1}{2m})$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $(\frac{1}{2})$ term. The following image summarizes what the cost function does:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNc79ly1fqqdhwkrbhj30i60aatak.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;214-cost-function---intuition-i&quot;&gt;2.1.4 Cost Function - Intuition I&lt;/h3&gt;

&lt;p&gt;If we try to think of it in visual terms, our training data set is scattered on the x-y plane. We are trying to make a straight line (defined by $h_θ(x)$) which passes through these scattered data points.&lt;/p&gt;

&lt;p&gt;Our objective is to get the best possible line. The best possible line will be such so that the average squared vertical distances of the scattered points from the line will be the least. Ideally, the line should pass through all the points of our training data set. In such a case, the value of $J(θ_0,θ_1)$ will be 0. The following example shows the ideal situation where we have a cost function of 0.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fqqe0hm9hyj30bf069dgo.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When $θ_1=1$, we get a slope of 1 which goes through every single data point in our model. Conversely, when $θ_1=0.5$, we see the vertical distance from our fit to the data points increase.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fqqe0kdkqrj30ba06adgi.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This increases our cost function to 0.58. Plotting several other points yields to the following graph:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fqqe0mpd6jj308g081dge.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thus as a goal, we should try to minimize the cost function. In this case, $θ_1=1$ is our global minimum.&lt;/p&gt;

&lt;h3 id=&quot;215-cost-function---intuition-ii&quot;&gt;2.1.5 Cost Function - Intuition II&lt;/h3&gt;

&lt;p&gt;A contour plot is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line. An example of such a graph is the one to the right below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fqqyl9uw92j30ic09qgo3.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Taking any color and going along the ‘circle’, one would expect to get the same value of the cost function. For example, the three green points found on the green line above have the same value for $J(θ_0,θ_1)$ and as a result, they are found along the same line. The circled x displays the value of the cost function for the graph on the left when $θ_0 = 800$ and $θ_1= -0.15$. Taking another h(x) and plotting its contour plot, one gets the following graphs:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fqqylauengj30i50a476m.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When $θ_0 = 360$ and $θ_1 = 0$, the value of $J(θ_0,θ_1)$ in the contour plot gets closer to the center thus reducing the cost function error. Now giving our hypothesis function a slightly positive slope results in a better fit of the data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fqqyl8lrv8j30hy08qgns.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The graph above minimizes the cost function as much as possible and consequently, the result of $θ_1$ and $θ_0$ tend to be around 0.12 and 250 respectively. Plotting those values on our graph to the right seems to put our point in the center of the inner most ‘circle’.&lt;/p&gt;

&lt;h2 id=&quot;22-parameter-learning&quot;&gt;2.2 Parameter Learning&lt;/h2&gt;

&lt;h3 id=&quot;221-gradient-descent&quot;&gt;2.2.1 Gradient Descent&lt;/h3&gt;

&lt;p&gt;So we have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That’s where gradient descent comes in.&lt;/p&gt;

&lt;p&gt;Imagine that we graph our hypothesis function based on its fields $θ_0$ and $θ_1$ (actually we are graphing the cost function as a function of the parameter estimates). We are not graphing x and y itself, but the parameter range of our hypothesis function and the cost resulting from selecting a particular set of parameters.&lt;/p&gt;

&lt;p&gt;We put $θ_0$ on the x axis and $θ_1$ on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fqqzup3o06j30ft08etby.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We will know that we have succeeded when our cost function is at the very bottom of the pits in our graph, i.e. when its value is the minimum. The red arrows show the minimum points in the graph.&lt;/p&gt;

&lt;p&gt;The way we do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter α, which is called the learning rate.&lt;/p&gt;

&lt;p&gt;For example, the distance between each ‘star’ in the graph above represents a step determined by our parameter α. A smaller α would result in a smaller step and a larger α results in a larger step. The direction in which the step is taken is determined by the partial derivative of J(θ0,θ1). Depending on where one starts on the graph, one could end up at different points. The image above shows us two different starting points that end up in two different places.&lt;/p&gt;

&lt;p&gt;The gradient descent algorithm is:&lt;/p&gt;

&lt;p&gt;repeat until convergence:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;θ_j:=θ_j−α\frac {∂}{∂θ_j}J(θ_0,θ_1)&lt;/script&gt;

&lt;p&gt;where j=0,1 represents the feature index number.&lt;/p&gt;

&lt;p&gt;At each iteration j, one should simultaneously update the parameters $θ_1,θ_2,…,θ_n$. Updating a specific parameter prior to calculating another one on the $j^{(th)}$ iteration would yield to a wrong implementation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fqqzulsm5oj30i903s75a.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;222-gradient-descent-intuition&quot;&gt;2.2.2 Gradient Descent Intuition&lt;/h3&gt;

&lt;p&gt;In this video we explored the scenario where we used one parameter $θ_1$ and plotted its cost function to implement a gradient descent. Our formula for a single parameter was :&lt;/p&gt;

&lt;p&gt;Repeat until convergence:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;θ_1:=θ_1−α\frac {d}{dθ_1}J(θ_1)&lt;/script&gt;

&lt;p&gt;Regardless of the slope’s sign for $\frac {d}{dθ_1}J(θ_1)$, $θ_1$ eventually converges to its minimum value. The following graph shows that when the slope is negative, the value of $θ_1$ increases and when it is positive, the value of $θ_1$ decreases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fqqzuqgjgwj30ho0a4myg.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On a side note, we should adjust our parameter α to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fqqzuix1r2j30if0a5gmx.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;how-does-gradient-descent-converge-with-a-fixed-step-size-α&quot;&gt;How does gradient descent converge with a fixed step size $α$?&lt;/h4&gt;

&lt;p&gt;The intuition behind the convergence is that $\frac {d}{dθ_1}J(θ_1)$ approaches 0 as we approach the bottom of our convex function. At the minimum, the derivative will always be 0 and thus we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;θ_1:=θ_1−α∗0&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://ws4.sinaimg.cn/large/006tNc79ly1fqqzuftdncj30i60a2400.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;223-gradient-descent-for-linear-regression&quot;&gt;2.2.3 Gradient Descent For Linear Regression&lt;/h3&gt;

&lt;p&gt;When specifically applied to the case of linear regression, a new form of the gradient descent equation can be derived. We can substitute our actual cost function and our actual hypothesis function and modify the equation to :&lt;/p&gt;

&lt;p&gt;repeat until convergence: {
&lt;script type=&quot;math/tex&quot;&gt;θ_0:=θ_0−α\frac {1}{m}∑_{i=1}^{m}(h_θ(x_i)−y_i)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;θ_1:=θ_1−α\frac {1}{m}∑_{i=1}^{m}((h_θ(x_i)−y_i)*x_i)&lt;/script&gt;
}&lt;/p&gt;

&lt;p&gt;where m is the size of the training set, $θ_0$ a constant that will be changing simultaneously with $θ_1$ and $x_i,y_i$ are values of the given training set (data).&lt;/p&gt;

&lt;p&gt;Note that we have separated out the two cases for $θ_j$ into separate equations for $θ_0$ and $θ_1$; and that for $θ_1$ we are multiplying $x_i$ at the end due to the derivative. The following is a derivation of $\frac{∂}{∂θ_j}J(θ)$ for a single example :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fqr0etb4uzj309s05a3yt.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The point of all this is that if we start with a guess for our hypothesis and then repeatedly apply these gradient descent equations, our hypothesis will become more and more accurate.&lt;/p&gt;

&lt;p&gt;So, this is simply gradient descent on the original cost function J. This method looks at every example in the entire training set on every step, and is called &lt;strong&gt;batch gradient descent&lt;/strong&gt;. Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum. Indeed, J is a convex quadratic function. Here is an example of gradient descent as it is run to minimize a quadratic function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/006tNc79ly1fqr0eu9wxnj308s06ot9c.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The ellipses shown above are the contours of a quadratic function. Also shown is the trajectory taken by gradient descent, which was initialized at (48,30). The x’s in the figure (joined by straight lines) mark the successive values of θ that gradient descent went through as it converged to its minimum.&lt;/p&gt;
</description>
        <pubDate>Thu, 26 Apr 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/04/26/ml-week1/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/04/26/ml-week1/</guid>
        
        <category>机器学习</category>
        
        
      </item>
    
      <item>
        <title>Mac OS下大数据工具环境搭建</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;本次配置具体运行环境如下：&lt;/p&gt;

  &lt;p&gt;macOS High Sierra 10.13.3&lt;/p&gt;

  &lt;p&gt;Java JDK 1.8.0&lt;/p&gt;

  &lt;p&gt;Hadoop 3.0.0&lt;/p&gt;

  &lt;p&gt;Scala 2.12.4&lt;/p&gt;

  &lt;p&gt;Spark 2.3.0&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;参考了很多帖子，然而很多并不适用。特此留下此马克，简洁易懂且适用，以便日后之需。&lt;/p&gt;

&lt;h3 id=&quot;准备工作配置ssh查看java路径&quot;&gt;准备工作（配置ssh、查看Java路径）&lt;/h3&gt;

&lt;p&gt;首先，打开系统偏好设置-共享-远程登录，勾选远程登录。然后打开 iTerm&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 查看Java JDK路径，并记录&lt;/span&gt;
/usr/libexec/java_home &lt;span class=&quot;nt&quot;&gt;-V&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 使用 ssh-keygen 生成一对密钥，并将其用于免密登录&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/.ssh
ssh-keygen
&lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;id_rsa.pub &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; authorized_keys
&lt;span class=&quot;c&quot;&gt;# 判断是否成功免密登录&lt;/span&gt;
ssh localhost
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;下面操作均在 &lt;code class=&quot;highlighter-rouge&quot;&gt;ssh localhost&lt;/code&gt; 之后进行&lt;/p&gt;

&lt;h3 id=&quot;1-安装hadoop&quot;&gt;1. 安装Hadoop&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 使用 Homebrew 安装 Hadoop&lt;/span&gt;
brew install hadoop
&lt;span class=&quot;c&quot;&gt;# 进入 Hadoop 路径，此处是 3.0.0 版本，其他版本自行更改命令&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /usr/local/Cellar/hadoop/3.0.0/libexec/etc/hadoop

vi hadoop-env.sh
&lt;span class=&quot;c&quot;&gt;# 使用 /HADOOP_OPTS 查找到被注释的行，添加以下两行，第二行修改为 Java 路径，修改后保存退出&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_OPTS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HADOOP_OPTS&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc= &quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;JAVA_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home&quot;&lt;/span&gt;

vi hdfs-site.xml
&lt;span class=&quot;c&quot;&gt;# &amp;lt;configuration&amp;gt; 标签中添加以下内容，修改后保存退出&lt;/span&gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;dfs.http.address&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;0.0.0.0:50070&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

vi mapred-site.xml
&lt;span class=&quot;c&quot;&gt;# &amp;lt;configuration&amp;gt; 标签中添加以下内容，修改后保存退出&lt;/span&gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;mapred.job.tracker&amp;lt;/name&amp;gt;
	&amp;lt;value&amp;gt;localhost:8021&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

vi core-site.xml
&lt;span class=&quot;c&quot;&gt;# &amp;lt;configuration&amp;gt; 标签中添加以下内容，修改后保存退出&lt;/span&gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt;
&amp;lt;value&amp;gt;/usr/local/Cellar/hadoop/hdfs/tmp&amp;lt;/value&amp;gt;
    &amp;lt;description&amp;gt;A base &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;other temporary directories.&amp;lt;/description&amp;gt;
&amp;lt;/property&amp;gt;
&amp;lt;property&amp;gt;
	&amp;lt;name&amp;gt;fs.default.name&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hdfs://localhost:8020&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;

&lt;span class=&quot;c&quot;&gt;# 格式化后即可&lt;/span&gt;
hdfs namenode &lt;span class=&quot;nt&quot;&gt;-format&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /usr/local/Cellar/hadoop/3.0.0/sbin
&lt;span class=&quot;c&quot;&gt;# 启动 Hadoop，WARN 不用管，打开浏览器，查看 localhost 的 50070 端口和 8088 端口&lt;/span&gt;
./start-dfs.sh
./start-yarn.sh
&lt;span class=&quot;c&quot;&gt;# 停止 Hadoop&lt;/span&gt;
./stop-all.sh

&lt;span class=&quot;c&quot;&gt;# 可以更改环境变量，之后可直接使用 start-dfs.sh 和 start-yarn.sh 来启动Hadoop&lt;/span&gt;
vi ~/.bash_profile
&lt;span class=&quot;c&quot;&gt;# 添加以下在尾部，修改后保存退出&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/Cellar/hadoop/3.0.0
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/sbin:&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/bin

&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ~/.bash_profile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2-安装scala&quot;&gt;2. 安装Scala&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 使用 Homebrew 安装 Scala&lt;/span&gt;
brew install scala
&lt;span class=&quot;c&quot;&gt;# 测试 Scala 安装成功&lt;/span&gt;
scala
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3-安装spark&quot;&gt;3. 安装Spark&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 使用 Homebrew 安装 Spark&lt;/span&gt;
brew install apache-spark
&lt;span class=&quot;c&quot;&gt;# 测试 Spark 安装成功&lt;/span&gt;
spark-shell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Mon, 19 Mar 2018 23:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/03/19/big-data-env/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/03/19/big-data-env/</guid>
        
        <category>大数据</category>
        
        
      </item>
    
      <item>
        <title>大学生能力测试</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;对于考研，我更愿意称之为大学生能力测试，因为里面的科目我都不太喜欢&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;今年报考的是北京大学软件与微电子学院，金融大数据方向。从结果来看，这次报考算是偷波鸡，明年是否还是这个情况我也不清楚了。留着当个念想吧，说不定还能帮到今后的考研勇士。&lt;/p&gt;

&lt;h3 id=&quot;一初试&quot;&gt;一、初试&lt;/h3&gt;

&lt;p&gt;初始分也不算太高（政治59，英语71，数学110，专业课109，总分349），各位按照自己的计划进行，可能不太适合所有人。说实话我的专业课我的确考得不太好。&lt;/p&gt;

&lt;h4 id=&quot;1-数学&quot;&gt;1. 数学&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;辅导书：推荐高数十八讲（张宇）、概率论九讲（张宇），以及线性代数辅导讲义（李永乐）。&lt;/li&gt;
  &lt;li&gt;网课：高数、概率论看张宇，线代看李永乐，看基础班、强化班。最后有时间可以看点题班，没时间也无所谓，影响不大。&lt;/li&gt;
  &lt;li&gt;习题集：看课的时候搭配教材和辅导书上的习题（较简单）以及题源1000题（难一些，张宇），看完视频了，最后标记一下错题。后面开始做数学历年真题集（张宇），做完了再整理一下错题。最后有时间可以做张宇的最后八套卷（非常难，分数不用当真），没时间就把整理的历年真题的错题再做一遍。&lt;/li&gt;
  &lt;li&gt;具体思路：建议前期直接看视频学习。先看高数基础班、线代基础班、概率论基础班，看的时候做好笔记，后面复习有用。有时间就做一做教材上的题或题源1000题的基础部分，一般都比较简单。然后看高数提高班、线代提高班、概率论提高班，看一节做完一节题源1000题的题，错题标记出来。学习完这两遍之后，最迟十一月就开始做真题吧，每天一套左右。遇到难的要硬着头皮啃完，理解每道题的解法，把错题记录下来。十二月上旬就开始做错题，有时间再做做最后八套卷，没时间就把错题做完就行了。至此，数学能考的题型你基本都见过了，考前再看看错题，想想思路就行了。&lt;/li&gt;
  &lt;li&gt;至于李林还能不能押到题，很难预测到，有空的话找找资源也不亏。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;2-政治&quot;&gt;2. 政治&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;辅导书和习题集：个人不管肖秀荣怎么样，无脑推荐徐涛。买考研政治核心考案、优题库、习题集，最后八套卷，考研政治考前必背20题（都是徐涛的，除了最后一本书以外，主要都是怼选择题）。考研政治考前必背20题需要20天左右背完，合理安排时间（大概60道小题的内容）。做好错题标记，有空了看一看。&lt;/li&gt;
  &lt;li&gt;网课：直接看徐涛的&lt;strong&gt;强化班&lt;/strong&gt;。最好暑假开始看视频，挤不出来时间的话九月份开始也行。&lt;/li&gt;
  &lt;li&gt;具体思路：也建议直接上视频，并从九月份开始准备。我是从十月份开始的，时间略紧。算好视频的数量，每天看几个视频，然后做对应的题（徐涛推荐看完一门课做一门课的题，自己抉择吧），做完把错题标记一下。看完强化班之后，根据其他科目准备的情况，有时间可以看看冲刺班。然后注意他的解答题讲解视频，尤其是马哲的解答题需要自己练习。后面等小黄书（考研政治考前必背20题）出了，最迟考试前三天把内容背完，最后根据年度热门事件选择性地背熟某几个考试概率高的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3-英语&quot;&gt;3. 英语&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;关于背单词：我当时看了恋恋有词，但是我觉得看了遗忘的比较多，而且比较费时间。我就下载的沪江开心英语这个APP，每天150个词，1个小时左右。每个词有解释的听解释，效率对我而言稍高一些。第二天复习的时候注意换个题型考察自己。一些可以用到作文里的经典例句可以提前摘抄下来，以备练习作文时套用。&lt;/li&gt;
  &lt;li&gt;辅导书：做了黄剑的阅读理解150提高版，但是感觉书的质量离真题还是差了一些，要是愿意做也可以当个参考。还用了黄剑的历年真题和写作书（他的书全都是黄色的）。&lt;/li&gt;
  &lt;li&gt;具体思路：每天花半个小时到一个小时背150个单词，考前都不能停下。暑假开始做考研英语真题，一套卷子拆成两三天慢慢做（不用做写作），最好用铅笔写，做完之后看解析，想自己思路哪里不对。全部做完了把笔迹擦掉，隔几天再做一遍，一两天一份（不用做写作）。做完了就一天一节黄剑阅读理解，这个书很怪，个人答案有些奇怪。不过也不用管，反正保持每天阅读就行了。考前一个月看一看他的作文书，背一背小作文模板，练练大作文，上考场就基本不慌了。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;4-专业课&quot;&gt;4. 专业课&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;辅导书：看你报考学校的要求，一般是操作系统、数据结构、计算机网络和组成原理。王道论坛的一套书（这个去他们官方的淘宝店买，后面会送模拟题）质量还行，根据自己需要购买。如果感觉数据结构有些困难，可以买一本天勤的数据结构搭配着看，天勤的数据结构比较细、容易理解。&lt;/li&gt;
  &lt;li&gt;习题：除了书上的题，没时间的话做王道的最后八套，有时间王道和天勤都做了。做好错题整理，考前都有用，特别是计算机网络的杂碎知识点。&lt;/li&gt;
  &lt;li&gt;失败的思路：直接看辅导书做题，不懂的看书。来来回回看了四遍辅导书。&lt;/li&gt;
  &lt;li&gt;觉得可能正确的思路：课本仔细看一遍，然后再看辅导书，不懂的再看书。需要记住书上算法的具体实现。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;5-关于网课视频&quot;&gt;5. 关于网课视频&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;一般买书送的视频都不太靠谱，我的视频基本都是在大傻考研网这个公众号找的（他也有微博号），这个更新的比较快。&lt;/li&gt;
  &lt;li&gt;如果想要更及时的视频，请上&lt;strong&gt;B站&lt;/strong&gt;，多搜搜老师的名字有惊喜，特别是徐涛的视频。&lt;/li&gt;
  &lt;li&gt;如果现在找不到资源，数学可以看去年的，尤其是线性代数。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;6-关于流量&quot;&gt;6. 关于流量&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;开个一年的百度云会员，200多块钱，去图书馆三楼机房下视频或看B站视频。&lt;/li&gt;
  &lt;li&gt;最好搞个内网穿透，把这一年流量问题解决掉，但是稍微有些麻烦，没时间的话请忽略。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;7-其他&quot;&gt;7. 其他&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;晚上七点半准时选第二天图书馆的座位，如果需要三楼的电脑，请早起抢座。&lt;/li&gt;
  &lt;li&gt;考试前一周是四六级考试，请抢好座位。&lt;/li&gt;
  &lt;li&gt;年末背政治犯困，可移步一楼站着背书，一楼可能有点冷风，穿厚点，别感冒了。&lt;/li&gt;
  &lt;li&gt;注意坐姿，眼镜干涩备好眼药水。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;二复试&quot;&gt;二、复试&lt;/h3&gt;

&lt;p&gt;考完初试，休息几天，就抓紧准备复试吧。看看报考方向需要的知识，最好看那么一两本完整的入门书，不至于到时候慌了阵脚。多做做OJ，即使没有进复试也是在为以后积累，这个怎么着也不亏。&lt;/p&gt;

&lt;p&gt;具体过程。进去之后敲门、向老师们问好。老师英语问看过什么大数据的书、什么金融的书，既然没怎么看过金融的书为什么报这个方向（那就暑假在补呗……）。然后就开始聊项目了。讲了讲Adaboost算法原理和具体实现，但是具体实现没有说得特别清楚，有些紧张。总的来说和上午的几个小伙伴差不多，问题都不算太刁钻，都是顺着回答的不断深入地问。&lt;/p&gt;

&lt;h3 id=&quot;三后记&quot;&gt;三、后记&lt;/h3&gt;

&lt;p&gt;想想这半年多的确还是辛苦，从八月回学校真正开始看视频到十二月结束考试，走过了一些弯路，当然也得到了很多人的帮助。在此感谢爸爸妈妈、各位叔叔阿姨、各位老师同学的理解和支持。感谢舍友们对寝室两个考研党的关照。也感谢肥猫一直陪我学习，要是没有肥猫的帮助，我不可能坚持下来。本来想着如果上不了，我也不打算再来一次了。结果3月16号复试结束，3月19日晚上就收到了拟录取通知，然而肥猫运气稍差了一些，还在努力地争取着自己的未来。&lt;/p&gt;

&lt;p&gt;顺境需谨慎，不顺才是是常态，愿你我都有光明的未来。&lt;/p&gt;
</description>
        <pubDate>Sat, 17 Mar 2018 01:00:00 +0800</pubDate>
        <link>http://localhost:4000/2018/03/16/about-eep/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/03/16/about-eep/</guid>
        
        <category>考研</category>
        
        
      </item>
    
  </channel>
</rss>
